{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24018313-ad8a-4c47-a484-1e0e0e6e2f52",
   "metadata": {},
   "source": [
    "## Research Questions\n",
    "\n",
    "1) Does the tone (positive, negative, uncertain) expressed during an earnings call predict short-window abnormal stock returns, defined as the firm’s actual return over the [0, +1]-day event window surrounding the call in excess of the market return (proxied by the S&P 500 ETF, SPY)?\n",
    "2) Does the tone to return relationship differ across industries, firm sizes, or leadership?\n",
    "3) Do tone effects weaken or strengthen during high-volatility market days, as measured against the SPY (if there are large increases/decreases in SPY price, are the impacts of tone amplified or dampened)?\n",
    "4) *Potential question*: After controlling for EPS surprise (the difference between actual returns and the forecasted returns by external analysts, which, when positive or negative, can have a significant impact on a company's stock performance), does tone still explain residual abnormal returns (measured with a [-1,+1] event window)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98247ef-f125-4224-ac36-0f6e6a1125eb",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "Corporate earnings calls serve as the main bridge between enterprises and investors. They shape how markets interpret financial performance beyond the raw numbers, providing context to numeric output. While the quantitative outcomes of an earnings report are easy to measure, the language executives use – be it their tone, confidence, or underlying uncertainty – can carry additional weight, which has the potential to influence investor sentiment when localized to each occurrence.\n",
    "\n",
    "It is important to study this relationship because, while markets are a quantitative beast, they also rely on narrative, context, and behavioral signals. Prior work has shown that tone effects on “abnormal performance” can be predicted in gradual post-announcement stock price drift. As a contrast, this project isolates the short-window reaction ([0,+1]) to measure the immediate market response to tone, providing a complementary perspective that is clear of other market influences, which conflate analyses. Understanding this aspect of the psychology behind financial decision-making provides an interesting lens into the impacts of behavior on markets, informing future analysis and serving as an input for future models.\n",
    "\n",
    "Beyond its economic ties, this project provides an interesting computational exploration, combining natural language analysis and statistical models, which is becoming an ever-larger part of financial and economic research. By linking these natural language signals to numerical outcomes, it deepens (my) understanding of how unstructured information can be linked to statistical analysis, and how this information translates to measurable impacts.\n",
    "\n",
    "Experience with finance and markets, which I have gained over the last 3 years, along with my interest in data science, serve as the foundation for my desire to pursue this project. It is particularly interesting in its combination of NLP and statistical analysis, and I look forward to seeing the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd470f7-1694-4d59-b288-eb0a7882fd5a",
   "metadata": {},
   "source": [
    "## Data Setting\n",
    "This project draws on three publicly available datasets that together support analysis of how executive tone in earnings calls relates to short-window abnormal stock returns.\n",
    "1. **[Earnings Call Transcripts (Motley Fool / Kaggle)](https://www.kaggle.com/datasets/tpotterer/motley-fool-scraped-earnings-call-transcripts)** - This dataset includes roughly 18,000 quarterly earnings-call transcripts for U.S.-listed companies. Each record provides the company ticker, call date, exchange, quarter, and full transcript text. The data were scraped from The Motley Fool’s public archives and compiled by Kaggle contributors. The transcripts are the unstructured textual foundation for tone analysis, allowing extraction of sentiment features using finance-specific linguistic dictionaries (see #4).\n",
    "2. **[NASDAQ Daily Prices (Kaggle / Paul Mooney)](https://www.kaggle.com/datasets/svaningelgem/nasdaq-daily-stock-prices)** - This dataset contains daily open, high, low, close, adjusted-close, and volume (OHLCV) data for U.S. equities from roughly 2015-2024. It enables the computation of firm-level daily returns and the construction of event-window returns surrounding each earnings call date.\n",
    "3. **[S&P 500 ETF (SPY) Prices (Kaggle)](https://www.kaggle.com/datasets/benjaminbtang/spy-historical-prices)*** - This dataset provides historical daily prices for the SPY ETF, which is used as a market benchmark. Subtracting SPY’s daily return from a firm’s daily return produces a simple measure of abnormal return, controlling for broad market movements.\n",
    "4. *Supplemental dataset/tool* **|** ***[Loughran-McDonald Financial Sentiment Dictionary](https://sraf.nd.edu/loughranmcdonald-master-dictionary/)*** - Used map word occurrences in transcripts to finance-specific tone categories (positive, negative, uncertainty, etc.). This resource, widely adopted in accounting and finance research, ensures that the tone scores reflect financial meaning rather than generic sentiment.\n",
    "\n",
    "Each dataset is stored in CSV format and will be merged on ticker and date keys to align firm-level and market-level data for each event window.\n",
    "\n",
    "#### Potential Challenges\n",
    "None of the datasets include formal datasheets; however, several contextual details may complicate or encourage deeper analysis:\n",
    "1. **Coverage and survivorship bias** - The transcript dataset includes only companies covered by The Motley Fool, potentially omitting small-cap or delisted firms. This may over-represent large, stable firms and bias results toward those with stronger disclosure practices.\n",
    "2. **Timing misalignment** - Earnings calls often take place after market hours, while price data are recorded at the market close. As a result, a “day 0” return may reflect information or expectations formed before the call rather than the call itself, making it important to define the event window ([0,+1]) carefully and account for weekends and holidays.\n",
    "3. **Linguistic and formatting variation** - Transcripts differ in speaker labeling, punctuation, and inclusion of boilerplate disclaimers or operator remarks. These inconsistencies may distort tone-scoring unless the text is systematically cleaned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd8332-27e9-4c25-9b0f-1c7e4b4f816a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Method\n",
    "Step 1: Load and prepare data\n",
    "- Load the three datasets (earnings call transcripts, stock prices, and SPY benchmark) using pandas\n",
    "- Standardize date formats and align all data by ticker and date.\n",
    "- Functions (data-manipulation): load_data(), standardize_dates()\n",
    "- Tests: Use small 3–5 row samples to confirm correct data types and successful merges.\n",
    "- Output: Three clean DataFrames with properly formatted and aligned dates.\n",
    "- Connection: Establishes base for the Multiple Datasets challenge goal by merging separate data sources.\n",
    "\n",
    "\n",
    "Step 2: Clean transcripts and compute tone features\n",
    "- Use Python’s built-in re library to remove punctuation, lowercase text, and normalize spacing.\n",
    "- Count occurrences of positive, negative, and uncertainty words using the Loughran–McDonald financial dictionary.\n",
    "- Calculate each tone category as a percentage of total words in the transcript.\n",
    "- Functions (data-manipulation): clean_text(), compute_tone_scores()\n",
    "- Tests: Verify results on short sample texts (“profits increased,” “uncertain outlook”) with known word counts.\n",
    "- Output: Dataset with tone metrics (pos_pct, neg_pct, uncert_pct) for each earnings call.\n",
    "- Connection: Creates the independent variables used in hypothesis testing (RQ1 and RQ2).\n",
    "\n",
    "\n",
    "Step 3: Compute event-window and abnormal returns\n",
    "- Compute daily returns for each stock and for SPY using adjusted close prices.\n",
    "-Define Day 0 as the first trading day on or after the call date and Day +1 as the following trading day.\n",
    "- Calculate abnormal returns as firm return minus SPY return, then sum over [0,+1] to get cumulative abnormal return (CAR).\n",
    "- Functions (data-manipulation): compute_returns(), compute_abnormal_returns()\n",
    "- Tests: Hand-check results on a small, synthetic dataset to confirm correct math and event-window handling.\n",
    "- Output: Event-level dataset linking each call to its short-window abnormal return.\n",
    "- Connection: Provides the dependent variable for statistical testing and supports Multiple Datasets.\n",
    "\n",
    "Step 4: Merge tone and return data\n",
    "- Merge tone metrics with event returns and add basic controls such as sector and firm-size proxies.\n",
    "- Functions (data-manipulation): merge_features(), add_controls()\n",
    "- Tests: Ensure one row per event after merging and confirm correct ticker/date alignment.\n",
    "- Output: Combined dataset ready for modeling.\n",
    "- Connection: Prepares data for hypothesis testing (RQ1 and RQ2).\n",
    "\n",
    "Step 5: Hypothesis testing and modeling\n",
    "- Run regression models using statsmodels to test whether tone predicts short-term abnormal returns:\n",
    "     car_0p1 ~ pos_pct + neg_pct + uncert_pct + sector + size_proxy\n",
    "- Evaluate coefficients, p-values, and confidence intervals to test significance.\n",
    "- Adjust for multiple comparisons (e.g., Benjamini–Hochberg correction) if running across multiple tone types or sectors.\n",
    "- Functions (data-manipulation): fit_model(), summarize_results()\n",
    "- Tests: Use synthetic data with known relationships to confirm correct coefficient direction and model behavior.\n",
    "- Interpretation:\n",
    "- - RQ1: Positive coefficients on pos_pct or negative on neg_pct indicate tone predicts abnormal returns.\n",
    "- - RQ2: Interaction terms or coefficient differences by sector suggest heterogeneity.\n",
    "- Connection: Directly achieves the Statistical Hypothesis Testing challenge goal.\n",
    "\n",
    "Step 6: Visualization\n",
    "- Create plots to display tone distributions, tone vs. return relationships, and regression coefficients.\n",
    "- Functions (plotting): plot_tone_vs_returns(), plot_coefficients()\n",
    "- Tests: No formal testing; figures checked visually for accuracy and clarity.\n",
    "- Output: Visual confirmation of tone–return relationships.\n",
    "- Connection: Helps interpret quantitative results for RQ1 and RQ2.\n",
    "\n",
    "\n",
    "*Step 7: Robustness and reporting (optional)*\n",
    "- *Re-run models using alternative event windows ([−1,+1] or [0,+5]) to confirm consistency.*\n",
    "- *Winsorize extreme returns to check for sensitivity to outliers.*\n",
    "- *Save outputs, figures, and summary tables for reporting.*\n",
    "- *Connection: Provides robustness checks for RQ1 and RQ3, ensuring conclusions are not window-dependent.*\n",
    "\n",
    "#### Plan\n",
    "\n",
    "The project will be completed in JupyterHub and divided into five main tasks, each designed to be clear, independent, and reproducible.\n",
    "\n",
    "\n",
    "1) Setup and data preparation (2 hours): I will create an organized folder structure in JupyterHub with subfolders for raw data, processed data, figures, and reports. After confirming the environment setup, I will load the earnings call transcripts, stock prices, and SPY benchmark data using pandas. During this step, I will standardize date formats, check for missing or duplicated keys, and ensure that tickers and dates align across datasets to prepare for merging.\n",
    "\n",
    "\n",
    "2) Text cleaning and tone computation (3 hours): Using Python’s re library, I will remove punctuation, normalize spacing, and lowercase the transcript text. I will then apply the Loughran-McDonald financial dictionary or the spaCy API to calculate the percentage of positive, negative, and uncertainty words for each transcript. The resulting tone features will be saved as a separate dataset and tested on a small subset of text examples to confirm accuracy.\n",
    "\n",
    "\n",
    "3) Return calculations and event-window construction (3 hours): I will compute daily returns for both individual tickers and the SPY benchmark. For each earnings call, I will define the event window as [0,+1], where Day 0 represents the first trading day on or after the call. Abnormal returns will be calculated as the firm’s return minus SPY’s return, and cumulative abnormal returns (CAR) will be saved for each event. Manual checks on a small synthetic dataset will verify the accuracy of these calculations.\n",
    "\n",
    "\n",
    "4) Merging, modeling, and hypothesis testing (5 hours): I will merge the tone dataset with abnormal returns and add control variables such as industry sector and firm size proxies (e.g., log of average volume). Using statsmodels, I will run regression models to test whether tone predicts short-window abnormal returns while controlling for other factors. I will interpret coefficients, p-values, and confidence intervals directly in the context of the research questions.\n",
    "\n",
    "\n",
    "5) Visualization and reporting (~3 hours): The final step will involve creating plots to display the distribution of tone features, the relationship between tone and abnormal returns, and regression coefficients with confidence intervals. If time allows, I will perform quick robustness checks such as alternate event windows or light outlier filtering. All intermediate results, figures, and tables will be saved for reproducibility.\n",
    "\n",
    "\n",
    "*This plan builts in buffer time and may be an overestimation*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dab228-c348-4832-897e-0cb08da79783",
   "metadata": {},
   "source": [
    "## EDA Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81162ef9-cf6b-4cd1-8a69-e5e76df90bc4",
   "metadata": {},
   "source": [
    "lalalla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf174a32-5149-4480-905a-6f6e0f5a1164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import doctest\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "from lmd_loader import load_masterdictionary # from custom python file\n",
    "from pandas import DataFrame # for type annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf13c8e-3b9b-4cdf-b24b-bb5b0373cb1e",
   "metadata": {},
   "source": [
    "### Initial Preparation and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cbfc1b-7766-41d3-9e48-75072cc4053c",
   "metadata": {},
   "source": [
    "#### I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e33f3e32-88d7-44e3-9a02-4e236f77086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earning Reports - cannot limit pickle read\n",
    "def read_er_pkl(path: str, head_rows=None) -> DataFrame:\n",
    "    '''Load the earnings report pickle file into a df.'''\n",
    "    with open(path, 'rb') as file:\n",
    "        df = pickle.load(file)\n",
    "    return df.head(head_rows) if head_rows else df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4d0ff9-3fac-4bda-89c0-fc1508566de7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Depracated IO below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d105b481-9f2a-4da7-999e-3a6c6eb407d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NASDAQ OHLCV\n",
    "# def read_nasdaq_folder(folder_path, pattern='*csv', limit_files=None, nrows=None):\n",
    "#     '''\n",
    "#     '''\n",
    "#     files = list(Path(folder_path).glob(pattern))\n",
    "#     if limit_files is not None:\n",
    "#         files = files[:limit_files]\n",
    "#     df_list = [pd.read_csv(file, nrows=nrows) for file in files]\n",
    "#     return pd.concat(df_list, ignore_index=True) if df_list else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5afb63bf-aeea-4646-a7fd-ef261d2d7996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1377"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# files = list(Path('data/nasdaq_prices').glob('*.csv'))\n",
    "# len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94b98ca-7ddd-43ee-99dd-837292a4c3b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed1906b5-c00e-4ba0-a1eb-4456b885c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earning Reports Date Conversion\n",
    "def er_transform(df: DataFrame, src_col: str ='date', out_col: str ='date_std') -> DataFrame:\n",
    "    '''\n",
    "    Given a dataset, returns a dataset with a cleaned and standardized data column. Receives \n",
    "    an input for the source date column, and the name for the outputted standardized date column.\n",
    "    '''\n",
    "    out = df.copy()\n",
    "    date_clean = out[src_col].str.strip()\n",
    "    date_clean = date_clean.str.replace(\".\",\"\")\n",
    "    date_clean = date_clean.str.replace(\"ET\",\"\")\n",
    "    out[out_col] = pd.to_datetime(date_clean, format='mixed')\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65318029-90de-4f71-88f4-d190d8e11884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NASDAQ OHLCV + SPY Date Conversions\n",
    "def date_transform(df: DataFrame, src_col: str, out_col: str) -> DataFrame:\n",
    "    '''\n",
    "    Converts a column from string to datetime and stores it under a new name.\n",
    "    '''\n",
    "    out = df.copy()\n",
    "    out[out_col] = pd.to_datetime(out[src_col])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f358b7-fefb-40ad-a005-3e63068909d5",
   "metadata": {},
   "source": [
    "#### File Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d4790-2607-46c1-be25-8e925711e6ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Earnings Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4262ed9-84cc-47dd-9aa7-b78efdbacc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earnings Reports\n",
    "er_df = read_er_pkl(\"data/motley-fool-data.pkl\")\n",
    "er_df = er_transform(er_df, 'date', 'date_std')\n",
    "er_df = er_df.dropna(subset=['date_std']).copy()\n",
    "# below: removes the time stamps -> not included in function for separate use case later\n",
    "# --> use times for adjusted window calcs in final\n",
    "er_df['date_std'] = er_df['date_std'].dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3731a3-6f86-43e1-9d3e-4ac17764dd2f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### NASDAQ OHLCV\n",
    "Simplified loading process that reduces memory usage by focusing on relevant tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9189576-4754-41de-aa37-7c30dad3bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_ohlcv(file: str) -> DataFrame:\n",
    "    \"\"\"Load a single OHLCV file with ticker column.\"\"\"\n",
    "    df = pd.read_csv(file)\n",
    "    if 'ticker' not in df.columns:\n",
    "        df['ticker'] = file.stem.upper()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14aa9882-0da7-4bf5-9228-d495b1ef7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_relevant_ohlcv(ohlcv_folder: str, er_df: DataFrame, prog_interval: int =100) -> DataFrame:\n",
    "    '''\n",
    "    Returns a df with cols [ticker, date_std, open, high, low, close],\n",
    "    after loading OHLCV data only for tickers that appear in the inputted\n",
    "    earnings report. Reduces storage significantly.\n",
    "    '''\n",
    "\n",
    "    rel_tickers = set(er_df['ticker'].str.upper()) # standardized to string and upper for comparison\n",
    "    print(f\"Found {len(rel_tickers)} unique tickers in earnings reports\")\n",
    "\n",
    "    files = list(Path(ohlcv_folder).glob('*.csv'))\n",
    "\n",
    "    rel_files = []\n",
    "    for file in files:\n",
    "        #pulls ticker names\n",
    "        ticker = file.stem.upper()\n",
    "        if ticker in rel_tickers:\n",
    "            rel_files.append(file)\n",
    "    \n",
    "    print(f\"{len(rel_files)} overlapping tickers in OHLCV\")\n",
    "\n",
    "    # full file loading\n",
    "    df_list = [load_single_ohlcv(f) for f in rel_files]\n",
    "\n",
    "    combined = pd.concat(df_list, ignore_index=True)\n",
    "    combined = date_transform(combined, 'date', 'date_std')\n",
    "    \n",
    "    cols_to_keep = ['ticker', 'date_std', 'open', 'high', 'low', 'close']\n",
    "    combined = combined[cols_to_keep]\n",
    "\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0b2d307-745d-489b-8a4c-03added646df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2869 unique tickers in earnings reports\n",
      "425 overlapping tickers in OHLCV\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date_std</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AXGN</td>\n",
       "      <td>1986-12-17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AXGN</td>\n",
       "      <td>1986-12-18</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AXGN</td>\n",
       "      <td>1986-12-19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AXGN</td>\n",
       "      <td>1986-12-22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AXGN</td>\n",
       "      <td>1986-12-23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.6700</td>\n",
       "      <td>3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231412</th>\n",
       "      <td>CHEF</td>\n",
       "      <td>2025-10-27</td>\n",
       "      <td>59.59</td>\n",
       "      <td>59.96</td>\n",
       "      <td>58.2250</td>\n",
       "      <td>58.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231413</th>\n",
       "      <td>CHEF</td>\n",
       "      <td>2025-10-28</td>\n",
       "      <td>58.06</td>\n",
       "      <td>60.44</td>\n",
       "      <td>57.1291</td>\n",
       "      <td>58.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231414</th>\n",
       "      <td>CHEF</td>\n",
       "      <td>2025-10-29</td>\n",
       "      <td>63.42</td>\n",
       "      <td>63.61</td>\n",
       "      <td>58.8500</td>\n",
       "      <td>62.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231415</th>\n",
       "      <td>CHEF</td>\n",
       "      <td>2025-10-30</td>\n",
       "      <td>62.35</td>\n",
       "      <td>62.82</td>\n",
       "      <td>59.4500</td>\n",
       "      <td>60.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231416</th>\n",
       "      <td>CHEF</td>\n",
       "      <td>2025-10-31</td>\n",
       "      <td>59.13</td>\n",
       "      <td>60.77</td>\n",
       "      <td>58.3500</td>\n",
       "      <td>58.98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2231417 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ticker   date_std   open   high      low  close\n",
       "0         AXGN 1986-12-17   0.00   4.10   4.0000   4.00\n",
       "1         AXGN 1986-12-18   0.00   4.10   4.0000   4.00\n",
       "2         AXGN 1986-12-19   0.00   4.10   4.0000   4.00\n",
       "3         AXGN 1986-12-22   0.00   4.10   4.0000   4.00\n",
       "4         AXGN 1986-12-23   0.00   3.89   3.6700   3.67\n",
       "...        ...        ...    ...    ...      ...    ...\n",
       "2231412   CHEF 2025-10-27  59.59  59.96  58.2250  58.33\n",
       "2231413   CHEF 2025-10-28  58.06  60.44  57.1291  58.61\n",
       "2231414   CHEF 2025-10-29  63.42  63.61  58.8500  62.36\n",
       "2231415   CHEF 2025-10-30  62.35  62.82  59.4500  60.02\n",
       "2231416   CHEF 2025-10-31  59.13  60.77  58.3500  58.98\n",
       "\n",
       "[2231417 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohlcv_df = load_relevant_ohlcv('data/nasdaq_prices', er_df)\n",
    "ohlcv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ea6ea-df44-496c-acfe-aaf3aa3e3402",
   "metadata": {},
   "source": [
    "##### SPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "302a2e53-1b92-4a28-8ad1-42b5ff37273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_df = pd.read_csv('data/SPY.csv')\n",
    "spy_df = date_transform(spy_df, 'Date', 'date_std')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b4cc3c-233c-4ddc-9c60-8bbfbb299cb9",
   "metadata": {},
   "source": [
    "##### LMD (Sentiment Dict) implementation\n",
    "Implemented using the lmd_loader.py script (pulled from the [official script](https://drive.google.com/file/d/18jbZ3o17PRI_s4xG9UslKnGMpnC1ZoLM/view))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "30cc4436-ce82-48b7-96f3-6d7b478454da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ...Loading Master Dictionary 85,000\n",
      "Master Dictionary loaded from file:\n",
      "  data/Loughran-McDonald_MasterDictionary_1993-2024.csv\n",
      "\n",
      "  master_dictionary has 86,553 words.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Dictionary\n",
    "lmd_path = 'data/Loughran-McDonald_MasterDictionary_1993-2024.csv'\n",
    "\n",
    "# load everything\n",
    "# vars identified in return stmnt at bottom of LMD loader, incl params for logging\n",
    "master_dict, md_header, sentiment_categories, sentiment_dicts, stopwords, total_docs = \\\n",
    "    load_masterdictionary(lmd_path, print_flag=True, f_log=None, get_other=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7342e926-a833-4a6f-8693-71cdbec21f53",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### *Tests/Checks - Initial load and cleaning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "517f5082-db56-445e-bbd1-c27807556f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'date_std' in er_df.columns\n",
    "assert 'ticker' in er_df.columns\n",
    "\n",
    "assert 'date_std' in ohlcv_df.columns\n",
    "assert 'ticker' in ohlcv_df.columns\n",
    "assert 'date_std' in spy_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d852a06b-0344-4bc6-9731-cb271cc6652e",
   "metadata": {},
   "source": [
    "#### Stock Return Calcs\n",
    "*Similar Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c069cbb8-fa4f-4149-aea7-9f8b13bd3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_returns_calc(df: DataFrame, price_col: str, date_col: str, group_col: str = None, return_col: str = None) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Computes daily returns given an input df, price col, date col, and grouping column,\n",
    "    using a simple percent change formula. Returns a df that retains the calculated\n",
    "    returns, that is sorted. \n",
    "    \"\"\"\n",
    "    # asserts for input verification?\n",
    "    assert price_col in df.columns, f\"Column '{price_col}' not found\"\n",
    "    assert date_col in df.columns, f\"Column '{date_col}' not found\"\n",
    "    assert len(df) > 0, \"DataFrame cannot be empty\"\n",
    "    \n",
    "    out = df.copy()\n",
    "\n",
    "    if group_col is None:\n",
    "        sort_cols = [date_col]\n",
    "        out = out.sort_values(sort_cols)\n",
    "        out[return_col] = out[price_col].pct_change()\n",
    "    else:\n",
    "        sort_cols = [group_col, date_col]\n",
    "        out = out.sort_values(sort_cols)\n",
    "        out[return_col] = out.groupby(group_col)[price_col].pct_change()\n",
    "\n",
    "    assert out[return_col].notna().sum() > 0, \"All returns are NaN\"\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b005d20b-1278-4975-8bdd-81dfc68cc59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def event_window_car(merged_df: DataFrame, window_days: int = 2) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Computes cumulative abnormal return (CAR) over [0, +1] event window.\n",
    "    Abnormal return = stock return - SPY return\n",
    "    \n",
    "    **NOTE** This is simplified - assumes next calendar day = next trading day;\n",
    "    full version will handle weekends/holidays properly.\n",
    "    \"\"\"\n",
    "    out = merged_df.copy()\n",
    "    \n",
    "    # For each earnings call, gets returns on day 0 and day +1\n",
    "    # Simplified v1: just gets  abnormal return on the call date\n",
    "    out['abnormal_return'] = out['return'] - out['spy_return']\n",
    "    \n",
    "    # **EDA using single-day abnormal return as proxy for CAR**\n",
    "    out['car_0p1'] = out['abnormal_return']  # Placeholder\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "100c89ea-7624-4f0b-a1fc-b5079bb126f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlcv_df = daily_returns_calc(ohlcv_df, 'close', 'date_std', 'ticker', 'return')\n",
    "spy_returns = daily_returns_calc(spy_df, 'Close', 'date_std', return_col='spy_return')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6314c3a6-3e8e-4712-86ee-693547a5c2b6",
   "metadata": {},
   "source": [
    "#### Data Merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "576771cf-3581-461d-b29d-6020d54519ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## move this to initial file loading etc!!\n",
    "ohlcv_df = ohlcv_df.copy()\n",
    "if 'datetime64' in str(ohlcv_df['date_std'].dtype): #can delete IF stmnt once in initial load\n",
    "    ohlcv_df['date_std'] = ohlcv_df['date_std'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d60cafcf-07c1-49a4-b48e-4055584ca0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>exchange</th>\n",
       "      <th>q</th>\n",
       "      <th>ticker</th>\n",
       "      <th>transcript</th>\n",
       "      <th>date_std</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>return</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aug 27, 2020, 9:00 p.m. ET</td>\n",
       "      <td>NASDAQ: BILI</td>\n",
       "      <td>2020-Q2</td>\n",
       "      <td>BILI</td>\n",
       "      <td>Prepared Remarks:\\nOperator\\nGood day, and wel...</td>\n",
       "      <td>2020-08-27</td>\n",
       "      <td>45.7835</td>\n",
       "      <td>46.3800</td>\n",
       "      <td>43.8000</td>\n",
       "      <td>45.1400</td>\n",
       "      <td>-0.073481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nov 6, 2019, 12:00 p.m. ET</td>\n",
       "      <td>NASDAQ: BBSI</td>\n",
       "      <td>2019-Q3</td>\n",
       "      <td>BBSI</td>\n",
       "      <td>Prepared Remarks:\\nOperator\\nGood day, everyon...</td>\n",
       "      <td>2019-11-06</td>\n",
       "      <td>86.9544</td>\n",
       "      <td>90.3949</td>\n",
       "      <td>84.1391</td>\n",
       "      <td>88.6084</td>\n",
       "      <td>0.087974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aug 7, 2019, 8:30 a.m. ET</td>\n",
       "      <td>NASDAQ: CSTE</td>\n",
       "      <td>2019-Q2</td>\n",
       "      <td>CSTE</td>\n",
       "      <td>Prepared Remarks:\\nOperator\\nGreetings and wel...</td>\n",
       "      <td>2019-08-07</td>\n",
       "      <td>12.5193</td>\n",
       "      <td>14.6062</td>\n",
       "      <td>12.0945</td>\n",
       "      <td>14.5777</td>\n",
       "      <td>0.083417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nov 06, 2019, 4:30 p.m. ET</td>\n",
       "      <td>NASDAQ: DXCM</td>\n",
       "      <td>2019-Q3</td>\n",
       "      <td>DXCM</td>\n",
       "      <td>Prepared Remarks:\\nOperator\\nWelcome to the De...</td>\n",
       "      <td>2019-11-06</td>\n",
       "      <td>38.4975</td>\n",
       "      <td>39.3138</td>\n",
       "      <td>38.0662</td>\n",
       "      <td>38.2800</td>\n",
       "      <td>-0.002865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feb 10, 2021, 9:00 a.m. ET</td>\n",
       "      <td>NASDAQ: EEFT</td>\n",
       "      <td>2020-Q4</td>\n",
       "      <td>EEFT</td>\n",
       "      <td>Prepared Remarks:\\nOperator\\nGreetings, and we...</td>\n",
       "      <td>2021-02-10</td>\n",
       "      <td>136.9200</td>\n",
       "      <td>145.3400</td>\n",
       "      <td>136.6900</td>\n",
       "      <td>141.1000</td>\n",
       "      <td>0.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2868</th>\n",
       "      <td>Feb 24, 2022, 5:00 p.m. ET</td>\n",
       "      <td>NASDAQ: ABCL</td>\n",
       "      <td>2021-Q4</td>\n",
       "      <td>ABCL</td>\n",
       "      <td>Prepared Remarks:\\nOperator\\nLadies and gentle...</td>\n",
       "      <td>2022-02-24</td>\n",
       "      <td>7.9100</td>\n",
       "      <td>8.4900</td>\n",
       "      <td>7.8500</td>\n",
       "      <td>8.4500</td>\n",
       "      <td>0.023002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2869</th>\n",
       "      <td>Aug 12, 2021, 4:30 p.m. ET</td>\n",
       "      <td>NASDAQ: AVXL</td>\n",
       "      <td>2021-Q3</td>\n",
       "      <td>AVXL</td>\n",
       "      <td>Prepared Remarks:\\nOperator\\nGood afternoon. M...</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>18.0300</td>\n",
       "      <td>19.6800</td>\n",
       "      <td>17.6800</td>\n",
       "      <td>19.2700</td>\n",
       "      <td>0.070556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2870</th>\n",
       "      <td>Jul 21, 2022, 11:00 a.m. ET</td>\n",
       "      <td>NASDAQ: BANR</td>\n",
       "      <td>2022-Q2</td>\n",
       "      <td>BANR</td>\n",
       "      <td>Prepared Remarks:\\nOperator\\nThank you all and...</td>\n",
       "      <td>2022-07-21</td>\n",
       "      <td>59.3659</td>\n",
       "      <td>59.3659</td>\n",
       "      <td>56.7114</td>\n",
       "      <td>58.4778</td>\n",
       "      <td>-0.010186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2871</th>\n",
       "      <td>Aug 04, 2022, 5:00 p.m. ET</td>\n",
       "      <td>NASDAQ: DH</td>\n",
       "      <td>2022-Q2</td>\n",
       "      <td>DH</td>\n",
       "      <td>Prepared Remarks:\\nOperator\\nGood day, and wel...</td>\n",
       "      <td>2022-08-04</td>\n",
       "      <td>29.4000</td>\n",
       "      <td>30.1100</td>\n",
       "      <td>28.0700</td>\n",
       "      <td>29.3300</td>\n",
       "      <td>0.001366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2872</th>\n",
       "      <td>Feb 28, 2022, 4:30 p.m. ET</td>\n",
       "      <td>NASDAQ: DVAX</td>\n",
       "      <td>2021-Q4</td>\n",
       "      <td>DVAX</td>\n",
       "      <td>Prepared Remarks:\\nOperator\\nGood day, ladies ...</td>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>12.3800</td>\n",
       "      <td>12.7000</td>\n",
       "      <td>12.0800</td>\n",
       "      <td>12.2600</td>\n",
       "      <td>-0.017628</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2873 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             date      exchange        q ticker  \\\n",
       "0      Aug 27, 2020, 9:00 p.m. ET  NASDAQ: BILI  2020-Q2   BILI   \n",
       "1      Nov 6, 2019, 12:00 p.m. ET  NASDAQ: BBSI  2019-Q3   BBSI   \n",
       "2       Aug 7, 2019, 8:30 a.m. ET  NASDAQ: CSTE  2019-Q2   CSTE   \n",
       "3      Nov 06, 2019, 4:30 p.m. ET  NASDAQ: DXCM  2019-Q3   DXCM   \n",
       "4      Feb 10, 2021, 9:00 a.m. ET  NASDAQ: EEFT  2020-Q4   EEFT   \n",
       "...                           ...           ...      ...    ...   \n",
       "2868   Feb 24, 2022, 5:00 p.m. ET  NASDAQ: ABCL  2021-Q4   ABCL   \n",
       "2869   Aug 12, 2021, 4:30 p.m. ET  NASDAQ: AVXL  2021-Q3   AVXL   \n",
       "2870  Jul 21, 2022, 11:00 a.m. ET  NASDAQ: BANR  2022-Q2   BANR   \n",
       "2871   Aug 04, 2022, 5:00 p.m. ET    NASDAQ: DH  2022-Q2     DH   \n",
       "2872   Feb 28, 2022, 4:30 p.m. ET  NASDAQ: DVAX  2021-Q4   DVAX   \n",
       "\n",
       "                                             transcript    date_std      open  \\\n",
       "0     Prepared Remarks:\\nOperator\\nGood day, and wel...  2020-08-27   45.7835   \n",
       "1     Prepared Remarks:\\nOperator\\nGood day, everyon...  2019-11-06   86.9544   \n",
       "2     Prepared Remarks:\\nOperator\\nGreetings and wel...  2019-08-07   12.5193   \n",
       "3     Prepared Remarks:\\nOperator\\nWelcome to the De...  2019-11-06   38.4975   \n",
       "4     Prepared Remarks:\\nOperator\\nGreetings, and we...  2021-02-10  136.9200   \n",
       "...                                                 ...         ...       ...   \n",
       "2868  Prepared Remarks:\\nOperator\\nLadies and gentle...  2022-02-24    7.9100   \n",
       "2869  Prepared Remarks:\\nOperator\\nGood afternoon. M...  2021-08-12   18.0300   \n",
       "2870  Prepared Remarks:\\nOperator\\nThank you all and...  2022-07-21   59.3659   \n",
       "2871  Prepared Remarks:\\nOperator\\nGood day, and wel...  2022-08-04   29.4000   \n",
       "2872  Prepared Remarks:\\nOperator\\nGood day, ladies ...  2022-02-28   12.3800   \n",
       "\n",
       "          high       low     close    return  \n",
       "0      46.3800   43.8000   45.1400 -0.073481  \n",
       "1      90.3949   84.1391   88.6084  0.087974  \n",
       "2      14.6062   12.0945   14.5777  0.083417  \n",
       "3      39.3138   38.0662   38.2800 -0.002865  \n",
       "4     145.3400  136.6900  141.1000  0.010600  \n",
       "...        ...       ...       ...       ...  \n",
       "2868    8.4900    7.8500    8.4500  0.023002  \n",
       "2869   19.6800   17.6800   19.2700  0.070556  \n",
       "2870   59.3659   56.7114   58.4778 -0.010186  \n",
       "2871   30.1100   28.0700   29.3300  0.001366  \n",
       "2872   12.7000   12.0800   12.2600 -0.017628  \n",
       "\n",
       "[2873 rows x 11 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uses inner join to merge pricing and report data (post ticker filter)\n",
    "merged = er_df.merge(ohlcv_df, on=['ticker', 'date_std'], how='inner')\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2fa69c-eec6-44f7-948a-9790a00ab34a",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b777545-7cbf-4449-8bed-9954658e45b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['negative', 'positive', 'uncertainty', 'litigious', 'strong_modal', 'weak_modal', 'constraining', 'complexity'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_dicts.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec81a187-86a5-4a71-aa77-fdb405e488fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "354"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentiment_dicts['positive'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9038ad8-f3aa-41a0-a625-dc3b585b664f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Q: How large is the dataset?\n",
    "The datasets combine three complementary sources: corporate earnings-call transcripts, daily stock prices, and a market benchmark. The earnings-report data includes 18,755 rows and 6 columns, where each row represents a single company’s earnings call on a specific date. Columns contain identifiers (ticker, date_std), the full transcript_text,  sentiment token scores, and supporting metadata such as company name or file origin. This dataset serves as the textual foundation for tone analysis, capturing the language used by executives during calls.\n",
    "\n",
    "The market price data (OHLCV) comprises 133,557 rows and 7 columns, with each row corresponding to one trading-day observation for a given firm (delineated by ticker). Columns record standard financial attributes: open, high, low, close, and volume. These values will later be used to calculate daily and event-window returns. For the benchmark dataset, it is drawn from the S&P 500 ETF (SPY), includes 7,703 rows and 8 columns, with each row representing one trading day for the broader market. It mirrors the OHLCV structure (with the addition of adj_close) and provides the baseline for measuring abnormal returns.\n",
    "\n",
    "Note: In this EDA, the OHLCV table was filtered immediately at load time (file-level limitation) for memory control purposes -> i.e., we did not first load the full datset and then filter by transcript overlap, like we will in later iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aedf507c-cc8a-4250-9c60-7d494965f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data checks\n",
    "required_cols = {\n",
    "    \"er_df\": {\"ticker\", \"date_std\"},\n",
    "    \"ohlcv_df\": {\"ticker\", \"date_std\"},\n",
    "    \"spy_df\": {\"date_std\"},\n",
    "}\n",
    "\n",
    "dfs = [(\"er_df\", er_df), (\"ohlcv_df\", ohlcv_df), (\"spy_df\", spy_df)]\n",
    "\n",
    "for name, df in dfs:\n",
    "    missing = required_cols[name] - set(df.columns)\n",
    "    assert not missing, f\"{name} missing columns: {missing}\"\n",
    "    assert pd.api.types.is_datetime64_any_dtype(df[\"date_std\"]), f\"{name} date_std must be datetime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "094bad71-edb5-4e6b-a56e-5faf06146300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ER (earnings reports) shape: 18755 rows × 6 cols\n",
      "OHLCV shape: 2240958 rows × 6 cols\n",
      "SPY shape: 7703 rows × 8 cols\n",
      "ER preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>exchange</th>\n",
       "      <th>q</th>\n",
       "      <th>ticker</th>\n",
       "      <th>transcript</th>\n",
       "      <th>date_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aug 27, 2020, 9:00 p.m. ET</td>\n",
       "      <td>NASDAQ: BILI</td>\n",
       "      <td>2020-Q2</td>\n",
       "      <td>BILI</td>\n",
       "      <td>Prepared Remarks:\\nOperator\\nGood day, and wel...</td>\n",
       "      <td>2020-08-27 21:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jul 30, 2020, 4:30 p.m. ET</td>\n",
       "      <td>NYSE: GFF</td>\n",
       "      <td>2020-Q3</td>\n",
       "      <td>GFF</td>\n",
       "      <td>Prepared Remarks:\\nOperator\\nThank you for sta...</td>\n",
       "      <td>2020-07-30 16:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oct 23, 2019, 5:00 p.m. ET</td>\n",
       "      <td>NASDAQ: LRCX</td>\n",
       "      <td>2020-Q1</td>\n",
       "      <td>LRCX</td>\n",
       "      <td>Prepared Remarks:\\nOperator\\nGood day and welc...</td>\n",
       "      <td>2019-10-23 17:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         date      exchange        q ticker  \\\n",
       "0  Aug 27, 2020, 9:00 p.m. ET  NASDAQ: BILI  2020-Q2   BILI   \n",
       "1  Jul 30, 2020, 4:30 p.m. ET     NYSE: GFF  2020-Q3    GFF   \n",
       "2  Oct 23, 2019, 5:00 p.m. ET  NASDAQ: LRCX  2020-Q1   LRCX   \n",
       "\n",
       "                                          transcript            date_std  \n",
       "0  Prepared Remarks:\\nOperator\\nGood day, and wel... 2020-08-27 21:00:00  \n",
       "1  Prepared Remarks:\\nOperator\\nThank you for sta... 2020-07-30 16:30:00  \n",
       "2  Prepared Remarks:\\nOperator\\nGood day and welc... 2019-10-23 17:00:00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OHLCV preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date_std</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AXGN</td>\n",
       "      <td>1986-12-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AXGN</td>\n",
       "      <td>1986-12-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AXGN</td>\n",
       "      <td>1986-12-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ticker   date_std  open  high  low  close\n",
       "0   AXGN 1986-12-17   0.0   4.1  4.0    4.0\n",
       "1   AXGN 1986-12-18   0.0   4.1  4.0    4.0\n",
       "2   AXGN 1986-12-19   0.0   4.1  4.0    4.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPY preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>date_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1993-01-29</td>\n",
       "      <td>43.96875</td>\n",
       "      <td>43.96875</td>\n",
       "      <td>43.75000</td>\n",
       "      <td>43.93750</td>\n",
       "      <td>25.029377</td>\n",
       "      <td>1003200</td>\n",
       "      <td>1993-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1993-02-01</td>\n",
       "      <td>43.96875</td>\n",
       "      <td>44.25000</td>\n",
       "      <td>43.96875</td>\n",
       "      <td>44.25000</td>\n",
       "      <td>25.207405</td>\n",
       "      <td>480500</td>\n",
       "      <td>1993-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1993-02-02</td>\n",
       "      <td>44.21875</td>\n",
       "      <td>44.37500</td>\n",
       "      <td>44.12500</td>\n",
       "      <td>44.34375</td>\n",
       "      <td>25.260784</td>\n",
       "      <td>201300</td>\n",
       "      <td>1993-02-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Open      High       Low     Close  Adj Close   Volume  \\\n",
       "0  1993-01-29  43.96875  43.96875  43.75000  43.93750  25.029377  1003200   \n",
       "1  1993-02-01  43.96875  44.25000  43.96875  44.25000  25.207405   480500   \n",
       "2  1993-02-02  44.21875  44.37500  44.12500  44.34375  25.260784   201300   \n",
       "\n",
       "    date_std  \n",
       "0 1993-01-29  \n",
       "1 1993-02-01  \n",
       "2 1993-02-02  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data size summary\n",
    "print(f\"ER (earnings reports) shape: {er_df.shape[0]} rows × {er_df.shape[1]} cols\")\n",
    "print(f\"OHLCV shape: {ohlcv_df.shape[0]} rows × {ohlcv_df.shape[1]} cols\")\n",
    "print(f\"SPY shape: {spy_df.shape[0]} rows × {spy_df.shape[1]} cols\")\n",
    "\n",
    "print(\"ER preview:\")\n",
    "display(er_df.head(3))\n",
    "print(\"OHLCV preview:\")\n",
    "display(ohlcv_df.head(3))\n",
    "print(\"SPY preview:\")\n",
    "display(spy_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daf5f26-23f3-43f4-b866-99610e883726",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Does the dataset have any missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff4608-f8ae-4af3-93c9-257c73846a1b",
   "metadata": {},
   "source": [
    "Looking at the datasets, the missingness analysis shows that only the Earnings Reports (ER) dataset contained missing data. Specifically, the column date_std had 380 missing values, accounting for approximately 2.03% of all rows. These missing entries likely stem from parsing issues or inconsistent metadata in the raw date column that prevented some dates from being successfully standardized. Because the date_std column is a critical key for aligning transcripts with market data and defining event windows, these rows cannot be reliably used in downstream analysis.\n",
    "\n",
    "For this exploratory phase, we will remove the affected rows, as their proportion is small enough that exclusion will not be materially affecting results or representativeness of the data. The OHLCV and SPY datasets showed no missing values, indicating that the market data and benchmark series are structurally complete and ready for use in return computations. In future iterations, we may implement a more robust date-parsing procedure to recover these records rather than discarding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0bce1bde-fd08-45bd-b2d8-349f3a9336e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Return a DataFrame of columns with missing values and their %.\n",
    "    Always returns columns: ['missing_count','missing_pct'] -> they \n",
    "    may be empty.\n",
    "    '''\n",
    "    total = df.shape[0]\n",
    "    missing = df.isna().sum()\n",
    "    missing = missing[missing > 0]\n",
    "\n",
    "    if missing.empty:\n",
    "        print(f\"{name}: no missing values\")\n",
    "        return missing.to_frame(\"missing_count\").assign(\n",
    "            missing_pct=pd.Series(dtype=float)) # so that DF is returned even if there are no missing vals\n",
    "\n",
    "    result = (\n",
    "        missing.to_frame(\"missing_count\") \n",
    "        # .assign  adds a column in a chain - lambda=take the chained df and calc percent in new col\n",
    "        .assign(missing_pct=lambda x: (x[\"missing_count\"] / total * 100).round(2))\n",
    "        .sort_values(\"missing_pct\", ascending=False)) \n",
    "\n",
    "    print(f\"{name} missing data:\")\n",
    "    display(result)\n",
    "    print()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "148e2ff5-2647-4099-aa75-a1064f66c62c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earnings Reports (ER) missing data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>date_std</th>\n",
       "      <td>380</td>\n",
       "      <td>2.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          missing_count  missing_pct\n",
       "date_std            380         2.03"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OHLCV: no missing values\n",
      "SPY: no missing values\n"
     ]
    }
   ],
   "source": [
    "er_miss    = missing_data(er_df,    \"Earnings Reports (ER)\")\n",
    "ohlcv_miss   = missing_data(ohlcv_df,   \"OHLCV\")\n",
    "spy_miss   = missing_data(spy_df,   \"SPY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba46941-35cf-48a2-b799-f754f0a79ce9",
   "metadata": {},
   "source": [
    "### Variables of Interest\n",
    "Across the three datasets, the variables of interest collectively span the corporate language seen in earnings reports to market behavior. In the Earnings Reports (ER) data, each observation corresponds to one firm’s earnings call (ticker, date_std), with the transcript text (transcript) processed into quantitative tone features. token_count measures call length, while pos_pct, neg_pct, and uncert_pct represent the normalized share of positive, negative, and uncertainty-related words, respectively. These tone variables provide the linguistic inputs for assessing whether executive sentiment has an influence on short-term price reactions.\n",
    "\n",
    "The OHLCV dataset records daily market activity for each firm, including open, high, low, and close, as well as computed daily returns (ret) from adjusted closing prices. This data quantifies firm-level market response, serving as the behavioral side of the tone–return relationship.\n",
    "\n",
    "The SPY dataset represents the benchmark market index, structured identically to OHLCV but aggregated at the market level. Its daily return (ret) acts as the baseline for calculating abnormal returns. Together, these variables form a coherent system: linguistic tone captures firm-level communication sentiment, OHLCV captures firm-specific market behavior, and SPY provides the market context necessary for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c3d74992-e002-4601-9de9-a746e5765337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>token_count</th>\n",
       "      <td>18755.0</td>\n",
       "      <td>7863.904452</td>\n",
       "      <td>2511.699771</td>\n",
       "      <td>562.0</td>\n",
       "      <td>6121.000000</td>\n",
       "      <td>7961.000000</td>\n",
       "      <td>9502.500000</td>\n",
       "      <td>32286.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_pct</th>\n",
       "      <td>18755.0</td>\n",
       "      <td>0.005711</td>\n",
       "      <td>0.002916</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>0.005352</td>\n",
       "      <td>0.007455</td>\n",
       "      <td>0.025175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_pct</th>\n",
       "      <td>18755.0</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.011607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uncert_pct</th>\n",
       "      <td>18755.0</td>\n",
       "      <td>0.004685</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.004496</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>0.021007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count         mean          std    min          25%  \\\n",
       "token_count  18755.0  7863.904452  2511.699771  562.0  6121.000000   \n",
       "pos_pct      18755.0     0.005711     0.002916    0.0     0.003540   \n",
       "neg_pct      18755.0     0.001069     0.000984    0.0     0.000458   \n",
       "uncert_pct   18755.0     0.004685     0.001582    0.0     0.003583   \n",
       "\n",
       "                     50%          75%           max  \n",
       "token_count  7961.000000  9502.500000  32286.000000  \n",
       "pos_pct         0.005352     0.007455      0.025175  \n",
       "neg_pct         0.000816     0.001374      0.011607  \n",
       "uncert_pct      0.004496     0.005588      0.021007  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "er_summary = er_df[[\"token_count\",\"pos_pct\",\"neg_pct\",\"uncert_pct\"]].describe().T\n",
    "display(er_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9dcb8539-99fd-4f72-bffb-71a5650d6076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>open</th>\n",
       "      <td>133557.0</td>\n",
       "      <td>4.119939e+07</td>\n",
       "      <td>1.538108e+09</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.78</td>\n",
       "      <td>10.5900</td>\n",
       "      <td>25.50</td>\n",
       "      <td>9.281250e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>133557.0</td>\n",
       "      <td>4.301171e+07</td>\n",
       "      <td>1.608392e+09</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>5.49</td>\n",
       "      <td>11.2418</td>\n",
       "      <td>27.40</td>\n",
       "      <td>9.585000e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>133557.0</td>\n",
       "      <td>3.898297e+07</td>\n",
       "      <td>1.450471e+09</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>5.13</td>\n",
       "      <td>10.6874</td>\n",
       "      <td>25.55</td>\n",
       "      <td>8.100000e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>close</th>\n",
       "      <td>133557.0</td>\n",
       "      <td>4.142139e+07</td>\n",
       "      <td>1.546763e+09</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>5.30</td>\n",
       "      <td>10.9300</td>\n",
       "      <td>26.50</td>\n",
       "      <td>9.180000e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count          mean           std     min   25%      50%    75%  \\\n",
       "open   133557.0  4.119939e+07  1.538108e+09  0.0000  4.78  10.5900  25.50   \n",
       "high   133557.0  4.301171e+07  1.608392e+09  0.0274  5.49  11.2418  27.40   \n",
       "low    133557.0  3.898297e+07  1.450471e+09  0.0210  5.13  10.6874  25.55   \n",
       "close  133557.0  4.142139e+07  1.546763e+09  0.0222  5.30  10.9300  26.50   \n",
       "\n",
       "                max  \n",
       "open   9.281250e+10  \n",
       "high   9.585000e+10  \n",
       "low    8.100000e+10  \n",
       "close  9.180000e+10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ohlcv_summary = ohlcv_df[[\"open\",\"high\",\"low\",\"close\"]].describe().T\n",
    "display(ohlcv_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "594470ef-54a5-4e5e-b6b3-2b2d93ed3b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Open</th>\n",
       "      <td>7703.0</td>\n",
       "      <td>1.675022e+02</td>\n",
       "      <td>1.033269e+02</td>\n",
       "      <td>43.343750</td>\n",
       "      <td>1.047700e+02</td>\n",
       "      <td>1.316400e+02</td>\n",
       "      <td>2.088650e+02</td>\n",
       "      <td>4.792200e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>7703.0</td>\n",
       "      <td>1.685138e+02</td>\n",
       "      <td>1.038956e+02</td>\n",
       "      <td>43.531250</td>\n",
       "      <td>1.056125e+02</td>\n",
       "      <td>1.324400e+02</td>\n",
       "      <td>2.097850e+02</td>\n",
       "      <td>4.799800e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>7703.0</td>\n",
       "      <td>1.663896e+02</td>\n",
       "      <td>1.027221e+02</td>\n",
       "      <td>42.812500</td>\n",
       "      <td>1.038500e+02</td>\n",
       "      <td>1.306800e+02</td>\n",
       "      <td>2.078300e+02</td>\n",
       "      <td>4.760600e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Close</th>\n",
       "      <td>7703.0</td>\n",
       "      <td>1.675101e+02</td>\n",
       "      <td>1.033526e+02</td>\n",
       "      <td>43.406250</td>\n",
       "      <td>1.048800e+02</td>\n",
       "      <td>1.315600e+02</td>\n",
       "      <td>2.088500e+02</td>\n",
       "      <td>4.777100e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adj Close</th>\n",
       "      <td>7703.0</td>\n",
       "      <td>1.398787e+02</td>\n",
       "      <td>1.094186e+02</td>\n",
       "      <td>24.726746</td>\n",
       "      <td>7.082888e+01</td>\n",
       "      <td>9.370667e+01</td>\n",
       "      <td>1.812353e+02</td>\n",
       "      <td>4.665634e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volume</th>\n",
       "      <td>7703.0</td>\n",
       "      <td>8.451404e+07</td>\n",
       "      <td>9.280526e+07</td>\n",
       "      <td>5200.000000</td>\n",
       "      <td>9.742550e+06</td>\n",
       "      <td>6.263260e+07</td>\n",
       "      <td>1.168936e+08</td>\n",
       "      <td>8.710263e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count          mean           std          min           25%  \\\n",
       "Open       7703.0  1.675022e+02  1.033269e+02    43.343750  1.047700e+02   \n",
       "High       7703.0  1.685138e+02  1.038956e+02    43.531250  1.056125e+02   \n",
       "Low        7703.0  1.663896e+02  1.027221e+02    42.812500  1.038500e+02   \n",
       "Close      7703.0  1.675101e+02  1.033526e+02    43.406250  1.048800e+02   \n",
       "Adj Close  7703.0  1.398787e+02  1.094186e+02    24.726746  7.082888e+01   \n",
       "Volume     7703.0  8.451404e+07  9.280526e+07  5200.000000  9.742550e+06   \n",
       "\n",
       "                    50%           75%           max  \n",
       "Open       1.316400e+02  2.088650e+02  4.792200e+02  \n",
       "High       1.324400e+02  2.097850e+02  4.799800e+02  \n",
       "Low        1.306800e+02  2.078300e+02  4.760600e+02  \n",
       "Close      1.315600e+02  2.088500e+02  4.777100e+02  \n",
       "Adj Close  9.370667e+01  1.812353e+02  4.665634e+02  \n",
       "Volume     6.263260e+07  1.168936e+08  8.710263e+08  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spy_summary = spy_df[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']].describe().T\n",
    "display(spy_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea3f7ef-f419-4c93-9bb7-b0ea6a8f0ea8",
   "metadata": {},
   "source": [
    "### Challenge Goals:\n",
    "The challenge goal for this EDA was to extend beyond transcript length and incorporate actual tone measures into the analysis. This was achieved by implementing an in-notebook sentiment analysis using a subset of the Loughran–McDonald dictionary to create normalized tone variables (pos_pct, neg_pct, uncert_pct). While these are prototype features for EDA, the structure directly supports future hypothesis testing and will scale easily when the full dictionary is applied for final analysis. No further challenge extensions (e.g., multi-dataset joins) in order to maintain focus on the core."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0542a985-208f-4c3a-b89a-08c7a7224df9",
   "metadata": {},
   "source": [
    "### Plan Evaluation\n",
    "The initial work plan proved fairly accurate in scope: dataset loading and cleaning took roughly the estimated time, while tone extraction and EDA each required slightly longer due to testing and validation. The decision to keep the datasets separate simplified memory use and avoided alignment errors. The current tasks completed - shape validation, missingness, tone feature generation, and visual summaries - align closely with the planned timeline. Remaining tasks include finalizing hypothesis testing and abnormal-return calculation, estimated to require one additional work session. Overall, the plan has remained realistic and on track."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c6b09-9b07-46d1-86a5-660d369aa7e8",
   "metadata": {},
   "source": [
    "### Testing Overview (proof)\n",
    "Testing approach: All of the transformations feeding the EDA were validated with assertions and small doctests. The tone_counts() function was manually verified with sample strings to confirm correct tokenization and sentiment counting. Key tests include verifying that token_count >= 0, tone percentages lie within [0,1], and date_std is properly formatted as datetime. Plotting functions were not directly tested per rubric, but their inputs were validated through descriptive statistics and shape checks. Together, these confirm that the EDA results can be trusted and replicated on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2927ca7a-ce25-4e9a-bef5-d5f9839af023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'date_std']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spy_df.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd9abf9-b291-4c82-b384-d50d4fd8410d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Extras:\n",
    "##### Table Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9a52e551-956a-4f6d-ab3e-bd5c920aaa14",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticker\n",
       "False    15832\n",
       "True      2923\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "er['ticker'].isin(ohlcv['ticker']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e521eda3-2259-4359-bf18-e4a88495f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticker_compare (df1, col1, df2, col2):\n",
    "    '''\n",
    "    Given two dataframes and two column names, returns the values shared\n",
    "    between the two dataframe columns, for each dataframe (returned as a\n",
    "    tuple of two separate dataframes).\n",
    "\n",
    "    >>> df1 = pd.DataFrame({'ticker': ['AAPL', 'MSFT', 'GOOG'], 'price': [100, 200, 300]})\n",
    "    >>> df2 = pd.DataFrame({'ticker': ['AAPL', 'TSLA'], 'text': ['apple er', 'tesla er']})\n",
    "    >>> df1_common, df2_common = ticker_compare(df1, 'ticker', df2, 'ticker')\n",
    "    >>> sorted(df1_common['ticker'].unique())\n",
    "    ['AAPL']\n",
    "    >>> sorted(df1_common['price'].unique())\n",
    "    [100]\n",
    "    >>> sorted(df2_common['ticker'].unique())\n",
    "    ['AAPL']\n",
    "\n",
    "    >>> df3 = pd.DataFrame({'ticker': ['AMZN'], 'close': [150]})\n",
    "    >>> df4 = pd.DataFrame({'ticker': ['NFLX'], 'text': ['netflix']})\n",
    "    >>> df3_common, df4_common = ticker_compare(df3, 'ticker', df4, 'ticker')\n",
    "    >>> len(df3_common)\n",
    "    0\n",
    "    >>> len(df4_common)\n",
    "    0\n",
    "    >>> sorted(df2_common['text'].unique())\n",
    "    ['apple er']\n",
    "    '''\n",
    "    ticker_1 = set(df1[col1])\n",
    "    ticker_2 = set(df2[col2])\n",
    "    shared = ticker_1 & ticker_2\n",
    "\n",
    "    df1_common = df1[df1[col1].isin(shared)]\n",
    "    df2_common = df2[df2[col2].isin(shared)]\n",
    "\n",
    "    return df1_common, df2_common\n",
    "\n",
    "doctest.run_docstring_examples(ticker_compare, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35435aef-3c3d-41e7-9507-d056195b02c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlcv_common, er_common = ticker_compare(ohlcv, 'ticker', er, 'ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3f8b510-8b77-4ba2-9a21-3fdf2ffb1f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>date_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.208552</td>\n",
       "      <td>9.468413</td>\n",
       "      <td>8.923588</td>\n",
       "      <td>9.201829</td>\n",
       "      <td>2008-02-29 22:38:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.670000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>2.580000</td>\n",
       "      <td>1993-06-11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.250000</td>\n",
       "      <td>4.407500</td>\n",
       "      <td>4.080000</td>\n",
       "      <td>4.387500</td>\n",
       "      <td>1993-09-27 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.622800</td>\n",
       "      <td>10.774850</td>\n",
       "      <td>10.411800</td>\n",
       "      <td>10.601200</td>\n",
       "      <td>2012-06-02 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.417500</td>\n",
       "      <td>11.668275</td>\n",
       "      <td>11.160000</td>\n",
       "      <td>11.350000</td>\n",
       "      <td>2018-05-02 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.800000</td>\n",
       "      <td>22.700000</td>\n",
       "      <td>19.600000</td>\n",
       "      <td>21.280000</td>\n",
       "      <td>2018-08-17 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.411799</td>\n",
       "      <td>4.588522</td>\n",
       "      <td>4.216676</td>\n",
       "      <td>4.373212</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             open        high         low       close             date_std\n",
       "count  300.000000  300.000000  300.000000  300.000000                  300\n",
       "mean     9.208552    9.468413    8.923588    9.201829  2008-02-29 22:38:24\n",
       "min      2.500000    2.670000    2.420000    2.580000  1993-06-11 00:00:00\n",
       "25%      4.250000    4.407500    4.080000    4.387500  1993-09-27 18:00:00\n",
       "50%     10.622800   10.774850   10.411800   10.601200  2012-06-02 12:00:00\n",
       "75%     11.417500   11.668275   11.160000   11.350000  2018-05-02 06:00:00\n",
       "max     20.800000   22.700000   19.600000   21.280000  2018-08-17 00:00:00\n",
       "std      4.411799    4.588522    4.216676    4.373212                  NaN"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohlcv_common.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
