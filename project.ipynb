{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24018313-ad8a-4c47-a484-1e0e0e6e2f52",
   "metadata": {},
   "source": [
    "## Research Questions\n",
    "\n",
    "1) Does the tone (positive, negative, uncertain) expressed during an earnings call predict short-window abnormal stock returns, defined as the firm’s actual return over the [0, +1]-day event window surrounding the call in excess of the market return (proxied by the S&P 500 ETF, SPY)?\n",
    "2) Does the tone to return relationship differ across industries, firm sizes, or leadership?\n",
    "3) Do tone effects weaken or strengthen during high-volatility market days, as measured against the SPY (if there are large increases/decreases in SPY price, are the impacts of tone amplified or dampened)?\n",
    "4) *Potential question*: After controlling for EPS surprise (the difference between actual returns and the forecasted returns by external analysts, which, when positive or negative, can have a significant impact on a company's stock performance), does tone still explain residual abnormal returns (measured with a [-1,+1] event window)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98247ef-f125-4224-ac36-0f6e6a1125eb",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "Corporate earnings calls serve as the main bridge between enterprises and investors. They shape how markets interpret financial performance beyond the raw numbers, providing context to numeric output. While the quantitative outcomes of an earnings report are easy to measure, the language executives use – be it their tone, confidence, or underlying uncertainty – can carry additional weight, which has the potential to influence investor sentiment when localized to each occurrence.\n",
    "\n",
    "It is important to study this relationship because, while markets are a quantitative beast, they also rely on narrative, context, and behavioral signals. Prior work has shown that tone effects on “abnormal performance” can be predicted in gradual post-announcement stock price drift. As a contrast, this project isolates the short-window reaction ([0,+1]) to measure the immediate market response to tone, providing a complementary perspective that is clear of other market influences, which conflate analyses. Understanding this aspect of the psychology behind financial decision-making provides an interesting lens into the impacts of behavior on markets, informing future analysis and serving as an input for future models.\n",
    "\n",
    "Beyond its economic ties, this project provides an interesting computational exploration, combining natural language analysis and statistical models, which is becoming an ever-larger part of financial and economic research. By linking these natural language signals to numerical outcomes, it deepens (my) understanding of how unstructured information can be linked to statistical analysis, and how this information translates to measurable impacts.\n",
    "\n",
    "Experience with finance and markets, which I have gained over the last 3 years, along with my interest in data science, serve as the foundation for my desire to pursue this project. It is particularly interesting in its combination of NLP and statistical analysis, and I look forward to seeing the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd470f7-1694-4d59-b288-eb0a7882fd5a",
   "metadata": {},
   "source": [
    "## Data Setting\n",
    "This project draws on three publicly available datasets that together support analysis of how executive tone in earnings calls relates to short-window abnormal stock returns.\n",
    "1. **[Earnings Call Transcripts (Motley Fool / Kaggle)](https://www.kaggle.com/datasets/tpotterer/motley-fool-scraped-earnings-call-transcripts)** - This dataset includes roughly 18,000 quarterly earnings-call transcripts for U.S.-listed companies. Each record provides the company ticker, call date, exchange, quarter, and full transcript text. The data were scraped from The Motley Fool’s public archives and compiled by Kaggle contributors. The transcripts are the unstructured textual foundation for tone analysis, allowing extraction of sentiment features using finance-specific linguistic dictionaries (see #4).\n",
    "2. **[NASDAQ Daily Prices (Kaggle / Paul Mooney)](https://www.kaggle.com/datasets/svaningelgem/nasdaq-daily-stock-prices)** - This dataset contains daily open, high, low, close, adjusted-close, and volume (OHLCV) data for U.S. equities from roughly 2015-2024. It enables the computation of firm-level daily returns and the construction of event-window returns surrounding each earnings call date.\n",
    "3. **[S&P 500 ETF (SPY) Prices (Kaggle)](https://www.kaggle.com/datasets/benjaminbtang/spy-historical-prices)*** - This dataset provides historical daily prices for the SPY ETF, which is used as a market benchmark. Subtracting SPY’s daily return from a firm’s daily return produces a simple measure of abnormal return, controlling for broad market movements.\n",
    "4. *Supplemental dataset/tool* **|** ***[Loughran-McDonald Financial Sentiment Dictionary](https://sraf.nd.edu/loughranmcdonald-master-dictionary/)*** - Used map word occurrences in transcripts to finance-specific tone categories (positive, negative, uncertainty, etc.). This resource, widely adopted in accounting and finance research, ensures that the tone scores reflect financial meaning rather than generic sentiment.\n",
    "\n",
    "Each dataset is stored in CSV format and will be merged on ticker and date keys to align firm-level and market-level data for each event window.\n",
    "\n",
    "#### Potential Challenges\n",
    "None of the datasets include formal datasheets; however, several contextual details may complicate or encourage deeper analysis:\n",
    "1. **Coverage and survivorship bias** - The transcript dataset includes only companies covered by The Motley Fool, potentially omitting small-cap or delisted firms. This may over-represent large, stable firms and bias results toward those with stronger disclosure practices.\n",
    "2. **Timing misalignment** - Earnings calls often take place after market hours, while price data are recorded at the market close. As a result, a “day 0” return may reflect information or expectations formed before the call rather than the call itself, making it important to define the event window ([0,+1]) carefully and account for weekends and holidays.\n",
    "3. **Linguistic and formatting variation** - Transcripts differ in speaker labeling, punctuation, and inclusion of boilerplate disclaimers or operator remarks. These inconsistencies may distort tone-scoring unless the text is systematically cleaned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd8332-27e9-4c25-9b0f-1c7e4b4f816a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Method\n",
    "Step 1: Load and prepare data\n",
    "- Load the three datasets (earnings call transcripts, stock prices, and SPY benchmark) using pandas\n",
    "- Standardize date formats and align all data by ticker and date.\n",
    "- Functions (data-manipulation): load_data(), standardize_dates()\n",
    "- Tests: Use small 3–5 row samples to confirm correct data types and successful merges.\n",
    "- Output: Three clean DataFrames with properly formatted and aligned dates.\n",
    "- Connection: Establishes base for the Multiple Datasets challenge goal by merging separate data sources.\n",
    "\n",
    "\n",
    "Step 2: Clean transcripts and compute tone features\n",
    "- Use Python’s built-in re library to remove punctuation, lowercase text, and normalize spacing.\n",
    "- Count occurrences of positive, negative, and uncertainty words using the Loughran–McDonald financial dictionary.\n",
    "- Calculate each tone category as a percentage of total words in the transcript.\n",
    "- Functions (data-manipulation): clean_text(), compute_tone_scores()\n",
    "- Tests: Verify results on short sample texts (“profits increased,” “uncertain outlook”) with known word counts.\n",
    "- Output: Dataset with tone metrics (pos_pct, neg_pct, uncert_pct) for each earnings call.\n",
    "- Connection: Creates the independent variables used in hypothesis testing (RQ1 and RQ2).\n",
    "\n",
    "\n",
    "Step 3: Compute event-window and abnormal returns\n",
    "- Compute daily returns for each stock and for SPY using adjusted close prices.\n",
    "-Define Day 0 as the first trading day on or after the call date and Day +1 as the following trading day.\n",
    "- Calculate abnormal returns as firm return minus SPY return, then sum over [0,+1] to get cumulative abnormal return (CAR).\n",
    "- Functions (data-manipulation): compute_returns(), compute_abnormal_returns()\n",
    "- Tests: Hand-check results on a small, synthetic dataset to confirm correct math and event-window handling.\n",
    "- Output: Event-level dataset linking each call to its short-window abnormal return.\n",
    "- Connection: Provides the dependent variable for statistical testing and supports Multiple Datasets.\n",
    "\n",
    "Step 4: Merge tone and return data\n",
    "- Merge tone metrics with event returns and add basic controls such as sector and firm-size proxies.\n",
    "- Functions (data-manipulation): merge_features(), add_controls()\n",
    "- Tests: Ensure one row per event after merging and confirm correct ticker/date alignment.\n",
    "- Output: Combined dataset ready for modeling.\n",
    "- Connection: Prepares data for hypothesis testing (RQ1 and RQ2).\n",
    "\n",
    "Step 5: Hypothesis testing and modeling\n",
    "- Run regression models using statsmodels to test whether tone predicts short-term abnormal returns:\n",
    "     car_0p1 ~ pos_pct + neg_pct + uncert_pct + sector + size_proxy\n",
    "- Evaluate coefficients, p-values, and confidence intervals to test significance.\n",
    "- Adjust for multiple comparisons (e.g., Benjamini–Hochberg correction) if running across multiple tone types or sectors.\n",
    "- Functions (data-manipulation): fit_model(), summarize_results()\n",
    "- Tests: Use synthetic data with known relationships to confirm correct coefficient direction and model behavior.\n",
    "- Interpretation:\n",
    "- - RQ1: Positive coefficients on pos_pct or negative on neg_pct indicate tone predicts abnormal returns.\n",
    "- - RQ2: Interaction terms or coefficient differences by sector suggest heterogeneity.\n",
    "- Connection: Directly achieves the Statistical Hypothesis Testing challenge goal.\n",
    "\n",
    "Step 6: Visualization\n",
    "- Create plots to display tone distributions, tone vs. return relationships, and regression coefficients.\n",
    "- Functions (plotting): plot_tone_vs_returns(), plot_coefficients()\n",
    "- Tests: No formal testing; figures checked visually for accuracy and clarity.\n",
    "- Output: Visual confirmation of tone–return relationships.\n",
    "- Connection: Helps interpret quantitative results for RQ1 and RQ2.\n",
    "\n",
    "\n",
    "*Step 7: Robustness and reporting (optional)*\n",
    "- *Re-run models using alternative event windows ([−1,+1] or [0,+5]) to confirm consistency.*\n",
    "- *Winsorize extreme returns to check for sensitivity to outliers.*\n",
    "- *Save outputs, figures, and summary tables for reporting.*\n",
    "- *Connection: Provides robustness checks for RQ1 and RQ3, ensuring conclusions are not window-dependent.*\n",
    "\n",
    "#### Plan\n",
    "\n",
    "The project will be completed in JupyterHub and divided into five main tasks, each designed to be clear, independent, and reproducible.\n",
    "\n",
    "\n",
    "1) Setup and data preparation (2 hours): I will create an organized folder structure in JupyterHub with subfolders for raw data, processed data, figures, and reports. After confirming the environment setup, I will load the earnings call transcripts, stock prices, and SPY benchmark data using pandas. During this step, I will standardize date formats, check for missing or duplicated keys, and ensure that tickers and dates align across datasets to prepare for merging.\n",
    "\n",
    "\n",
    "2) Text cleaning and tone computation (3 hours): Using Python’s re library, I will remove punctuation, normalize spacing, and lowercase the transcript text. I will then apply the Loughran-McDonald financial dictionary or the spaCy API to calculate the percentage of positive, negative, and uncertainty words for each transcript. The resulting tone features will be saved as a separate dataset and tested on a small subset of text examples to confirm accuracy.\n",
    "\n",
    "\n",
    "3) Return calculations and event-window construction (3 hours): I will compute daily returns for both individual tickers and the SPY benchmark. For each earnings call, I will define the event window as [0,+1], where Day 0 represents the first trading day on or after the call. Abnormal returns will be calculated as the firm’s return minus SPY’s return, and cumulative abnormal returns (CAR) will be saved for each event. Manual checks on a small synthetic dataset will verify the accuracy of these calculations.\n",
    "\n",
    "\n",
    "4) Merging, modeling, and hypothesis testing (5 hours): I will merge the tone dataset with abnormal returns and add control variables such as industry sector and firm size proxies (e.g., log of average volume). Using statsmodels, I will run regression models to test whether tone predicts short-window abnormal returns while controlling for other factors. I will interpret coefficients, p-values, and confidence intervals directly in the context of the research questions.\n",
    "\n",
    "\n",
    "5) Visualization and reporting (~3 hours): The final step will involve creating plots to display the distribution of tone features, the relationship between tone and abnormal returns, and regression coefficients with confidence intervals. If time allows, I will perform quick robustness checks such as alternate event windows or light outlier filtering. All intermediate results, figures, and tables will be saved for reproducibility.\n",
    "\n",
    "\n",
    "*This plan builts in buffer time and may be an overestimation*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dab228-c348-4832-897e-0cb08da79783",
   "metadata": {},
   "source": [
    "## EDA Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81162ef9-cf6b-4cd1-8a69-e5e76df90bc4",
   "metadata": {},
   "source": [
    "lalalla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cf174a32-5149-4480-905a-6f6e0f5a1164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import doctest\n",
    "from pathlib import Path\n",
    "from IPython.display import display\n",
    "from lmd_loader import load_masterdictionary # from custom python file\n",
    "from pandas import DataFrame # for type annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf13c8e-3b9b-4cdf-b24b-bb5b0373cb1e",
   "metadata": {},
   "source": [
    "### Initial Preparation and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cbfc1b-7766-41d3-9e48-75072cc4053c",
   "metadata": {},
   "source": [
    "#### I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e33f3e32-88d7-44e3-9a02-4e236f77086d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earning Reports - cannot limit pickle read\n",
    "def read_er_pkl(path: str, head_rows=None) -> DataFrame:\n",
    "    '''Load the earnings report pickle file into a df.'''\n",
    "    with open(path, 'rb') as file:\n",
    "        df = pickle.load(file)\n",
    "    return df.head(head_rows) if head_rows else df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4d0ff9-3fac-4bda-89c0-fc1508566de7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Depracated IO below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d105b481-9f2a-4da7-999e-3a6c6eb407d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NASDAQ OHLCV\n",
    "# def read_nasdaq_folder(folder_path, pattern='*csv', limit_files=None, nrows=None):\n",
    "#     '''\n",
    "#     '''\n",
    "#     files = list(Path(folder_path).glob(pattern))\n",
    "#     if limit_files is not None:\n",
    "#         files = files[:limit_files]\n",
    "#     df_list = [pd.read_csv(file, nrows=nrows) for file in files]\n",
    "#     return pd.concat(df_list, ignore_index=True) if df_list else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5afb63bf-aeea-4646-a7fd-ef261d2d7996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1377"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# files = list(Path('data/nasdaq_prices').glob('*.csv'))\n",
    "# len(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94b98ca-7ddd-43ee-99dd-837292a4c3b3",
   "metadata": {},
   "source": [
    "#### Data Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "65318029-90de-4f71-88f4-d190d8e11884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NASDAQ OHLCV + SPY Date Conversions\n",
    "def date_transform(df: DataFrame, src_col: str, out_col: str) -> DataFrame:\n",
    "    '''\n",
    "    Converts a column from string to datetime and stores it under a new name.\n",
    "    '''\n",
    "    df[out_col] = pd.to_datetime(df[src_col])\n",
    "    df[out_col] = df[out_col].dt.normalize()   # precautionary check\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ed1906b5-c00e-4ba0-a1eb-4456b885c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earning Reports Date Conversion\n",
    "def er_date_transform(df: DataFrame, src_col: str ='date', out_col: str ='date_std') -> DataFrame:\n",
    "    '''\n",
    "    Given a dataset, returns a dataset with a cleaned and standardized date column. Receives \n",
    "    an input for the source date column, and the name for the outputted standardized date column.\n",
    "    '''\n",
    "    date_clean = df[src_col].str.strip()\n",
    "    date_clean = date_clean.str.replace(\".\",\"\")\n",
    "    date_clean = date_clean.str.replace(\"ET\",\"\")\n",
    "    \n",
    "    df[out_col] = pd.to_datetime(date_clean, format='mixed')\n",
    "\n",
    "    # boolean mask for past 5:00 pm identification -> works because indexes line up etc\n",
    "    after_close = df[out_col].dt.hour >= 17\n",
    "    row_updates = df.loc[after_close, out_col]            # stores all \n",
    "    updated_dates = row_updates + pd.Timedelta(days=1)    # uses pandas timedelta function to add 1 day to the date\n",
    "    df.loc[after_close, 'date_std'] = updated_dates       # uses .loc for conditional selection base on True | False values in after_close\n",
    "    df[out_col] = df[out_col].dt.normalize()              # strip times for use with \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f358b7-fefb-40ad-a005-3e63068909d5",
   "metadata": {},
   "source": [
    "#### File Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ea6ea-df44-496c-acfe-aaf3aa3e3402",
   "metadata": {},
   "source": [
    "##### SPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "302a2e53-1b92-4a28-8ad1-42b5ff37273e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_df = pd.read_csv('data/SPY.csv')\n",
    "spy_df = date_transform(spy_df, 'Date', 'date_std')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b4cc3c-233c-4ddc-9c60-8bbfbb299cb9",
   "metadata": {},
   "source": [
    "##### LMD (Sentiment Dict) implementation\n",
    "Implemented using the lmd_loader.py script (pulled from the [official script](https://drive.google.com/file/d/18jbZ3o17PRI_s4xG9UslKnGMpnC1ZoLM/view))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "30cc4436-ce82-48b7-96f3-6d7b478454da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ...Loading Master Dictionary 85,000\n",
      "Master Dictionary loaded from file:\n",
      "  data/Loughran-McDonald_MasterDictionary_1993-2024.csv\n",
      "\n",
      "  master_dictionary has 86,553 words.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sentiment Dictionary\n",
    "lmd_path = 'data/Loughran-McDonald_MasterDictionary_1993-2024.csv'\n",
    "\n",
    "# load everything\n",
    "# vars identified in return stmnt at bottom of LMD loader, incl params for logging\n",
    "master_dict, md_header, sentiment_categories, sentiment_dicts, stopwords, total_docs = \\\n",
    "    load_masterdictionary(lmd_path, print_flag=True, f_log=None, get_other=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b8233aa-4c5b-44b8-b1e6-8554cd6074e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative',\n",
       " 'positive',\n",
       " 'uncertainty',\n",
       " 'litigious',\n",
       " 'strong_modal',\n",
       " 'weak_modal',\n",
       " 'constraining',\n",
       " 'complexity']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d4790-2607-46c1-be25-8e925711e6ea",
   "metadata": {},
   "source": [
    "##### Earnings Reports / Sentiment Analysis\n",
    "Must perform sentiment analysis in loading phase due to memory constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ab364ab0-7682-4168-8d57-625b2d3a8b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lmd_features(text: str, sentiment_dicts: dict[str, dict[str, int]], \n",
    "                 cat_order: list[str] = sentiment_categories) -> dict[str, float]:\n",
    "    '''\n",
    "    Computes proportion of words in input text that belong to each LMD sentiment\n",
    "    category. For every category (pre-loaded by sentiment dict), it counts the \n",
    "    number of matching words and produces a proportion of the total for that word.\n",
    "    Designed to avoid creating large intermediate lists. \n",
    "    '''\n",
    "    if not isinstance(text, str): # checks for string to prevent errors\n",
    "        if text is None:\n",
    "            text = \"\"\n",
    "        else:\n",
    "            text = str(text)\n",
    "    \n",
    "    upper_text = text.upper()\n",
    "\n",
    "    # declares the counter and assigns base 0s\n",
    "    counts = {}\n",
    "    for category in cat_order:\n",
    "        counts[category] = 0\n",
    "\n",
    "    text_convert = re.compile(r\"[A-Za-z]+\")\n",
    "    total = 0\n",
    "\n",
    "    # using advanced RE module for memory efficiency **\n",
    "    iterator = text_convert.finditer(upper_text) # identifies matches in the string for iteration\n",
    "    \n",
    "    # match being a container that has text, start position, end position\n",
    "    for match in iterator:                        # \"for words in text\" = n words = O(n)\n",
    "        total += 1\n",
    "        tok = match.group(0) \n",
    "        \n",
    "        for category in cat_order:                # for category in list of categories = 9 cat\n",
    "            cat_dict = sentiment_dicts[category]\n",
    "            if tok in cat_dict:                   # 'if' search through a dict = O(1)\n",
    "                counts[category] += 1\n",
    "    \n",
    "    if total < 1:\n",
    "        denom = 1           # avoids division by 0 errors\n",
    "    else:\n",
    "        denom = total\n",
    "    \n",
    "    # computes the percentages for each category\n",
    "        # does not work for syllables (would need avg syllables)\n",
    "        # -> must ensure the syllable column is dropped\n",
    "    result = {}\n",
    "    for category in cat_order: \n",
    "        result[category] = counts[category] / denom\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b3a50d5d-47aa-4877-a1be-a8df74e38909",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y Test 1: Positive text\n",
      "Y Test 2: Negative text\n",
      "Y Test 3: Empty string\n",
      "Y Test 4: None input\n"
     ]
    }
   ],
   "source": [
    "# feature tests\n",
    "# Test 1: Positive text\n",
    "result = lmd_features(\"able abundance\", sentiment_dicts, sentiment_categories)\n",
    "assert result['positive'] > 0, \"X Should detect positive words\"\n",
    "assert result['negative'] == 0, \"X Should have no negative words\"\n",
    "print(\"Y Test 1: Positive text\")\n",
    "\n",
    "# Test 2: Negative text\n",
    "result = lmd_features(\"loss decline\", sentiment_dicts, sentiment_categories)\n",
    "assert result['negative'] > 0, \"X Should detect negative words\"\n",
    "assert result['positive'] == 0, \"X Should have no positive words\"\n",
    "print(\"Y Test 2: Negative text\")\n",
    "\n",
    "# Test 3: Empty string\n",
    "result = lmd_features(\"\", sentiment_dicts, sentiment_categories)\n",
    "assert result['positive'] == 0, \"X Empty should be 0\"\n",
    "assert result['negative'] == 0, \"X Empty should be 0\"\n",
    "print(\"Y Test 3: Empty string\")\n",
    "\n",
    "# Test 4: None input\n",
    "result = lmd_features(None, sentiment_dicts, sentiment_categories)\n",
    "assert result['positive'] == 0, \"X None should be handled\"\n",
    "print(\"Y Test 4: None input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c80092c1-f557-4f61-a22e-e5688430af1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_features(df, text_col=\"transcript\", drop_text=True):\n",
    "    \"\"\"\n",
    "    Add sentiment columns to dataframe. **finsish\n",
    "    \"\"\"\n",
    "    # initial gaurdrail\n",
    "    if text_col not in df.columns:\n",
    "        raise KeyError(f\"Column '{text_col}' not in DataFrame\")\n",
    "    \n",
    "    # applies sentiment function to each transcript (s = the 'current' transcript)\n",
    "    feat_dicts = df[text_col].apply(\n",
    "        lambda s: lmd_features(s, sentiment_dicts, sentiment_categories))\n",
    "    \n",
    "    # .apply converts dicts into columns/series -> then appended\n",
    "    feats = feat_dicts.apply(pd.Series)\n",
    "    \n",
    "    # applies transcript drops (per parameter) -- can make this one line in final\n",
    "    if drop_text:\n",
    "        base = df.drop(columns=[text_col])\n",
    "    else:\n",
    "        base = df\n",
    "        \n",
    "    # concat original and features\n",
    "    out = pd.concat([base, feats], axis=1)\n",
    "    \n",
    "    # testing\n",
    "    for cat in sentiment_categories:\n",
    "        assert cat in out.columns, \"Columns missing\"\n",
    "        print(\"Columns present\")\n",
    "        assert (out[cat].between(0, 1)).all(), \"Improper values\"\n",
    "        print(\"Values eligible\")\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b4262ed9-84cc-47dd-9aa7-b78efdbacc84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns present\n",
      "Values eligible\n",
      "Columns present\n",
      "Values eligible\n",
      "Columns present\n",
      "Values eligible\n",
      "Columns present\n",
      "Values eligible\n",
      "Columns present\n",
      "Values eligible\n",
      "Columns present\n",
      "Values eligible\n",
      "Columns present\n",
      "Values eligible\n",
      "Columns present\n",
      "Values eligible\n"
     ]
    }
   ],
   "source": [
    "# Earnings Reports\n",
    "er_df = read_er_pkl(\"data/motley-fool-data.pkl\")\n",
    "er_df = er_date_transform(er_df, 'date', 'date_std')\n",
    "er_df = er_df.dropna(subset=['date_std'])\n",
    "# below: removes the time stamps -> not included in function for separate use case later\n",
    "# --> use times for adjusted window calcs in final\n",
    "# ** er_df['date_std'] = er_df['date_std'].dt.date\n",
    "# applies tokenizer below\n",
    "er_df = apply_features(er_df, 'transcript', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3731a3-6f86-43e1-9d3e-4ac17764dd2f",
   "metadata": {},
   "source": [
    "##### NASDAQ OHLCV\n",
    "Simplified loading process that reduces memory usage by focusing on relevant tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e9189576-4754-41de-aa37-7c30dad3bddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_single_ohlcv(file: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Load a single OHLCV file with ticker column, barring\n",
    "    ticker column errors, and returns the dataframe.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file)\n",
    "    if 'ticker' not in df.columns:\n",
    "        df['ticker'] = file.stem.upper()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "14aa9882-0da7-4bf5-9228-d495b1ef7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_relevant_ohlcv(ohlcv_folder: str, er_df: DataFrame) -> DataFrame:\n",
    "    '''\n",
    "    Returns a df with cols [ticker, date_std, open, high, low, close],\n",
    "    after loading OHLCV data only for tickers that appear in the inputted\n",
    "    earnings report (reducing storage significantly).\n",
    "    '''\n",
    "\n",
    "    # get unique tickers using set()\n",
    "    rel_tickers = set(er_df['ticker'].str.upper()) # standardized to string and upper for comparison\n",
    "    print(f\"Found {len(rel_tickers)} unique tickers in earnings reports\")\n",
    "\n",
    "    # uses path module/object with .glob to get all file paths, and place in list\n",
    "    files = list(Path(ohlcv_folder).glob('*.csv'))\n",
    "\n",
    "    # filters file paths to only retain 'relevant' tickers\n",
    "    rel_files = []\n",
    "    for file in files:\n",
    "        # pulls ticker names using the .stem()\n",
    "        ticker = file.stem.upper()\n",
    "        if ticker in rel_tickers:\n",
    "            rel_files.append(file)\n",
    "    \n",
    "    print(f\"{len(rel_files)} overlapping tickers in OHLCV\")\n",
    "\n",
    "    # full file loading using prev-built loading function\n",
    "    df_list = []\n",
    "    for file in rel_files:\n",
    "        df_list.append(load_single_ohlcv(file))\n",
    "\n",
    "    # takes the list of df, concats into one, and normalizes dates\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    combined_df = date_transform(combined_df, 'date', 'date_std')\n",
    "    \n",
    "    # filter columns for export\n",
    "    cols_to_keep = ['ticker', 'date_std', 'open', 'high', 'low', 'close']\n",
    "    combined_df = combined_df[cols_to_keep]\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0b2d307-745d-489b-8a4c-03added646df",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'er_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ohlcv_df \u001b[38;5;241m=\u001b[39m load_relevant_ohlcv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/nasdaq_prices\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[43mer_df\u001b[49m)\n\u001b[1;32m      2\u001b[0m ohlcv_df\n",
      "\u001b[0;31mNameError\u001b[0m: name 'er_df' is not defined"
     ]
    }
   ],
   "source": [
    "ohlcv_df = load_relevant_ohlcv('data/nasdaq_prices', er_df)\n",
    "ohlcv_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7342e926-a833-4a6f-8693-71cdbec21f53",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### *Tests/Checks - Initial load and cleaning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "517f5082-db56-445e-bbd1-c27807556f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'date_std' in er_df.columns\n",
    "assert 'ticker' in er_df.columns\n",
    "\n",
    "assert 'date_std' in ohlcv_df.columns\n",
    "assert 'ticker' in ohlcv_df.columns\n",
    "assert 'date_std' in spy_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d852a06b-0344-4bc6-9731-cb271cc6652e",
   "metadata": {},
   "source": [
    "#### Stock Return Calcs\n",
    "*Similar Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c069cbb8-fa4f-4149-aea7-9f8b13bd3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_returns_calc(df: DataFrame, price_col: str, date_col: str, group_col: str, return_col: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Computes daily returns given an input df, price col, date col, and grouping column,\n",
    "    using a simple percent change formula. Returns a df that retains the calculated\n",
    "    returns, and that is sorted ensuring congruent calculations. \n",
    "    \"\"\"\n",
    "    # asserts for input verification (are they necessary?)\n",
    "    assert price_col in df.columns, f\"Column '{price_col}' not found\"\n",
    "    assert date_col in df.columns, f\"Column '{date_col}' not found\"\n",
    "    assert len(df) > 0, \"DataFrame cannot be empty\"\n",
    "    \n",
    "    new_df = df.copy()      # could avoid the copy for memory improvement, but use if possible\n",
    "\n",
    "    # data should be sorted, but pct change calc will be WRONG if it isnt, so we ensure\n",
    "    # group_col must be specified as None in the call if performing for a single stock -> \n",
    "        # risk too high if accidentally forgetten with multi ticker data\n",
    "    if group_col is None:\n",
    "        new_df = new_df.sort_values(date_col)\n",
    "        new_df[return_col] = new_df[price_col].pct_change()\n",
    "    else:\n",
    "        sort_cols = [group_col, date_col]\n",
    "        new_df = new_df.sort_values(sort_cols)\n",
    "        new_df[return_col] = new_df.groupby(group_col)[price_col].pct_change()\n",
    "\n",
    "    assert new_df[return_col].notna().sum() > 0, \"All returns are NaN | 0\"\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "100c89ea-7624-4f0b-a1fc-b5079bb126f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "daily_returns_calc() missing 1 required positional argument: 'group_col'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ohlcv_returns \u001b[38;5;241m=\u001b[39m daily_returns_calc(ohlcv_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_std\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mticker\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m spy_returns \u001b[38;5;241m=\u001b[39m \u001b[43mdaily_returns_calc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspy_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mClose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdate_std\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspy_return\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: daily_returns_calc() missing 1 required positional argument: 'group_col'"
     ]
    }
   ],
   "source": [
    "ohlcv_returns = daily_returns_calc(ohlcv_df, 'close', 'date_std', 'ticker', 'return_fract')\n",
    "spy_returns = daily_returns_calc(spy_df, 'Close', 'date_std', group_col = None, return_col='spy_return_fract')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767f9599-814e-4868-9316-e8a285a22de6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Window Return Calculations\n",
    "Window return calc needed pre-merge, due to data loss on date filter\n",
    "- Ensure the window factors in weekend etc\n",
    "- Then call the window calc function for different day variations (i.e. 1d, 2d, 5d)\n",
    "- Bring in something else to actually get the real drift outside of surprise??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b005d20b-1278-4975-8bdd-81dfc68cc59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def event_window_car(merged_df: DataFrame, window_days: int = 2) -> DataFrame:\n",
    "#     \"\"\"\n",
    "#     Computes cumulative abnormal return (CAR) over [0, +1] event window.\n",
    "#     Abnormal return = stock return - SPY return\n",
    "    \n",
    "#     **NOTE** This is simplified - assumes next calendar day = next trading day;\n",
    "#     full version will handle weekends/holidays properly.\n",
    "#     \"\"\"\n",
    "#     out = merged_df.copy()\n",
    "    \n",
    "#     # For each earnings call, gets returns on day 0 and day +1\n",
    "#     # Simplified v1: just gets  abnormal return on the call date\n",
    "#     out['abnormal_return'] = out['return'] - out['spy_return']\n",
    "    \n",
    "#     # **EDA using single-day abnormal return as proxy for CAR**\n",
    "#     out['car_0p1'] = out['abnormal_return']  # Placeholder\n",
    "    \n",
    "#     return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee93d8b-e0d3-4a69-8524-58c6c8cdfdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def window_return(df: DataFrame, window_period):\n",
    "#     \"\"\"\n",
    "#     \"\"\"\n",
    "#     return_col = f\"return_{window_period}d\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6314c3a6-3e8e-4712-86ee-693547a5c2b6",
   "metadata": {},
   "source": [
    "#### Data Merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d60cafcf-07c1-49a4-b48e-4055584ca0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>exchange</th>\n",
       "      <th>q</th>\n",
       "      <th>ticker</th>\n",
       "      <th>date_std</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>uncertainty</th>\n",
       "      <th>litigious</th>\n",
       "      <th>strong_modal</th>\n",
       "      <th>weak_modal</th>\n",
       "      <th>constraining</th>\n",
       "      <th>complexity</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aug 27, 2020, 9:00 p.m. ET</td>\n",
       "      <td>NASDAQ: BILI</td>\n",
       "      <td>2020-Q2</td>\n",
       "      <td>BILI</td>\n",
       "      <td>2020-08-28</td>\n",
       "      <td>0.004299</td>\n",
       "      <td>0.022356</td>\n",
       "      <td>0.006879</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.006191</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>45.7100</td>\n",
       "      <td>48.8600</td>\n",
       "      <td>45.5100</td>\n",
       "      <td>47.3900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nov 6, 2019, 12:00 p.m. ET</td>\n",
       "      <td>NASDAQ: BBSI</td>\n",
       "      <td>2019-Q3</td>\n",
       "      <td>BBSI</td>\n",
       "      <td>2019-11-06</td>\n",
       "      <td>0.007390</td>\n",
       "      <td>0.011442</td>\n",
       "      <td>0.010965</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>0.004648</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>86.9544</td>\n",
       "      <td>90.3949</td>\n",
       "      <td>84.1391</td>\n",
       "      <td>88.6084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aug 7, 2019, 8:30 a.m. ET</td>\n",
       "      <td>NASDAQ: CSTE</td>\n",
       "      <td>2019-Q2</td>\n",
       "      <td>CSTE</td>\n",
       "      <td>2019-08-07</td>\n",
       "      <td>0.009237</td>\n",
       "      <td>0.019979</td>\n",
       "      <td>0.009667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>12.5193</td>\n",
       "      <td>14.6062</td>\n",
       "      <td>12.0945</td>\n",
       "      <td>14.5777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nov 06, 2019, 4:30 p.m. ET</td>\n",
       "      <td>NASDAQ: DXCM</td>\n",
       "      <td>2019-Q3</td>\n",
       "      <td>DXCM</td>\n",
       "      <td>2019-11-06</td>\n",
       "      <td>0.006592</td>\n",
       "      <td>0.023799</td>\n",
       "      <td>0.007263</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>0.005251</td>\n",
       "      <td>0.003464</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>38.4975</td>\n",
       "      <td>39.3138</td>\n",
       "      <td>38.0662</td>\n",
       "      <td>38.2800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Feb 10, 2021, 9:00 a.m. ET</td>\n",
       "      <td>NASDAQ: EEFT</td>\n",
       "      <td>2020-Q4</td>\n",
       "      <td>EEFT</td>\n",
       "      <td>2021-02-10</td>\n",
       "      <td>0.006983</td>\n",
       "      <td>0.015793</td>\n",
       "      <td>0.006983</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.003653</td>\n",
       "      <td>0.004620</td>\n",
       "      <td>0.002578</td>\n",
       "      <td>0.003223</td>\n",
       "      <td>136.9200</td>\n",
       "      <td>145.3400</td>\n",
       "      <td>136.6900</td>\n",
       "      <td>141.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>Feb 24, 2022, 5:00 p.m. ET</td>\n",
       "      <td>NASDAQ: ABCL</td>\n",
       "      <td>2021-Q4</td>\n",
       "      <td>ABCL</td>\n",
       "      <td>2022-02-25</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.016592</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>0.004354</td>\n",
       "      <td>0.004236</td>\n",
       "      <td>0.004119</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>0.000353</td>\n",
       "      <td>9.3600</td>\n",
       "      <td>9.3600</td>\n",
       "      <td>8.5650</td>\n",
       "      <td>8.9300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>Aug 12, 2021, 4:30 p.m. ET</td>\n",
       "      <td>NASDAQ: AVXL</td>\n",
       "      <td>2021-Q3</td>\n",
       "      <td>AVXL</td>\n",
       "      <td>2021-08-12</td>\n",
       "      <td>0.014473</td>\n",
       "      <td>0.015809</td>\n",
       "      <td>0.011133</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.006235</td>\n",
       "      <td>0.004453</td>\n",
       "      <td>0.004231</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>18.0300</td>\n",
       "      <td>19.6800</td>\n",
       "      <td>17.6800</td>\n",
       "      <td>19.2700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>Jul 21, 2022, 11:00 a.m. ET</td>\n",
       "      <td>NASDAQ: BANR</td>\n",
       "      <td>2022-Q2</td>\n",
       "      <td>BANR</td>\n",
       "      <td>2022-07-21</td>\n",
       "      <td>0.012136</td>\n",
       "      <td>0.012432</td>\n",
       "      <td>0.007548</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.004144</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>59.3659</td>\n",
       "      <td>59.3659</td>\n",
       "      <td>56.7114</td>\n",
       "      <td>58.4778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2835</th>\n",
       "      <td>Aug 04, 2022, 5:00 p.m. ET</td>\n",
       "      <td>NASDAQ: DH</td>\n",
       "      <td>2022-Q2</td>\n",
       "      <td>DH</td>\n",
       "      <td>2022-08-05</td>\n",
       "      <td>0.008494</td>\n",
       "      <td>0.020363</td>\n",
       "      <td>0.006167</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.003258</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>22.9700</td>\n",
       "      <td>25.2200</td>\n",
       "      <td>21.7000</td>\n",
       "      <td>24.5050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>Feb 28, 2022, 4:30 p.m. ET</td>\n",
       "      <td>NASDAQ: DVAX</td>\n",
       "      <td>2021-Q4</td>\n",
       "      <td>DVAX</td>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>0.008307</td>\n",
       "      <td>0.020768</td>\n",
       "      <td>0.012461</td>\n",
       "      <td>0.003115</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.003461</td>\n",
       "      <td>0.002077</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>12.3800</td>\n",
       "      <td>12.7000</td>\n",
       "      <td>12.0800</td>\n",
       "      <td>12.2600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2837 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             date      exchange        q ticker   date_std  \\\n",
       "0      Aug 27, 2020, 9:00 p.m. ET  NASDAQ: BILI  2020-Q2   BILI 2020-08-28   \n",
       "1      Nov 6, 2019, 12:00 p.m. ET  NASDAQ: BBSI  2019-Q3   BBSI 2019-11-06   \n",
       "2       Aug 7, 2019, 8:30 a.m. ET  NASDAQ: CSTE  2019-Q2   CSTE 2019-08-07   \n",
       "3      Nov 06, 2019, 4:30 p.m. ET  NASDAQ: DXCM  2019-Q3   DXCM 2019-11-06   \n",
       "4      Feb 10, 2021, 9:00 a.m. ET  NASDAQ: EEFT  2020-Q4   EEFT 2021-02-10   \n",
       "...                           ...           ...      ...    ...        ...   \n",
       "2832   Feb 24, 2022, 5:00 p.m. ET  NASDAQ: ABCL  2021-Q4   ABCL 2022-02-25   \n",
       "2833   Aug 12, 2021, 4:30 p.m. ET  NASDAQ: AVXL  2021-Q3   AVXL 2021-08-12   \n",
       "2834  Jul 21, 2022, 11:00 a.m. ET  NASDAQ: BANR  2022-Q2   BANR 2022-07-21   \n",
       "2835   Aug 04, 2022, 5:00 p.m. ET    NASDAQ: DH  2022-Q2     DH 2022-08-05   \n",
       "2836   Feb 28, 2022, 4:30 p.m. ET  NASDAQ: DVAX  2021-Q4   DVAX 2022-02-28   \n",
       "\n",
       "      negative  positive  uncertainty  litigious  strong_modal  weak_modal  \\\n",
       "0     0.004299  0.022356     0.006879   0.001376      0.006191    0.002752   \n",
       "1     0.007390  0.011442     0.010965   0.001073      0.002384    0.004648   \n",
       "2     0.009237  0.019979     0.009667   0.000000      0.003437    0.005800   \n",
       "3     0.006592  0.023799     0.007263   0.001341      0.005251    0.003464   \n",
       "4     0.006983  0.015793     0.006983   0.000752      0.003653    0.004620   \n",
       "...        ...       ...          ...        ...           ...         ...   \n",
       "2832  0.007060  0.016592     0.010708   0.004354      0.004236    0.004119   \n",
       "2833  0.014473  0.015809     0.011133   0.000668      0.006235    0.004453   \n",
       "2834  0.012136  0.012432     0.007548   0.000740      0.004144    0.002220   \n",
       "2835  0.008494  0.020363     0.006167   0.001280      0.003258    0.003258   \n",
       "2836  0.008307  0.020768     0.012461   0.003115      0.004673    0.003461   \n",
       "\n",
       "      constraining  complexity      open      high       low     close  \n",
       "0         0.000688    0.000172   45.7100   48.8600   45.5100   47.3900  \n",
       "1         0.000596    0.000596   86.9544   90.3949   84.1391   88.6084  \n",
       "2         0.000859    0.002793   12.5193   14.6062   12.0945   14.5777  \n",
       "3         0.001676    0.001229   38.4975   39.3138   38.0662   38.2800  \n",
       "4         0.002578    0.003223  136.9200  145.3400  136.6900  141.1000  \n",
       "...            ...         ...       ...       ...       ...       ...  \n",
       "2832      0.001294    0.000353    9.3600    9.3600    8.5650    8.9300  \n",
       "2833      0.004231    0.000668   18.0300   19.6800   17.6800   19.2700  \n",
       "2834      0.000740    0.000444   59.3659   59.3659   56.7114   58.4778  \n",
       "2835      0.001862    0.000931   22.9700   25.2200   21.7000   24.5050  \n",
       "2836      0.002077    0.001038   12.3800   12.7000   12.0800   12.2600  \n",
       "\n",
       "[2837 rows x 17 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uses inner join to merge pricing and report data (post ticker filter)\n",
    "merged = er_df.merge(ohlcv_returns, on=['ticker', 'date_std'], how='inner')\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9038ad8-f3aa-41a0-a625-dc3b585b664f",
   "metadata": {},
   "source": [
    "### Q: How large is the dataset?\n",
    "The datasets combine three complementary sources: corporate earnings-call transcripts, daily stock prices, and a market benchmark. The earnings-report data includes 18,755 rows and 6 columns, where each row represents a single company’s earnings call on a specific date. Columns contain identifiers (ticker, date_std), the full transcript_text,  sentiment token scores, and supporting metadata such as company name or file origin. This dataset serves as the textual foundation for tone analysis, capturing the language used by executives during calls.\n",
    "\n",
    "The market price data (OHLCV) comprises 133,557 rows and 7 columns, with each row corresponding to one trading-day observation for a given firm (delineated by ticker). Columns record standard financial attributes: open, high, low, close, and volume. These values will later be used to calculate daily and event-window returns. For the benchmark dataset, it is drawn from the S&P 500 ETF (SPY), includes 7,703 rows and 8 columns, with each row representing one trading day for the broader market. It mirrors the OHLCV structure (with the addition of adj_close) and provides the baseline for measuring abnormal returns.\n",
    "\n",
    "Note: In this EDA, the OHLCV table was filtered immediately at load time (file-level limitation) for memory control purposes -> i.e., we did not first load the full datset and then filter by transcript overlap, like we will in later iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aedf507c-cc8a-4250-9c60-7d494965f905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data checks\n",
    "required_cols = {\n",
    "    \"er_df\": {\"ticker\", \"date_std\"},\n",
    "    \"ohlcv_df\": {\"ticker\", \"date_std\"},\n",
    "    \"spy_df\": {\"date_std\"},\n",
    "}\n",
    "\n",
    "dfs = [(\"er_df\", er_df), (\"ohlcv_df\", ohlcv_df), (\"spy_df\", spy_df)]\n",
    "\n",
    "for name, df in dfs:\n",
    "    missing = required_cols[name] - set(df.columns)\n",
    "    assert not missing, f\"{name} missing columns: {missing}\"\n",
    "    assert pd.api.types.is_datetime64_any_dtype(df[\"date_std\"]), f\"{name} date_std must be datetime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "094bad71-edb5-4e6b-a56e-5faf06146300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged stock data shape: 2837 rows × 17 cols\n",
      "SPY shape: 7703 rows × 8 cols\n",
      "Merged data preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>exchange</th>\n",
       "      <th>q</th>\n",
       "      <th>ticker</th>\n",
       "      <th>date_std</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "      <th>uncertainty</th>\n",
       "      <th>litigious</th>\n",
       "      <th>strong_modal</th>\n",
       "      <th>weak_modal</th>\n",
       "      <th>constraining</th>\n",
       "      <th>complexity</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aug 27, 2020, 9:00 p.m. ET</td>\n",
       "      <td>NASDAQ: BILI</td>\n",
       "      <td>2020-Q2</td>\n",
       "      <td>BILI</td>\n",
       "      <td>2020-08-28</td>\n",
       "      <td>0.004299</td>\n",
       "      <td>0.022356</td>\n",
       "      <td>0.006879</td>\n",
       "      <td>0.001376</td>\n",
       "      <td>0.006191</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.000172</td>\n",
       "      <td>45.7100</td>\n",
       "      <td>48.8600</td>\n",
       "      <td>45.5100</td>\n",
       "      <td>47.3900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nov 6, 2019, 12:00 p.m. ET</td>\n",
       "      <td>NASDAQ: BBSI</td>\n",
       "      <td>2019-Q3</td>\n",
       "      <td>BBSI</td>\n",
       "      <td>2019-11-06</td>\n",
       "      <td>0.007390</td>\n",
       "      <td>0.011442</td>\n",
       "      <td>0.010965</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>0.004648</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>86.9544</td>\n",
       "      <td>90.3949</td>\n",
       "      <td>84.1391</td>\n",
       "      <td>88.6084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Aug 7, 2019, 8:30 a.m. ET</td>\n",
       "      <td>NASDAQ: CSTE</td>\n",
       "      <td>2019-Q2</td>\n",
       "      <td>CSTE</td>\n",
       "      <td>2019-08-07</td>\n",
       "      <td>0.009237</td>\n",
       "      <td>0.019979</td>\n",
       "      <td>0.009667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.002793</td>\n",
       "      <td>12.5193</td>\n",
       "      <td>14.6062</td>\n",
       "      <td>12.0945</td>\n",
       "      <td>14.5777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         date      exchange        q ticker   date_std  \\\n",
       "0  Aug 27, 2020, 9:00 p.m. ET  NASDAQ: BILI  2020-Q2   BILI 2020-08-28   \n",
       "1  Nov 6, 2019, 12:00 p.m. ET  NASDAQ: BBSI  2019-Q3   BBSI 2019-11-06   \n",
       "2   Aug 7, 2019, 8:30 a.m. ET  NASDAQ: CSTE  2019-Q2   CSTE 2019-08-07   \n",
       "\n",
       "   negative  positive  uncertainty  litigious  strong_modal  weak_modal  \\\n",
       "0  0.004299  0.022356     0.006879   0.001376      0.006191    0.002752   \n",
       "1  0.007390  0.011442     0.010965   0.001073      0.002384    0.004648   \n",
       "2  0.009237  0.019979     0.009667   0.000000      0.003437    0.005800   \n",
       "\n",
       "   constraining  complexity     open     high      low    close  \n",
       "0      0.000688    0.000172  45.7100  48.8600  45.5100  47.3900  \n",
       "1      0.000596    0.000596  86.9544  90.3949  84.1391  88.6084  \n",
       "2      0.000859    0.002793  12.5193  14.6062  12.0945  14.5777  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPY preview:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>date_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1993-01-29</td>\n",
       "      <td>43.96875</td>\n",
       "      <td>43.96875</td>\n",
       "      <td>43.75000</td>\n",
       "      <td>43.93750</td>\n",
       "      <td>25.029377</td>\n",
       "      <td>1003200</td>\n",
       "      <td>1993-01-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1993-02-01</td>\n",
       "      <td>43.96875</td>\n",
       "      <td>44.25000</td>\n",
       "      <td>43.96875</td>\n",
       "      <td>44.25000</td>\n",
       "      <td>25.207405</td>\n",
       "      <td>480500</td>\n",
       "      <td>1993-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1993-02-02</td>\n",
       "      <td>44.21875</td>\n",
       "      <td>44.37500</td>\n",
       "      <td>44.12500</td>\n",
       "      <td>44.34375</td>\n",
       "      <td>25.260784</td>\n",
       "      <td>201300</td>\n",
       "      <td>1993-02-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Open      High       Low     Close  Adj Close   Volume  \\\n",
       "0  1993-01-29  43.96875  43.96875  43.75000  43.93750  25.029377  1003200   \n",
       "1  1993-02-01  43.96875  44.25000  43.96875  44.25000  25.207405   480500   \n",
       "2  1993-02-02  44.21875  44.37500  44.12500  44.34375  25.260784   201300   \n",
       "\n",
       "    date_std  \n",
       "0 1993-01-29  \n",
       "1 1993-02-01  \n",
       "2 1993-02-02  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data size summary\n",
    "print(f\"Merged stock data shape: {merged.shape[0]} rows × {merged.shape[1]} cols\")\n",
    "print(f\"SPY shape: {spy_df.shape[0]} rows × {spy_df.shape[1]} cols\")\n",
    "\n",
    "print(\"Merged data preview:\")\n",
    "display(merged.head(3))\n",
    "print(\"SPY preview:\")\n",
    "display(spy_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daf5f26-23f3-43f4-b866-99610e883726",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Does the dataset have any missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ff4608-f8ae-4af3-93c9-257c73846a1b",
   "metadata": {},
   "source": [
    "Looking at the datasets, the missingness analysis shows that only the Earnings Reports (ER) dataset contained missing data. Specifically, the column date_std had 380 missing values, accounting for approximately 2.03% of all rows. These missing entries likely stem from parsing issues or inconsistent metadata in the raw date column that prevented some dates from being successfully standardized. Because the date_std column is a critical key for aligning transcripts with market data and defining event windows, these rows cannot be reliably used in downstream analysis.\n",
    "\n",
    "For this exploratory phase, we will remove the affected rows, as their proportion is small enough that exclusion will not be materially affecting results or representativeness of the data. The OHLCV and SPY datasets showed no missing values, indicating that the market data and benchmark series are structurally complete and ready for use in return computations. In future iterations, we may implement a more robust date-parsing procedure to recover these records rather than discarding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0bce1bde-fd08-45bd-b2d8-349f3a9336e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data(df: pd.DataFrame, name: str) -> pd.DataFrame:\n",
    "    '''\n",
    "    Return a DataFrame of columns with missing values and their %.\n",
    "    Always returns columns: ['missing_count','missing_pct'] -> they \n",
    "    may be empty.\n",
    "    '''\n",
    "    total = df.shape[0]\n",
    "    missing = df.isna().sum()\n",
    "    missing = missing[missing > 0]\n",
    "\n",
    "    if missing.empty:\n",
    "        print(f\"{name}: no missing values\")\n",
    "        return missing.to_frame(\"missing_count\").assign(\n",
    "            missing_pct=pd.Series(dtype=float)) # so that DF is returned even if there are no missing vals\n",
    "\n",
    "    result = (\n",
    "        missing.to_frame(\"missing_count\") \n",
    "        # .assign  adds a column in a chain - lambda=take the chained df and calc percent in new col\n",
    "        .assign(missing_pct=lambda x: (x[\"missing_count\"] / total * 100).round(2))\n",
    "        .sort_values(\"missing_pct\", ascending=False)) \n",
    "\n",
    "    print(f\"{name} missing data:\")\n",
    "    display(result)\n",
    "    print()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "148e2ff5-2647-4099-aa75-a1064f66c62c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Earnings Reports (ER) missing data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_count</th>\n",
       "      <th>missing_pct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>date_std</th>\n",
       "      <td>380</td>\n",
       "      <td>2.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          missing_count  missing_pct\n",
       "date_std            380         2.03"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OHLCV: no missing values\n",
      "SPY: no missing values\n"
     ]
    }
   ],
   "source": [
    "er_miss    = missing_data(er_df,    \"Earnings Reports (ER)\")\n",
    "ohlcv_miss   = missing_data(ohlcv_df,   \"OHLCV\")\n",
    "spy_miss   = missing_data(spy_df,   \"SPY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba46941-35cf-48a2-b799-f754f0a79ce9",
   "metadata": {},
   "source": [
    "### Variables of Interest\n",
    "Across the three datasets, the variables of interest collectively span the corporate language seen in earnings reports to market behavior. In the Earnings Reports (ER) data, each observation corresponds to one firm’s earnings call (ticker, date_std), with the transcript text (transcript) processed into quantitative tone features. token_count measures call length, while pos_pct, neg_pct, and uncert_pct represent the normalized share of positive, negative, and uncertainty-related words, respectively. These tone variables provide the linguistic inputs for assessing whether executive sentiment has an influence on short-term price reactions.\n",
    "\n",
    "The OHLCV dataset records daily market activity for each firm, including open, high, low, and close, as well as computed daily returns (ret) from adjusted closing prices. This data quantifies firm-level market response, serving as the behavioral side of the tone–return relationship.\n",
    "\n",
    "The SPY dataset represents the benchmark market index, structured identically to OHLCV but aggregated at the market level. Its daily return (ret) acts as the baseline for calculating abnormal returns. Together, these variables form a coherent system: linguistic tone captures firm-level communication sentiment, OHLCV captures firm-specific market behavior, and SPY provides the market context necessary for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c3d74992-e002-4601-9de9-a746e5765337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>token_count</th>\n",
       "      <td>18755.0</td>\n",
       "      <td>7863.904452</td>\n",
       "      <td>2511.699771</td>\n",
       "      <td>562.0</td>\n",
       "      <td>6121.000000</td>\n",
       "      <td>7961.000000</td>\n",
       "      <td>9502.500000</td>\n",
       "      <td>32286.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_pct</th>\n",
       "      <td>18755.0</td>\n",
       "      <td>0.005711</td>\n",
       "      <td>0.002916</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>0.005352</td>\n",
       "      <td>0.007455</td>\n",
       "      <td>0.025175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg_pct</th>\n",
       "      <td>18755.0</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.011607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uncert_pct</th>\n",
       "      <td>18755.0</td>\n",
       "      <td>0.004685</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003583</td>\n",
       "      <td>0.004496</td>\n",
       "      <td>0.005588</td>\n",
       "      <td>0.021007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count         mean          std    min          25%  \\\n",
       "token_count  18755.0  7863.904452  2511.699771  562.0  6121.000000   \n",
       "pos_pct      18755.0     0.005711     0.002916    0.0     0.003540   \n",
       "neg_pct      18755.0     0.001069     0.000984    0.0     0.000458   \n",
       "uncert_pct   18755.0     0.004685     0.001582    0.0     0.003583   \n",
       "\n",
       "                     50%          75%           max  \n",
       "token_count  7961.000000  9502.500000  32286.000000  \n",
       "pos_pct         0.005352     0.007455      0.025175  \n",
       "neg_pct         0.000816     0.001374      0.011607  \n",
       "uncert_pct      0.004496     0.005588      0.021007  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "er_summary = er_df[[\"token_count\",\"pos_pct\",\"neg_pct\",\"uncert_pct\"]].describe().T\n",
    "display(er_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9dcb8539-99fd-4f72-bffb-71a5650d6076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>open</th>\n",
       "      <td>133557.0</td>\n",
       "      <td>4.119939e+07</td>\n",
       "      <td>1.538108e+09</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.78</td>\n",
       "      <td>10.5900</td>\n",
       "      <td>25.50</td>\n",
       "      <td>9.281250e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high</th>\n",
       "      <td>133557.0</td>\n",
       "      <td>4.301171e+07</td>\n",
       "      <td>1.608392e+09</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>5.49</td>\n",
       "      <td>11.2418</td>\n",
       "      <td>27.40</td>\n",
       "      <td>9.585000e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low</th>\n",
       "      <td>133557.0</td>\n",
       "      <td>3.898297e+07</td>\n",
       "      <td>1.450471e+09</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>5.13</td>\n",
       "      <td>10.6874</td>\n",
       "      <td>25.55</td>\n",
       "      <td>8.100000e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>close</th>\n",
       "      <td>133557.0</td>\n",
       "      <td>4.142139e+07</td>\n",
       "      <td>1.546763e+09</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>5.30</td>\n",
       "      <td>10.9300</td>\n",
       "      <td>26.50</td>\n",
       "      <td>9.180000e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count          mean           std     min   25%      50%    75%  \\\n",
       "open   133557.0  4.119939e+07  1.538108e+09  0.0000  4.78  10.5900  25.50   \n",
       "high   133557.0  4.301171e+07  1.608392e+09  0.0274  5.49  11.2418  27.40   \n",
       "low    133557.0  3.898297e+07  1.450471e+09  0.0210  5.13  10.6874  25.55   \n",
       "close  133557.0  4.142139e+07  1.546763e+09  0.0222  5.30  10.9300  26.50   \n",
       "\n",
       "                max  \n",
       "open   9.281250e+10  \n",
       "high   9.585000e+10  \n",
       "low    8.100000e+10  \n",
       "close  9.180000e+10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ohlcv_summary = ohlcv_df[[\"open\",\"high\",\"low\",\"close\"]].describe().T\n",
    "display(ohlcv_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "594470ef-54a5-4e5e-b6b3-2b2d93ed3b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Open</th>\n",
       "      <td>7703.0</td>\n",
       "      <td>1.675022e+02</td>\n",
       "      <td>1.033269e+02</td>\n",
       "      <td>43.343750</td>\n",
       "      <td>1.047700e+02</td>\n",
       "      <td>1.316400e+02</td>\n",
       "      <td>2.088650e+02</td>\n",
       "      <td>4.792200e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>High</th>\n",
       "      <td>7703.0</td>\n",
       "      <td>1.685138e+02</td>\n",
       "      <td>1.038956e+02</td>\n",
       "      <td>43.531250</td>\n",
       "      <td>1.056125e+02</td>\n",
       "      <td>1.324400e+02</td>\n",
       "      <td>2.097850e+02</td>\n",
       "      <td>4.799800e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Low</th>\n",
       "      <td>7703.0</td>\n",
       "      <td>1.663896e+02</td>\n",
       "      <td>1.027221e+02</td>\n",
       "      <td>42.812500</td>\n",
       "      <td>1.038500e+02</td>\n",
       "      <td>1.306800e+02</td>\n",
       "      <td>2.078300e+02</td>\n",
       "      <td>4.760600e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Close</th>\n",
       "      <td>7703.0</td>\n",
       "      <td>1.675101e+02</td>\n",
       "      <td>1.033526e+02</td>\n",
       "      <td>43.406250</td>\n",
       "      <td>1.048800e+02</td>\n",
       "      <td>1.315600e+02</td>\n",
       "      <td>2.088500e+02</td>\n",
       "      <td>4.777100e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Adj Close</th>\n",
       "      <td>7703.0</td>\n",
       "      <td>1.398787e+02</td>\n",
       "      <td>1.094186e+02</td>\n",
       "      <td>24.726746</td>\n",
       "      <td>7.082888e+01</td>\n",
       "      <td>9.370667e+01</td>\n",
       "      <td>1.812353e+02</td>\n",
       "      <td>4.665634e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Volume</th>\n",
       "      <td>7703.0</td>\n",
       "      <td>8.451404e+07</td>\n",
       "      <td>9.280526e+07</td>\n",
       "      <td>5200.000000</td>\n",
       "      <td>9.742550e+06</td>\n",
       "      <td>6.263260e+07</td>\n",
       "      <td>1.168936e+08</td>\n",
       "      <td>8.710263e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count          mean           std          min           25%  \\\n",
       "Open       7703.0  1.675022e+02  1.033269e+02    43.343750  1.047700e+02   \n",
       "High       7703.0  1.685138e+02  1.038956e+02    43.531250  1.056125e+02   \n",
       "Low        7703.0  1.663896e+02  1.027221e+02    42.812500  1.038500e+02   \n",
       "Close      7703.0  1.675101e+02  1.033526e+02    43.406250  1.048800e+02   \n",
       "Adj Close  7703.0  1.398787e+02  1.094186e+02    24.726746  7.082888e+01   \n",
       "Volume     7703.0  8.451404e+07  9.280526e+07  5200.000000  9.742550e+06   \n",
       "\n",
       "                    50%           75%           max  \n",
       "Open       1.316400e+02  2.088650e+02  4.792200e+02  \n",
       "High       1.324400e+02  2.097850e+02  4.799800e+02  \n",
       "Low        1.306800e+02  2.078300e+02  4.760600e+02  \n",
       "Close      1.315600e+02  2.088500e+02  4.777100e+02  \n",
       "Adj Close  9.370667e+01  1.812353e+02  4.665634e+02  \n",
       "Volume     6.263260e+07  1.168936e+08  8.710263e+08  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spy_summary = spy_df[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']].describe().T\n",
    "display(spy_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea3f7ef-f419-4c93-9bb7-b0ea6a8f0ea8",
   "metadata": {},
   "source": [
    "### Challenge Goals:\n",
    "The challenge goal for this EDA was to extend beyond transcript length and incorporate actual tone measures into the analysis. This was achieved by implementing an in-notebook sentiment analysis using a subset of the Loughran–McDonald dictionary to create normalized tone variables (pos_pct, neg_pct, uncert_pct). While these are prototype features for EDA, the structure directly supports future hypothesis testing and will scale easily when the full dictionary is applied for final analysis. No further challenge extensions (e.g., multi-dataset joins) in order to maintain focus on the core."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0542a985-208f-4c3a-b89a-08c7a7224df9",
   "metadata": {},
   "source": [
    "### Plan Evaluation\n",
    "The initial work plan proved fairly accurate in scope: dataset loading and cleaning took roughly the estimated time, while tone extraction and EDA each required slightly longer due to testing and validation. The decision to keep the datasets separate simplified memory use and avoided alignment errors. The current tasks completed - shape validation, missingness, tone feature generation, and visual summaries - align closely with the planned timeline. Remaining tasks include finalizing hypothesis testing and abnormal-return calculation, estimated to require one additional work session. Overall, the plan has remained realistic and on track."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547c6b09-9b07-46d1-86a5-660d369aa7e8",
   "metadata": {},
   "source": [
    "### Testing Overview (proof)\n",
    "Testing approach: All of the transformations feeding the EDA were validated with assertions and small doctests. The tone_counts() function was manually verified with sample strings to confirm correct tokenization and sentiment counting. Key tests include verifying that token_count >= 0, tone percentages lie within [0,1], and date_std is properly formatted as datetime. Plotting functions were not directly tested per rubric, but their inputs were validated through descriptive statistics and shape checks. Together, these confirm that the EDA results can be trusted and replicated on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2927ca7a-ce25-4e9a-bef5-d5f9839af023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume', 'date_std']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spy_df.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd9abf9-b291-4c82-b384-d50d4fd8410d",
   "metadata": {},
   "source": [
    "### Extras:\n",
    "##### Table Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9a52e551-956a-4f6d-ab3e-bd5c920aaa14",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticker\n",
       "False    15832\n",
       "True      2923\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "er['ticker'].isin(ohlcv['ticker']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e521eda3-2259-4359-bf18-e4a88495f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticker_compare (df1, col1, df2, col2):\n",
    "    '''\n",
    "    Given two dataframes and two column names, returns the values shared\n",
    "    between the two dataframe columns, for each dataframe (returned as a\n",
    "    tuple of two separate dataframes).\n",
    "\n",
    "    >>> df1 = pd.DataFrame({'ticker': ['AAPL', 'MSFT', 'GOOG'], 'price': [100, 200, 300]})\n",
    "    >>> df2 = pd.DataFrame({'ticker': ['AAPL', 'TSLA'], 'text': ['apple er', 'tesla er']})\n",
    "    >>> df1_common, df2_common = ticker_compare(df1, 'ticker', df2, 'ticker')\n",
    "    >>> sorted(df1_common['ticker'].unique())\n",
    "    ['AAPL']\n",
    "    >>> sorted(df1_common['price'].unique())\n",
    "    [100]\n",
    "    >>> sorted(df2_common['ticker'].unique())\n",
    "    ['AAPL']\n",
    "\n",
    "    >>> df3 = pd.DataFrame({'ticker': ['AMZN'], 'close': [150]})\n",
    "    >>> df4 = pd.DataFrame({'ticker': ['NFLX'], 'text': ['netflix']})\n",
    "    >>> df3_common, df4_common = ticker_compare(df3, 'ticker', df4, 'ticker')\n",
    "    >>> len(df3_common)\n",
    "    0\n",
    "    >>> len(df4_common)\n",
    "    0\n",
    "    >>> sorted(df2_common['text'].unique())\n",
    "    ['apple er']\n",
    "    '''\n",
    "    ticker_1 = set(df1[col1])\n",
    "    ticker_2 = set(df2[col2])\n",
    "    shared = ticker_1 & ticker_2\n",
    "\n",
    "    df1_common = df1[df1[col1].isin(shared)]\n",
    "    df2_common = df2[df2[col2].isin(shared)]\n",
    "\n",
    "    return df1_common, df2_common\n",
    "\n",
    "doctest.run_docstring_examples(ticker_compare, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35435aef-3c3d-41e7-9507-d056195b02c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlcv_common, er_common = ticker_compare(ohlcv, 'ticker', er, 'ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d3f8b510-8b77-4ba2-9a21-3fdf2ffb1f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>date_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300.000000</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.208552</td>\n",
       "      <td>9.468413</td>\n",
       "      <td>8.923588</td>\n",
       "      <td>9.201829</td>\n",
       "      <td>2008-02-29 22:38:24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.670000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>2.580000</td>\n",
       "      <td>1993-06-11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>4.250000</td>\n",
       "      <td>4.407500</td>\n",
       "      <td>4.080000</td>\n",
       "      <td>4.387500</td>\n",
       "      <td>1993-09-27 18:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10.622800</td>\n",
       "      <td>10.774850</td>\n",
       "      <td>10.411800</td>\n",
       "      <td>10.601200</td>\n",
       "      <td>2012-06-02 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11.417500</td>\n",
       "      <td>11.668275</td>\n",
       "      <td>11.160000</td>\n",
       "      <td>11.350000</td>\n",
       "      <td>2018-05-02 06:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>20.800000</td>\n",
       "      <td>22.700000</td>\n",
       "      <td>19.600000</td>\n",
       "      <td>21.280000</td>\n",
       "      <td>2018-08-17 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.411799</td>\n",
       "      <td>4.588522</td>\n",
       "      <td>4.216676</td>\n",
       "      <td>4.373212</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             open        high         low       close             date_std\n",
       "count  300.000000  300.000000  300.000000  300.000000                  300\n",
       "mean     9.208552    9.468413    8.923588    9.201829  2008-02-29 22:38:24\n",
       "min      2.500000    2.670000    2.420000    2.580000  1993-06-11 00:00:00\n",
       "25%      4.250000    4.407500    4.080000    4.387500  1993-09-27 18:00:00\n",
       "50%     10.622800   10.774850   10.411800   10.601200  2012-06-02 12:00:00\n",
       "75%     11.417500   11.668275   11.160000   11.350000  2018-05-02 06:00:00\n",
       "max     20.800000   22.700000   19.600000   21.280000  2018-08-17 00:00:00\n",
       "std      4.411799    4.588522    4.216676    4.373212                  NaN"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohlcv_common.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfa6f3e-13bc-4b32-99cc-ec230e56a4a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f64e9689-4fc7-4a2d-b5e5-fb15dffd1052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281/23:\n",
      "def plot_county_population_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame) -> Axes:\n",
      "    \"\"\"\n",
      "    Given a state population geo dataframe and a geodataframe for the state background, plots and returns\n",
      "    a map of Washington counties shaded by their population level (as an Axes object) with counties lacking\n",
      "    data shaded in grey.\n",
      "    \"\"\"\n",
      "\n",
      "    fig, ax = plt.subplots(figsize=(10,5))\n",
      "    state_background.plot(ax=ax, color=\"#EEE\")\n",
      "    \n",
      "    counties = state_data[[\"geometry\", \"County\", \"POP2010\"]].dissolve(by=\"County\", aggfunc=\"sum\")\n",
      "    counties.plot(ax=ax, \n",
      "                  column=\"POP2010\", \n",
      "                  legend=True)\n",
      "\n",
      "    ax.set_title(\"Washington County Populations\")\n",
      "    ax.set_axis_off()\n",
      "\n",
      "    return ax\n",
      "\n",
      "\n",
      "ax = plot_county_population_map(state_data, entire_state)\n",
      "assert type(ax) == Axes, \"this function should return an Axes object\"\n",
      "layers = ax.findobj(PatchCollection)\n",
      "assert len(layers) == 2, \"expected to have 2 layers (one background and one foreground)\"\\\n",
      "                         +f\" but got {len(layers)} layer(s)\"\n",
      "assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "assert_patches_allclose(layers[1], geoms=all_counties.geometry, num_colors=20, layer=1)\n",
      "assert ax.get_title() == \"Washington County Populations\", \"title does not match expected\"\n",
      "assert not ax.axison, \"borders and labels must be hidden\"\n",
      "cbar = ax.get_figure().get_axes()[-1]\n",
      "assert cbar.get_label() == \"<colorbar>\", \"missing legend\"\n",
      "assert ax.bbox.height == cbar.bbox.height, \"map can be enlarged\"1 = plt.subplots(2, 2, figsize=(15,10))\n",
      "    state_background.plot(ax=ax, color=\"#EEE\")\n",
      "\n",
      "    return ax\n",
      "281/24:\n",
      "def plot_county_population_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame) -> Axes:\n",
      "    \"\"\"\n",
      "    Given a state population geo dataframe and a geodataframe for the state background, plots and returns\n",
      "    a map of Washington counties shaded by their population level (as an Axes object) with counties lacking\n",
      "    data shaded in grey.\n",
      "    \"\"\"\n",
      "\n",
      "    fig, ax = plt.subplots(figsize=(10,5))\n",
      "    state_background.plot(ax=ax, color=\"#EEE\")\n",
      "    \n",
      "    counties = state_data[[\"geometry\", \"County\", \"POP2010\"]].dissolve(by=\"County\", aggfunc=\"sum\")\n",
      "    counties.plot(ax=ax, \n",
      "                  column=\"POP2010\", \n",
      "                  legend=True)\n",
      "\n",
      "    ax.set_title(\"Washington County Populations\")\n",
      "    ax.set_axis_off()\n",
      "\n",
      "    return ax\n",
      "\n",
      "\n",
      "ax = plot_county_population_map(state_data, entire_state)\n",
      "assert type(ax) == Axes, \"this function should return an Axes object\"\n",
      "layers = ax.findobj(PatchCollection)\n",
      "assert len(layers) == 2, \"expected to have 2 layers (one background and one foreground)\"\\\n",
      "                         +f\" but got {len(layers)} layer(s)\"\n",
      "assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "assert_patches_allclose(layers[1], geoms=all_counties.geometry, num_colors=20, layer=1)\n",
      "assert ax.get_title() == \"Washington County Populations\", \"title does not match expected\"\n",
      "assert not ax.axison, \"borders and labels must be hidden\"\n",
      "cbar = ax.get_figure().get_axes()[-1]\n",
      "assert cbar.get_label() == \"<colorbar>\", \"missing legend\"\n",
      "assert ax.bbox.height == cbar.bbox.height, \"map can be enlarged\"1 = plt.subplots(2, 2, figsize=(15,10))\n",
      "    state_background.plot(ax=ax, color=\"#EEE\")\n",
      "\n",
      "    return ax\n",
      "281/25:\n",
      "def plot_county_population_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame) -> Axes:\n",
      "    \"\"\"\n",
      "    Given a state population geo dataframe and a geodataframe for the state background, plots and returns\n",
      "    a map of Washington counties shaded by their population level (as an Axes object) with counties lacking\n",
      "    data shaded in grey.\n",
      "    \"\"\"\n",
      "\n",
      "    fig, ax = plt.subplots(figsize=(10,5))\n",
      "    state_background.plot(ax=ax, color=\"#EEE\")\n",
      "    \n",
      "    counties = state_data[[\"geometry\", \"County\", \"POP2010\"]].dissolve(by=\"County\", aggfunc=\"sum\")\n",
      "    counties.plot(ax=ax, \n",
      "                  column=\"POP2010\", \n",
      "                  legend=True)\n",
      "\n",
      "    ax.set_title(\"Washington County Populations\")\n",
      "    ax.set_axis_off()\n",
      "\n",
      "    return ax\n",
      "\n",
      "\n",
      "ax = plot_county_population_map(state_data, entire_state)\n",
      "assert type(ax) == Axes, \"this function should return an Axes object\"\n",
      "layers = ax.findobj(PatchCollection)\n",
      "assert len(layers) == 2, \"expected to have 2 layers (one background and one foreground)\"\\\n",
      "                         +f\" but got {len(layers)} layer(s)\"\n",
      "assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "assert_patches_allclose(layers[1], geoms=all_counties.geometry, num_colors=20, layer=1)\n",
      "assert ax.get_title() == \"Washington County Populations\", \"title does not match expected\"\n",
      "assert not ax.axison, \"borders and labels must be hidden\"\n",
      "cbar = ax.get_figure().get_axes()[-1]\n",
      "assert cbar.get_label() == \"<colorbar>\", \"missing legend\"\n",
      "assert ax.bbox.height == cbar.bbox.height, \"map can be enlarged\"1 = plt.subplots(2, 2, figsize=(15,10))\n",
      "    state_background.plot(ax=ax, color=\"#EEE\")\n",
      "281/26:\n",
      "def plot_county_population_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame) -> Axes:\n",
      "    \"\"\"\n",
      "    Given a state population geo dataframe and a geodataframe for the state background, plots and returns\n",
      "    a map of Washington counties shaded by their population level (as an Axes object) with counties lacking\n",
      "    data shaded in grey.\n",
      "    \"\"\"\n",
      "\n",
      "    fig, ax = plt.subplots(figsize=(10,5))\n",
      "    state_background.plot(ax=ax, color=\"#EEE\")\n",
      "    \n",
      "    counties = state_data[[\"geometry\", \"County\", \"POP2010\"]].dissolve(by=\"County\", aggfunc=\"sum\")\n",
      "    counties.plot(ax=ax, \n",
      "                  column=\"POP2010\", \n",
      "                  legend=True)\n",
      "\n",
      "    ax.set_title(\"Washington County Populations\")\n",
      "    ax.set_axis_off()\n",
      "\n",
      "    return ax\n",
      "\n",
      "\n",
      "ax = plot_county_population_map(state_data, entire_state)\n",
      "assert type(ax) == Axes, \"this function should return an Axes object\"\n",
      "layers = ax.findobj(PatchCollection)\n",
      "assert len(layers) == 2, \"expected to have 2 layers (one background and one foreground)\"\\\n",
      "                         +f\" but got {len(layers)} layer(s)\"\n",
      "assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "assert_patches_allclose(layers[1], geoms=all_counties.geometry, num_colors=20, layer=1)\n",
      "assert ax.get_title() == \"Washington County Populations\", \"title does not match expected\"\n",
      "assert not ax.axison, \"borders and labels must be hidden\"\n",
      "cbar = ax.get_figure().get_axes()[-1]\n",
      "assert cbar.get_label() == \"<colorbar>\", \"missing legend\"\n",
      "assert ax.bbox.height == cbar.bbox.height, \"map can be enlarged\" = plt.subplots(2, 2, figsize=(15,10))\n",
      "    state_background.plot(ax=ax, color=\"#EEE\")\n",
      "281/27:\n",
      "def plot_county_population_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame) -> Axes:\n",
      "    \"\"\"\n",
      "    Given a state population geo dataframe and a geodataframe for the state background, plots and returns\n",
      "    a map of Washington counties shaded by their population level (as an Axes object) with counties lacking\n",
      "    data shaded in grey.\n",
      "    \"\"\"\n",
      "\n",
      "    fig, ax = plt.subplots(figsize=(10,5))\n",
      "    state_background.plot(ax=ax, color=\"#EEE\")\n",
      "    \n",
      "    counties = state_data[[\"geometry\", \"County\", \"POP2010\"]].dissolve(by=\"County\", aggfunc=\"sum\")\n",
      "    counties.plot(ax=ax, \n",
      "                  column=\"POP2010\", \n",
      "                  legend=True)\n",
      "\n",
      "    ax.set_title(\"Washington County Populations\")\n",
      "    ax.set_axis_off()\n",
      "\n",
      "    return ax\n",
      "\n",
      "\n",
      "ax = plot_county_population_map(state_data, entire_state)\n",
      "assert type(ax) == Axes, \"this function should return an Axes object\"\n",
      "layers = ax.findobj(PatchCollection)\n",
      "assert len(layers) == 2, \"expected to have 2 layers (one background and one foreground)\"\\\n",
      "                         +f\" but got {len(layers)} layer(s)\"\n",
      "assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "assert_patches_allclose(layers[1], geoms=all_counties.geometry, num_colors=20, layer=1)\n",
      "assert ax.get_title() == \"Washington County Populations\", \"title does not match expected\"\n",
      "assert not ax.axison, \"borders and labels must be hidden\"\n",
      "cbar = ax.get_figure().get_axes()[-1]\n",
      "assert cbar.get_label() == \"<colorbar>\", \"missing legend\"\n",
      "assert ax.bbox.height == cbar.bbox.height, \"map can be enlarged\"\n",
      "281/28:\n",
      "def plot_food_access_by_county_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame):\n",
      "    \"\"\"\n",
      "    ...\n",
      "    \"\"\"\n",
      "    fig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(2, 2, figsize=(15, 10))\n",
      "\n",
      "    county_data = state_data[['geometry', 'County', 'POP2010', \n",
      "                               'lapophalf', 'lalowihalf', \n",
      "                               'lapop10', 'lalowi10']].dissolve(by='County', aggfunc='sum')\n",
      "\n",
      "    subplot_data = [('lapophalf', 'lahalf_prop', \"Low Access: Half Mile\"),\n",
      "        ('lalowihalf', 'lalowihalf_prop', \"Low Access + Low Income: Half Mile\"),\n",
      "        ('lapop10', 'la10_prop', \"Low Access: 10 Miles\"),\n",
      "        ('lalowi10', 'lalowi10_prop', \"Low Access + Low Income: 10 Miles\")]\n",
      "    \n",
      "\n",
      "\n",
      "axs = plot_food_access_by_county_map(state_data, entire_state)\n",
      "assert axs is not None, \"this function should return a 4-tuple of Axes objects\"\n",
      "assert len(axs) == 4, \"the returned tuple's length should be 4\"\n",
      "expected_titles = [\"Low Access: Half Mile\", \"Low Access + Low Income: Half Mile\",\n",
      "                   \"Low Access: 10 Miles\", \"Low Access + Low Income: 10 Miles\"]\n",
      "for ax, expected_num_colors, expected_title in zip(axs, [31, 23, 19, 16], expected_titles):\n",
      "    assert type(ax) == Axes, \"each value in the returned tuple should be an Axes object\"\n",
      "    layers = ax.findobj(PatchCollection)\n",
      "    assert len(layers) == 2, \"expected to have 2 layers (one background and one foreground)\"\\\n",
      "                             +f\" but got {len(layers)} layer(s)\"\n",
      "    assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "    assert_patches_allclose(layers[1], geoms=all_counties.geometry, num_colors=expected_num_colors, layer=1)\n",
      "    assert ax.get_title() == expected_title, f\"title {ax.get_title()} does not match expected\"\n",
      "    assert ax.get_legend() is None, \"plot should not have a legend\"\n",
      "    assert not ax.axison, \"borders and labels must be hidden\"\n",
      "281/29:\n",
      "def plot_food_access_by_county_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame):\n",
      "    \"\"\"\n",
      "    Given state population data and an outline of the state background in two geo data frames, returns\n",
      "    4 subplots showing the food access levels by county, dilineating food access by distance and \n",
      "    low income validation.\n",
      "    \"\"\"\n",
      "    fig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(2, 2, figsize=(15, 10))\n",
      "\n",
      "    county_data = state_data[['geometry', 'County', 'POP2010', \n",
      "                               'lapophalf', 'lalowihalf', \n",
      "                               'lapop10', 'lalowi10']].dissolve(by='County', aggfunc='sum')\n",
      "\n",
      "    subplot_data = [('lapophalf', 'lahalf_prop', \"Low Access: Half Mile\"),\n",
      "        ('lalowihalf', 'lalowihalf_prop', \"Low Access + Low Income: Half Mile\"),\n",
      "        ('lapop10', 'la10_prop', \"Low Access: 10 Miles\"),\n",
      "        ('lalowi10', 'lalowi10_prop', \"Low Access + Low Income: 10 Miles\")]\n",
      "    \n",
      "    for (numer_col, prop_col, title), ax in zip(plot_configs, [ax1, ax2, ax3, ax4]):\n",
      "        county_data[prop_col] = county_data[numer_col] / county_data['POP2010']\n",
      "\n",
      "        state_background.plot(ax=ax, color=\"#EEE\")\n",
      "        county_data.plot(ax=ax, column=prop_col, vmin=0, vmax=1)\n",
      "\n",
      "        ax.set_title(title)\n",
      "        ax.set_axis_off()\n",
      "\n",
      "    return(ax1, ax2, ax3, ax4)\n",
      "\n",
      "axs = plot_food_access_by_county_map(state_data, entire_state)\n",
      "assert axs is not None, \"this function should return a 4-tuple of Axes objects\"\n",
      "assert len(axs) == 4, \"the returned tuple's length should be 4\"\n",
      "expected_titles = [\"Low Access: Half Mile\", \"Low Access + Low Income: Half Mile\",\n",
      "                   \"Low Access: 10 Miles\", \"Low Access + Low Income: 10 Miles\"]\n",
      "for ax, expected_num_colors, expected_title in zip(axs, [31, 23, 19, 16], expected_titles):\n",
      "    assert type(ax) == Axes, \"each value in the returned tuple should be an Axes object\"\n",
      "    layers = ax.findobj(PatchCollection)\n",
      "    assert len(layers) == 2, \"expected to have 2 layers (one background and one foreground)\"\\\n",
      "                             +f\" but got {len(layers)} layer(s)\"\n",
      "    assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "    assert_patches_allclose(layers[1], geoms=all_counties.geometry, num_colors=expected_num_colors, layer=1)\n",
      "    assert ax.get_title() == expected_title, f\"title {ax.get_title()} does not match expected\"\n",
      "    assert ax.get_legend() is None, \"plot should not have a legend\"\n",
      "    assert not ax.axison, \"borders and labels must be hidden\"\n",
      "281/30:\n",
      "def plot_food_access_by_county_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame):\n",
      "    \"\"\"\n",
      "    Given state population data and an outline of the state background in two geo data frames, returns\n",
      "    4 subplots showing the food access levels by county, dilineating food access by distance and \n",
      "    low income validation.\n",
      "    \"\"\"\n",
      "    fig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(2, 2, figsize=(15, 10))\n",
      "\n",
      "    county_data = state_data[['geometry', 'County', 'POP2010', \n",
      "                               'lapophalf', 'lalowihalf', \n",
      "                               'lapop10', 'lalowi10']].dissolve(by='County', aggfunc='sum')\n",
      "\n",
      "    subplot_data = [('lapophalf', 'lahalf_prop', \"Low Access: Half Mile\"),\n",
      "        ('lalowihalf', 'lalowihalf_prop', \"Low Access + Low Income: Half Mile\"),\n",
      "        ('lapop10', 'la10_prop', \"Low Access: 10 Miles\"),\n",
      "        ('lalowi10', 'lalowi10_prop', \"Low Access + Low Income: 10 Miles\")]\n",
      "    \n",
      "    for (numer_col, prop_col, title), ax in zip(subplot_data, [ax1, ax2, ax3, ax4]):\n",
      "        county_data[prop_col] = county_data[numer_col] / county_data['POP2010']\n",
      "\n",
      "        state_background.plot(ax=ax, color=\"#EEE\")\n",
      "        county_data.plot(ax=ax, column=prop_col, vmin=0, vmax=1)\n",
      "\n",
      "        ax.set_title(title)\n",
      "        ax.set_axis_off()\n",
      "\n",
      "    return(ax1, ax2, ax3, ax4)\n",
      "\n",
      "axs = plot_food_access_by_county_map(state_data, entire_state)\n",
      "assert axs is not None, \"this function should return a 4-tuple of Axes objects\"\n",
      "assert len(axs) == 4, \"the returned tuple's length should be 4\"\n",
      "expected_titles = [\"Low Access: Half Mile\", \"Low Access + Low Income: Half Mile\",\n",
      "                   \"Low Access: 10 Miles\", \"Low Access + Low Income: 10 Miles\"]\n",
      "for ax, expected_num_colors, expected_title in zip(axs, [31, 23, 19, 16], expected_titles):\n",
      "    assert type(ax) == Axes, \"each value in the returned tuple should be an Axes object\"\n",
      "    layers = ax.findobj(PatchCollection)\n",
      "    assert len(layers) == 2, \"expected to have 2 layers (one background and one foreground)\"\\\n",
      "                             +f\" but got {len(layers)} layer(s)\"\n",
      "    assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "    assert_patches_allclose(layers[1], geoms=all_counties.geometry, num_colors=expected_num_colors, layer=1)\n",
      "    assert ax.get_title() == expected_title, f\"title {ax.get_title()} does not match expected\"\n",
      "    assert ax.get_legend() is None, \"plot should not have a legend\"\n",
      "    assert not ax.axison, \"borders and labels must be hidden\"\n",
      "281/31:\n",
      "def plot_food_access_by_county_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame):\n",
      "    \"\"\"\n",
      "    Given state population data and an outline of the state background in two geo data frames, returns\n",
      "    4 subplots showing the food access levels by county, visualizng food access by distance (half mile\n",
      "    and 10 miles), and income level (low-income vs. not low-income). \n",
      "    \"\"\"\n",
      "    fig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(2, 2, figsize=(15, 10))\n",
      "\n",
      "    county_data = state_data[['geometry', 'County', 'POP2010', \n",
      "                               'lapophalf', 'lalowihalf', \n",
      "                               'lapop10', 'lalowi10']].dissolve(by='County', aggfunc='sum')\n",
      "\n",
      "    subplot_data = [('lapophalf', 'lahalf_prop', \"Low Access: Half Mile\"),\n",
      "        ('lalowihalf', 'lalowihalf_prop', \"Low Access + Low Income: Half Mile\"),\n",
      "        ('lapop10', 'la10_prop', \"Low Access: 10 Miles\"),\n",
      "        ('lalowi10', 'lalowi10_prop', \"Low Access + Low Income: 10 Miles\")]\n",
      "    \n",
      "    for (numer_col, prop_col, title), ax in zip(subplot_data, [ax1, ax2, ax3, ax4]):\n",
      "        county_data[prop_col] = county_data[numer_col] / county_data['POP2010']\n",
      "\n",
      "        state_background.plot(ax=ax, color=\"#EEE\")\n",
      "        county_data.plot(ax=ax, column=prop_col, vmin=0, vmax=1)\n",
      "\n",
      "        ax.set_title(title)\n",
      "        ax.set_axis_off()\n",
      "\n",
      "    return(ax1, ax2, ax3, ax4)\n",
      "\n",
      "axs = plot_food_access_by_county_map(state_data, entire_state)\n",
      "assert axs is not None, \"this function should return a 4-tuple of Axes objects\"\n",
      "assert len(axs) == 4, \"the returned tuple's length should be 4\"\n",
      "expected_titles = [\"Low Access: Half Mile\", \"Low Access + Low Income: Half Mile\",\n",
      "                   \"Low Access: 10 Miles\", \"Low Access + Low Income: 10 Miles\"]\n",
      "for ax, expected_num_colors, expected_title in zip(axs, [31, 23, 19, 16], expected_titles):\n",
      "    assert type(ax) == Axes, \"each value in the returned tuple should be an Axes object\"\n",
      "    layers = ax.findobj(PatchCollection)\n",
      "    assert len(layers) == 2, \"expected to have 2 layers (one background and one foreground)\"\\\n",
      "                             +f\" but got {len(layers)} layer(s)\"\n",
      "    assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "    assert_patches_allclose(layers[1], geoms=all_counties.geometry, num_colors=expected_num_colors, layer=1)\n",
      "    assert ax.get_title() == expected_title, f\"title {ax.get_title()} does not match expected\"\n",
      "    assert ax.get_legend() is None, \"plot should not have a legend\"\n",
      "    assert not ax.axison, \"borders and labels must be hidden\"\n",
      "281/32:\n",
      "def plot_census_low_access_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame) -> Axes:\n",
      "    \"\"\"\n",
      "    ...\n",
      "    \"\"\"\n",
      "    ax = state_background.plot(color=\"#EEE\")\n",
      "    \n",
      "    urban = state_data[state_data['Urban'] == 1]\n",
      "    urban.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    rural = state_data[state_data['Rural'] == 1]\n",
      "    rural.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    urban_low_acc = urban[\n",
      "        (urban['lapophalf'] >= 500) | \n",
      "        (urban['lapophalf'] >= 0.33 * urban['POP2010'])]\n",
      "    \n",
      "    urban_low_acc.plot(ax=ax)\n",
      "    \n",
      "    rural_low_acc = rural[\n",
      "        (rural['lapop10'] >= 500) |\n",
      "        (rural['lapop10'] >= 0.33 * rural['POP2010'])]\n",
      "    \n",
      "    rural_low_acc.plot(ax=ax)\n",
      "    \n",
      "    ax.set_title(\"Low Access Census Tracts\")\n",
      "    ax.set_axis_off()\n",
      "    return ax\n",
      "\n",
      "ax = plot_census_low_access_map(state_data, entire_state)\n",
      "assert type(ax) == Axes, \"this function should return an Axes object\"\n",
      "layers = ax.findobj(PatchCollection)\n",
      "assert len(layers) == 5, \"expected to have 5 layers (one background and 4 foregrounds)\"\\\n",
      "                         +f\" but got {len(layers)} layer(s)\"\n",
      "assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "urban_idx = state_data[\"Urban\"].notna() & ~state_data.index.isin(rural_idx)\n",
      "urban_la_idx = urban_idx & ~state_data.index.isin(urban_ha_idx)\n",
      "try:\n",
      "    assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    assert_patches_allclose(layers[2], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=2)\n",
      "    assert_patches_allclose(layers[3], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=3)\n",
      "    assert_patches_allclose(layers[4], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=4)\n",
      "    ## Debugging tip: if you want to check one layer, comment out the four lines above\n",
      "    ## and use each of the following lines ONE AT A TIME\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=1)\n",
      "except AssertionError as e:\n",
      "    # this message was to remind you to plot in the correct order; you can ignore if you just kept one layer\n",
      "    print(\"Please make sure the order of your plot is correct, and also check any other errors below.\")\n",
      "    raise e\n",
      "assert ax.get_title() == \"Low Access Census Tracts\", \"title does not match expected\"\n",
      "assert not ax.axison, \"borders and labels must be hidden\"\n",
      "281/33:\n",
      "def plot_census_low_access_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame) -> Axes:\n",
      "    \"\"\"\n",
      "    ...\n",
      "    \"\"\"\n",
      "    ax = state_background.plot(color=\"#EEE\")\n",
      "    \n",
      "    # urban = state_data[state_data['Urban'] == 1]\n",
      "    # urban.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    # rural = state_data[state_data['Rural'] == 1]\n",
      "    # rural.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    # urban_low_acc = urban[\n",
      "    #     (urban['lapophalf'] >= 500) | \n",
      "    #     (urban['lapophalf'] >= 0.33 * urban['POP2010'])]\n",
      "    \n",
      "    # urban_low_acc.plot(ax=ax)\n",
      "    \n",
      "    # rural_low_acc = rural[\n",
      "    #     (rural['lapop10'] >= 500) |\n",
      "    #     (rural['lapop10'] >= 0.33 * rural['POP2010'])]\n",
      "    \n",
      "    # rural_low_acc.plot(ax=ax)\n",
      "    \n",
      "    ax.set_title(\"Low Access Census Tracts\")\n",
      "    ax.set_axis_off()\n",
      "    return ax\n",
      "\n",
      "ax = plot_census_low_access_map(state_data, entire_state)\n",
      "assert type(ax) == Axes, \"this function should return an Axes object\"\n",
      "layers = ax.findobj(PatchCollection)\n",
      "assert len(layers) == 5, \"expected to have 5 layers (one background and 4 foregrounds)\"\\\n",
      "                         +f\" but got {len(layers)} layer(s)\"\n",
      "assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "urban_idx = state_data[\"Urban\"].notna() & ~state_data.index.isin(rural_idx)\n",
      "urban_la_idx = urban_idx & ~state_data.index.isin(urban_ha_idx)\n",
      "try:\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[2], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=2)\n",
      "    # assert_patches_allclose(layers[3], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=3)\n",
      "    # assert_patches_allclose(layers[4], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=4)\n",
      "    ## Debugging tip: if you want to check one layer, comment out the four lines above\n",
      "    ## and use each of the following lines ONE AT A TIME\n",
      "    assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=1)\n",
      "except AssertionError as e:\n",
      "    # this message was to remind you to plot in the correct order; you can ignore if you just kept one layer\n",
      "    print(\"Please make sure the order of your plot is correct, and also check any other errors below.\")\n",
      "    raise e\n",
      "assert ax.get_title() == \"Low Access Census Tracts\", \"title does not match expected\"\n",
      "assert not ax.axison, \"borders and labels must be hidden\"\n",
      "281/34:\n",
      "def plot_census_low_access_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame) -> Axes:\n",
      "    \"\"\"\n",
      "    ...\n",
      "    \"\"\"\n",
      "    ax = state_background.plot(color=\"#EEE\")\n",
      "    \n",
      "    # urban = state_data[state_data['Urban'] == 1]\n",
      "    # urban.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    # rural = state_data[state_data['Rural'] == 1]\n",
      "    # rural.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    # urban_low_acc = urban[\n",
      "    #     (urban['lapophalf'] >= 500) | \n",
      "    #     (urban['lapophalf'] >= 0.33 * urban['POP2010'])]\n",
      "    \n",
      "    # urban_low_acc.plot(ax=ax)\n",
      "    \n",
      "    # rural_low_acc = rural[\n",
      "    #     (rural['lapop10'] >= 500) |\n",
      "    #     (rural['lapop10'] >= 0.33 * rural['POP2010'])]\n",
      "    \n",
      "    # rural_low_acc.plot(ax=ax)\n",
      "    \n",
      "    ax.set_title(\"Low Access Census Tracts\")\n",
      "    ax.set_axis_off()\n",
      "    return ax\n",
      "\n",
      "ax = plot_census_low_access_map(state_data, entire_state)\n",
      "assert type(ax) == Axes, \"this function should return an Axes object\"\n",
      "layers = ax.findobj(PatchCollection)\n",
      "# assert len(layers) == 5, \"expected to have 5 layers (one background and 4 foregrounds)\"\\\n",
      "#                          +f\" but got {len(layers)} layer(s)\"\n",
      "assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "urban_idx = state_data[\"Urban\"].notna() & ~state_data.index.isin(rural_idx)\n",
      "urban_la_idx = urban_idx & ~state_data.index.isin(urban_ha_idx)\n",
      "try:\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[2], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=2)\n",
      "    # assert_patches_allclose(layers[3], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=3)\n",
      "    # assert_patches_allclose(layers[4], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=4)\n",
      "    ## Debugging tip: if you want to check one layer, comment out the four lines above\n",
      "    ## and use each of the following lines ONE AT A TIME\n",
      "    assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=1)\n",
      "except AssertionError as e:\n",
      "    # this message was to remind you to plot in the correct order; you can ignore if you just kept one layer\n",
      "    print(\"Please make sure the order of your plot is correct, and also check any other errors below.\")\n",
      "    raise e\n",
      "assert ax.get_title() == \"Low Access Census Tracts\", \"title does not match expected\"\n",
      "assert not ax.axison, \"borders and labels must be hidden\"\n",
      "281/35:\n",
      "def plot_census_low_access_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame) -> Axes:\n",
      "    \"\"\"\n",
      "    ...\n",
      "    \"\"\"\n",
      "    ax = state_background.plot(color=\"#EEE\")\n",
      "    \n",
      "    urban = state_data[state_data['Urban'] == 1]\n",
      "    urban.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    # rural = state_data[state_data['Rural'] == 1]\n",
      "    # rural.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    # urban_low_acc = urban[\n",
      "    #     (urban['lapophalf'] >= 500) | \n",
      "    #     (urban['lapophalf'] >= 0.33 * urban['POP2010'])]\n",
      "    \n",
      "    # urban_low_acc.plot(ax=ax)\n",
      "    \n",
      "    # rural_low_acc = rural[\n",
      "    #     (rural['lapop10'] >= 500) |\n",
      "    #     (rural['lapop10'] >= 0.33 * rural['POP2010'])]\n",
      "    \n",
      "    # rural_low_acc.plot(ax=ax)\n",
      "    \n",
      "    ax.set_title(\"Low Access Census Tracts\")\n",
      "    ax.set_axis_off()\n",
      "    return ax\n",
      "\n",
      "ax = plot_census_low_access_map(state_data, entire_state)\n",
      "assert type(ax) == Axes, \"this function should return an Axes object\"\n",
      "layers = ax.findobj(PatchCollection)\n",
      "# assert len(layers) == 5, \"expected to have 5 layers (one background and 4 foregrounds)\"\\\n",
      "#                          +f\" but got {len(layers)} layer(s)\"\n",
      "assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "urban_idx = state_data[\"Urban\"].notna() & ~state_data.index.isin(rural_idx)\n",
      "urban_la_idx = urban_idx & ~state_data.index.isin(urban_ha_idx)\n",
      "try:\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[2], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=2)\n",
      "    # assert_patches_allclose(layers[3], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=3)\n",
      "    # assert_patches_allclose(layers[4], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=4)\n",
      "    ## Debugging tip: if you want to check one layer, comment out the four lines above\n",
      "    ## and use each of the following lines ONE AT A TIME\n",
      "    assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=1)\n",
      "except AssertionError as e:\n",
      "    # this message was to remind you to plot in the correct order; you can ignore if you just kept one layer\n",
      "    print(\"Please make sure the order of your plot is correct, and also check any other errors below.\")\n",
      "    raise e\n",
      "assert ax.get_title() == \"Low Access Census Tracts\", \"title does not match expected\"\n",
      "assert not ax.axison, \"borders and labels must be hidden\"\n",
      "281/36:\n",
      "def plot_census_low_access_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame) -> Axes:\n",
      "    \"\"\"\n",
      "    ...\n",
      "    \"\"\"\n",
      "    ax = state_background.plot(color=\"#EEE\")\n",
      "    \n",
      "    urban = state_data[state_data['Urban'] == 1]\n",
      "    urban.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    # rural = state_data[state_data['Rural'] == 1]\n",
      "    # rural.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    # urban_low_acc = urban[\n",
      "    #     (urban['lapophalf'] >= 500) | \n",
      "    #     (urban['lapophalf'] >= 0.33 * urban['POP2010'])]\n",
      "    \n",
      "    # urban_low_acc.plot(ax=ax)\n",
      "    \n",
      "    # rural_low_acc = rural[\n",
      "    #     (rural['lapop10'] >= 500) |\n",
      "    #     (rural['lapop10'] >= 0.33 * rural['POP2010'])]\n",
      "    \n",
      "    # rural_low_acc.plot(ax=ax)\n",
      "    \n",
      "    ax.set_title(\"Low Access Census Tracts\")\n",
      "    ax.set_axis_off()\n",
      "    return ax\n",
      "\n",
      "ax = plot_census_low_access_map(state_data, entire_state)\n",
      "assert type(ax) == Axes, \"this function should return an Axes object\"\n",
      "layers = ax.findobj(PatchCollection)\n",
      "# assert len(layers) == 5, \"expected to have 5 layers (one background and 4 foregrounds)\"\\\n",
      "#                          +f\" but got {len(layers)} layer(s)\"\n",
      "assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "urban_idx = state_data[\"Urban\"].notna() & ~state_data.index.isin(rural_idx)\n",
      "urban_la_idx = urban_idx & ~state_data.index.isin(urban_ha_idx)\n",
      "try:\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[2], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=2)\n",
      "    # assert_patches_allclose(layers[3], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=3)\n",
      "    # assert_patches_allclose(layers[4], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=4)\n",
      "    ## Debugging tip: if you want to check one layer, comment out the four lines above\n",
      "    ## and use each of the following lines ONE AT A TIME\n",
      "    assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    assert_patches_allclose(layers[1], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=1)\n",
      "except AssertionError as e:\n",
      "    # this message was to remind you to plot in the correct order; you can ignore if you just kept one layer\n",
      "    print(\"Please make sure the order of your plot is correct, and also check any other errors below.\")\n",
      "    raise e\n",
      "assert ax.get_title() == \"Low Access Census Tracts\", \"title does not match expected\"\n",
      "assert not ax.axison, \"borders and labels must be hidden\"\n",
      "281/37:\n",
      "def plot_census_low_access_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame) -> Axes:\n",
      "    \"\"\"\n",
      "    ...\n",
      "    \"\"\"\n",
      "    ax = state_background.plot(color=\"#EEE\")\n",
      "    \n",
      "    urban = state_data[state_data['Urban'] == 1]\n",
      "    urban.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    rural = state_data[state_data['Rural'] == 1]\n",
      "    rural.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    # urban_low_acc = urban[\n",
      "    #     (urban['lapophalf'] >= 500) | \n",
      "    #     (urban['lapophalf'] >= 0.33 * urban['POP2010'])]\n",
      "    \n",
      "    # urban_low_acc.plot(ax=ax)\n",
      "    \n",
      "    # rural_low_acc = rural[\n",
      "    #     (rural['lapop10'] >= 500) |\n",
      "    #     (rural['lapop10'] >= 0.33 * rural['POP2010'])]\n",
      "    \n",
      "    # rural_low_acc.plot(ax=ax)\n",
      "    \n",
      "    ax.set_title(\"Low Access Census Tracts\")\n",
      "    ax.set_axis_off()\n",
      "    return ax\n",
      "\n",
      "ax = plot_census_low_access_map(state_data, entire_state)\n",
      "assert type(ax) == Axes, \"this function should return an Axes object\"\n",
      "layers = ax.findobj(PatchCollection)\n",
      "# assert len(layers) == 5, \"expected to have 5 layers (one background and 4 foregrounds)\"\\\n",
      "#                          +f\" but got {len(layers)} layer(s)\"\n",
      "assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "urban_idx = state_data[\"Urban\"].notna() & ~state_data.index.isin(rural_idx)\n",
      "urban_la_idx = urban_idx & ~state_data.index.isin(urban_ha_idx)\n",
      "try:\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[2], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=2)\n",
      "    # assert_patches_allclose(layers[3], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=3)\n",
      "    # assert_patches_allclose(layers[4], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=4)\n",
      "    ## Debugging tip: if you want to check one layer, comment out the four lines above\n",
      "    ## and use each of the following lines ONE AT A TIME\n",
      "    assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    assert_patches_allclose(layers[1], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=1)\n",
      "except AssertionError as e:\n",
      "    # this message was to remind you to plot in the correct order; you can ignore if you just kept one layer\n",
      "    print(\"Please make sure the order of your plot is correct, and also check any other errors below.\")\n",
      "    raise e\n",
      "assert ax.get_title() == \"Low Access Census Tracts\", \"title does not match expected\"\n",
      "assert not ax.axison, \"borders and labels must be hidden\"\n",
      "281/38:\n",
      "def plot_census_low_access_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame) -> Axes:\n",
      "    \"\"\"\n",
      "    ...\n",
      "    \"\"\"\n",
      "    ax = state_background.plot(color=\"#EEE\")\n",
      "    \n",
      "    # urban = state_data[state_data['Urban'] == 1]\n",
      "    # urban.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    rural = state_data[state_data['Rural'] == 1]\n",
      "    rural.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    # urban_low_acc = urban[\n",
      "    #     (urban['lapophalf'] >= 500) | \n",
      "    #     (urban['lapophalf'] >= 0.33 * urban['POP2010'])]\n",
      "    \n",
      "    # urban_low_acc.plot(ax=ax)\n",
      "    \n",
      "    # rural_low_acc = rural[\n",
      "    #     (rural['lapop10'] >= 500) |\n",
      "    #     (rural['lapop10'] >= 0.33 * rural['POP2010'])]\n",
      "    \n",
      "    # rural_low_acc.plot(ax=ax)\n",
      "    \n",
      "    ax.set_title(\"Low Access Census Tracts\")\n",
      "    ax.set_axis_off()\n",
      "    return ax\n",
      "\n",
      "ax = plot_census_low_access_map(state_data, entire_state)\n",
      "assert type(ax) == Axes, \"this function should return an Axes object\"\n",
      "layers = ax.findobj(PatchCollection)\n",
      "# assert len(layers) == 5, \"expected to have 5 layers (one background and 4 foregrounds)\"\\\n",
      "#                          +f\" but got {len(layers)} layer(s)\"\n",
      "assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "urban_idx = state_data[\"Urban\"].notna() & ~state_data.index.isin(rural_idx)\n",
      "urban_la_idx = urban_idx & ~state_data.index.isin(urban_ha_idx)\n",
      "try:\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[2], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=2)\n",
      "    # assert_patches_allclose(layers[3], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=3)\n",
      "    # assert_patches_allclose(layers[4], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=4)\n",
      "    ## Debugging tip: if you want to check one layer, comment out the four lines above\n",
      "    ## and use each of the following lines ONE AT A TIME\n",
      "    assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    assert_patches_allclose(layers[1], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=1)\n",
      "except AssertionError as e:\n",
      "    # this message was to remind you to plot in the correct order; you can ignore if you just kept one layer\n",
      "    print(\"Please make sure the order of your plot is correct, and also check any other errors below.\")\n",
      "    raise e\n",
      "assert ax.get_title() == \"Low Access Census Tracts\", \"title does not match expected\"\n",
      "assert not ax.axison, \"borders and labels must be hidden\"\n",
      "281/39:\n",
      "def plot_census_low_access_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame) -> Axes:\n",
      "    \"\"\"\n",
      "    ...\n",
      "    \"\"\"\n",
      "    ax = state_background.plot(color=\"#EEE\")\n",
      "    \n",
      "    # urban = state_data[state_data['Urban'] == 1]\n",
      "    # urban.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    rural = state_data[state_data['Rural'] == 1]\n",
      "    rural.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    # urban_low_acc = urban[\n",
      "    #     (urban['lapophalf'] >= 500) | \n",
      "    #     (urban['lapophalf'] >= 0.33 * urban['POP2010'])]\n",
      "    \n",
      "    # urban_low_acc.plot(ax=ax)\n",
      "    \n",
      "    # rural_low_acc = rural[\n",
      "    #     (rural['lapop10'] >= 500) |\n",
      "    #     (rural['lapop10'] >= 0.33 * rural['POP2010'])]\n",
      "    \n",
      "    # rural_low_acc.plot(ax=ax)\n",
      "    \n",
      "    ax.set_title(\"Low Access Census Tracts\")\n",
      "    ax.set_axis_off()\n",
      "    return ax\n",
      "\n",
      "ax = plot_census_low_access_map(state_data, entire_state)\n",
      "assert type(ax) == Axes, \"this function should return an Axes object\"\n",
      "layers = ax.findobj(PatchCollection)\n",
      "# assert len(layers) == 5, \"expected to have 5 layers (one background and 4 foregrounds)\"\\\n",
      "#                          +f\" but got {len(layers)} layer(s)\"\n",
      "assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "urban_idx = state_data[\"Urban\"].notna() & ~state_data.index.isin(rural_idx)\n",
      "urban_la_idx = urban_idx & ~state_data.index.isin(urban_ha_idx)\n",
      "try:\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[2], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=2)\n",
      "    # assert_patches_allclose(layers[3], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=3)\n",
      "    # assert_patches_allclose(layers[4], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=4)\n",
      "    ## Debugging tip: if you want to check one layer, comment out the four lines above\n",
      "    ## and use each of the following lines ONE AT A TIME\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    assert_patches_allclose(layers[1], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=1)\n",
      "except AssertionError as e:\n",
      "    # this message was to remind you to plot in the correct order; you can ignore if you just kept one layer\n",
      "    print(\"Please make sure the order of your plot is correct, and also check any other errors below.\")\n",
      "    raise e\n",
      "assert ax.get_title() == \"Low Access Census Tracts\", \"title does not match expected\"\n",
      "assert not ax.axison, \"borders and labels must be hidden\"\n",
      "281/40:\n",
      "def plot_census_low_access_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame) -> Axes:\n",
      "    \"\"\"\n",
      "    ...\n",
      "    \"\"\"\n",
      "    ax = state_background.plot(color=\"#EEE\")\n",
      "    \n",
      "    # urban = state_data[state_data['Urban'] == 1]\n",
      "    # urban.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    rural = state_data[state_data['Rural'] == 1]\n",
      "    rural.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    # urban_low_acc = urban[\n",
      "    #     (urban['lapophalf'] >= 500) | \n",
      "    #     (urban['lapophalf'] >= 0.33 * urban['POP2010'])]\n",
      "    \n",
      "    # urban_low_acc.plot(ax=ax)\n",
      "    \n",
      "    # rural_low_acc = rural[\n",
      "    #     (rural['lapop10'] >= 500) |\n",
      "    #     (rural['lapop10'] >= 0.33 * rural['POP2010'])]\n",
      "    \n",
      "    # rural_low_acc.plot(ax=ax)\n",
      "    \n",
      "    ax.set_title(\"Low Access Census Tracts\")\n",
      "    ax.set_axis_off()\n",
      "    return ax\n",
      "\n",
      "ax = plot_census_low_access_map(state_data, entire_state)\n",
      "assert type(ax) == Axes, \"this function should return an Axes object\"\n",
      "layers = ax.findobj(PatchCollection)\n",
      "assert len(layers) == 5, \"expected to have 5 layers (one background and 4 foregrounds)\"\\\n",
      "                         +f\" but got {len(layers)} layer(s)\"\n",
      "assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "urban_idx = state_data[\"Urban\"].notna() & ~state_data.index.isin(rural_idx)\n",
      "urban_la_idx = urban_idx & ~state_data.index.isin(urban_ha_idx)\n",
      "try:\n",
      "    assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    assert_patches_allclose(layers[2], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=2)\n",
      "    assert_patches_allclose(layers[3], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=3)\n",
      "    assert_patches_allclose(layers[4], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=4)\n",
      "    ## Debugging tip: if you want to check one layer, comment out the four lines above\n",
      "    ## and use each of the following lines ONE AT A TIME\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=1)\n",
      "except AssertionError as e:\n",
      "    # this message was to remind you to plot in the correct order; you can ignore if you just kept one layer\n",
      "    print(\"Please make sure the order of your plot is correct, and also check any other errors below.\")\n",
      "    raise e\n",
      "assert ax.get_title() == \"Low Access Census Tracts\", \"title does not match expected\"\n",
      "assert not ax.axison, \"borders and labels must be hidden\"\n",
      "281/41:\n",
      "def plot_census_low_access_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame) -> Axes:\n",
      "    \"\"\"\n",
      "    ...\n",
      "    \"\"\"\n",
      "    ax = state_background.plot(color=\"#EEE\")\n",
      "    \n",
      "    urban = state_data[state_data['Urban'] == 1]\n",
      "    urban.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    rural = state_data[state_data['Rural'] == 1]\n",
      "    rural.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    urban_low_acc = urban[\n",
      "        (urban['lapophalf'] >= 500) | \n",
      "        (urban['lapophalf'] >= 0.33 * urban['POP2010'])]\n",
      "    \n",
      "    urban_low_acc.plot(ax=ax)\n",
      "    \n",
      "    rural_low_acc = rural[\n",
      "        (rural['lapop10'] >= 500) |\n",
      "        (rural['lapop10'] >= 0.33 * rural['POP2010'])]\n",
      "    \n",
      "    rural_low_acc.plot(ax=ax)\n",
      "    \n",
      "    ax.set_title(\"Low Access Census Tracts\")\n",
      "    ax.set_axis_off()\n",
      "    return ax\n",
      "\n",
      "ax = plot_census_low_access_map(state_data, entire_state)\n",
      "assert type(ax) == Axes, \"this function should return an Axes object\"\n",
      "layers = ax.findobj(PatchCollection)\n",
      "assert len(layers) == 5, \"expected to have 5 layers (one background and 4 foregrounds)\"\\\n",
      "                         +f\" but got {len(layers)} layer(s)\"\n",
      "assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "urban_idx = state_data[\"Urban\"].notna() & ~state_data.index.isin(rural_idx)\n",
      "urban_la_idx = urban_idx & ~state_data.index.isin(urban_ha_idx)\n",
      "try:\n",
      "    assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    assert_patches_allclose(layers[2], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=2)\n",
      "    assert_patches_allclose(layers[3], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=3)\n",
      "    assert_patches_allclose(layers[4], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=4)\n",
      "    ## Debugging tip: if you want to check one layer, comment out the four lines above\n",
      "    ## and use each of the following lines ONE AT A TIME\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=1)\n",
      "except AssertionError as e:\n",
      "    # this message was to remind you to plot in the correct order; you can ignore if you just kept one layer\n",
      "    print(\"Please make sure the order of your plot is correct, and also check any other errors below.\")\n",
      "    raise e\n",
      "assert ax.get_title() == \"Low Access Census Tracts\", \"title does not match expected\"\n",
      "assert not ax.axison, \"borders and labels must be hidden\"\n",
      "281/42:\n",
      "def plot_census_low_access_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame) -> Axes:\n",
      "    \"\"\"\n",
      "    Returns a map visualization of census tracts in Washington state according to their qualification\n",
      "    as low access and dilineation as rural or urban. Inputted is two geo data frames; one with tract-level\n",
      "    food access census data and one with the state outline for background reference. The returned map is an\n",
      "    Axes object with layers (5) that togehter display regions lacking data (seen in grey), regions with data \n",
      "    (seen in white), and regions with data that are low access (seen in blue).\n",
      "    \"\"\"\n",
      "    ax = state_background.plot(color=\"#EEE\")\n",
      "    \n",
      "    urban = state_data[state_data['Urban'] == 1]\n",
      "    urban.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    rural = state_data[state_data['Rural'] == 1]\n",
      "    rural.plot(ax=ax, color=\"#AAA\")\n",
      "    \n",
      "    urban_low_acc = urban[\n",
      "        (urban['lapophalf'] >= 500) | \n",
      "        (urban['lapophalf'] >= 0.33 * urban['POP2010'])]\n",
      "    \n",
      "    urban_low_acc.plot(ax=ax)\n",
      "    \n",
      "    rural_low_acc = rural[\n",
      "        (rural['lapop10'] >= 500) |\n",
      "        (rural['lapop10'] >= 0.33 * rural['POP2010'])]\n",
      "    \n",
      "    rural_low_acc.plot(ax=ax)\n",
      "    \n",
      "    ax.set_title(\"Low Access Census Tracts\")\n",
      "    ax.set_axis_off()\n",
      "    return ax\n",
      "\n",
      "ax = plot_census_low_access_map(state_data, entire_state)\n",
      "assert type(ax) == Axes, \"this function should return an Axes object\"\n",
      "layers = ax.findobj(PatchCollection)\n",
      "assert len(layers) == 5, \"expected to have 5 layers (one background and 4 foregrounds)\"\\\n",
      "                         +f\" but got {len(layers)} layer(s)\"\n",
      "assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "urban_idx = state_data[\"Urban\"].notna() & ~state_data.index.isin(rural_idx)\n",
      "urban_la_idx = urban_idx & ~state_data.index.isin(urban_ha_idx)\n",
      "try:\n",
      "    assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    assert_patches_allclose(layers[2], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=2)\n",
      "    assert_patches_allclose(layers[3], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=3)\n",
      "    assert_patches_allclose(layers[4], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=4)\n",
      "    ## Debugging tip: if you want to check one layer, comment out the four lines above\n",
      "    ## and use each of the following lines ONE AT A TIME\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[rural_idx, \"geometry\"], color=\"#AAA\", layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[urban_la_idx, \"geometry\"], layer=1)\n",
      "    # assert_patches_allclose(layers[1], geoms=state_data.loc[rural_la_idx, \"geometry\"], layer=1)\n",
      "except AssertionError as e:\n",
      "    # this message was to remind you to plot in the correct order; you can ignore if you just kept one layer\n",
      "    print(\"Please make sure the order of your plot is correct, and also check any other errors below.\")\n",
      "    raise e\n",
      "assert ax.get_title() == \"Low Access Census Tracts\", \"title does not match expected\"\n",
      "assert not ax.axison, \"borders and labels must be hidden\"\n",
      "281/43:\n",
      "def interactive_map(state_data):\n",
      "    \"\"\"\n",
      "    ...\n",
      "    \"\"\"\n",
      "    low_inc = state_data[state_data['LowIncomeTracts'] == 1]\n",
      "    \n",
      "    low_inc_access = low_inc[\n",
      "        ((low_inc['Urban'] == 1) & (low_inc['LATracts_half'] == 1)) |\n",
      "        ((low_inc['Rural'] == 1) & (low_inc['LATracts10'] == 1))]\n",
      "    \n",
      "    access_map = low_inc_access.explore()\n",
      "    return access_map\n",
      "\n",
      "\n",
      "map = interactive_map(state_data)\n",
      "assert type(map) == Map, \"this function should return an interactive Map object\"\n",
      "display(map)\n",
      "last_child = next(reversed(map._children.values()))\n",
      "assert type(last_child) == GeoJson, \"last child should be GeoJson; do not specify column\"\n",
      "geojson = last_child.data\n",
      "assert set(int(d[\"id\"]) for d in geojson[\"features\"]) == set(lalowi_idx), \"wrong selection\"\n",
      "281/44: gpd.GeoDataFrame.from_features(geojson, crs=\"EPSG:4326\").plot(ax=entire_state.plot(color=\"#EEE\")).set_axis_off()\n",
      "281/45:\n",
      "def interactive_map(state_data: gpd.GeoDataFrame) -> Map:\n",
      "    \"\"\"\n",
      "    ...\n",
      "    \"\"\"\n",
      "    low_inc = state_data[state_data['LowIncomeTracts'] == 1]\n",
      "    \n",
      "    low_inc_access = low_inc[\n",
      "        ((low_inc['Urban'] == 1) & (low_inc['LATracts_half'] == 1)) |\n",
      "        ((low_inc['Rural'] == 1) & (low_inc['LATracts10'] == 1))]\n",
      "    \n",
      "    access_map = low_inc_access.explore()\n",
      "    return access_map\n",
      "\n",
      "\n",
      "map = interactive_map(state_data)\n",
      "assert type(map) == Map, \"this function should return an interactive Map object\"\n",
      "display(map)\n",
      "last_child = next(reversed(map._children.values()))\n",
      "assert type(last_child) == GeoJson, \"last child should be GeoJson; do not specify column\"\n",
      "geojson = last_child.data\n",
      "assert set(int(d[\"id\"]) for d in geojson[\"features\"]) == set(lalowi_idx), \"wrong selection\"\n",
      "281/46:\n",
      "def plot_food_access_by_county_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame):\n",
      "    \"\"\"\n",
      "    Given state population data and an outline of the state background in two geo data frames, returns\n",
      "    4 subplots showing the food access levels by county, visualizng food access by distance (half mile\n",
      "    and 10 miles), and income level (low-income vs. not low-income). \n",
      "    \"\"\"\n",
      "    fig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(2, 2, figsize=(15, 10))\n",
      "\n",
      "    county_data = state_data[['geometry', 'County', 'POP2010', \n",
      "                               'lapophalf', 'lalowihalf', \n",
      "                               'lapop10', 'lalowi10']].dissolve(by='County', aggfunc='sum')\n",
      "\n",
      "    subplot_data = [('lapophalf', 'lahalf_prop', \"Low Access: Half Mile\"),\n",
      "        ('lalowihalf', 'lalowihalf_prop', \"Low Access + Low Income: Half Mile\"),\n",
      "        ('lapop10', 'la10_prop', \"Low Access: 10 Miles\"),\n",
      "        ('lalowi10', 'lalowi10_prop', \"Low Access + Low Income: 10 Miles\")]\n",
      "    \n",
      "    for (numer_col, prop_col, title), ax in zip(subplot_data, [ax1, ax2, ax3, ax4]):\n",
      "        county_data[prop_col] = county_data[numer_col] / county_data['POP2010']\n",
      "\n",
      "        state_background.plot(ax=ax, color=\"#EEE\")\n",
      "        county_data.plot(ax=ax, column=prop_col, vmin=0, vmax=1)\n",
      "\n",
      "        ax.set_title(title)\n",
      "        ax.set_axis_off()\n",
      "\n",
      "    return(ax1, ax2, ax3, ax4)\n",
      "\n",
      "axs = plot_food_access_by_county_map(state_data, entire_state)\n",
      "assert axs is not None, \"this function should return a 4-tuple of Axes objects\"\n",
      "assert len(axs) == 4, \"the returned tuple's length should be 4\"\n",
      "expected_titles = [\"Low Access: Half Mile\", \"Low Access + Low Income: Half Mile\",\n",
      "                   \"Low Access: 10 Miles\", \"Low Access + Low Income: 10 Miles\"]\n",
      "for ax, expected_num_colors, expected_title in zip(axs, [31, 23, 19, 16], expected_titles):\n",
      "    assert type(ax) == Axes, \"each value in the returned tuple should be an Axes object\"\n",
      "    layers = ax.findobj(PatchCollection)\n",
      "    assert len(layers) == 2, \"expected to have 2 layers (one background and one foreground)\"\\\n",
      "                             +f\" but got {len(layers)} layer(s)\"\n",
      "    assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "    assert_patches_allclose(layers[1], geoms=all_counties.geometry, num_colors=expected_num_colors, layer=1)\n",
      "    assert ax.get_title() == expected_title, f\"title {ax.get_title()} does not match expected\"\n",
      "    assert ax.get_legend() is None, \"plot should not have a legend\"\n",
      "    assert not ax.axison, \"borders and labels must be hidden\"\n",
      "281/47:\n",
      "def plot_food_access_by_county_map(state_data: gpd.GeoDataFrame, state_background: gpd.GeoDataFrame):\n",
      "    \"\"\"\n",
      "    Given state population data and an outline of the state background in two geo data frames, returns\n",
      "    4 subplots showing the food access levels by county, visualizng food access by distance (half mile\n",
      "    and 10 miles), and income level (low-income vs. not low-income). \n",
      "    \"\"\"\n",
      "    fig, [[ax1, ax2], [ax3, ax4]] = plt.subplots(2, 2, figsize=(15, 10))\n",
      "\n",
      "    county_data = state_data[['geometry', 'County', 'POP2010', \n",
      "                               'lapophalf', 'lalowihalf', \n",
      "                               'lapop10', 'lalowi10']].dissolve(by='County', aggfunc='sum')\n",
      "\n",
      "    subplot_data = [('lapophalf', 'lahalf_prop', \"Low Access: Half Mile\"),\n",
      "        ('lalowihalf', 'lalowihalf_prop', \"Low Access + Low Income: Half Mile\"),\n",
      "        ('lapop10', 'la10_prop', \"Low Access: 10 Miles\"),\n",
      "        ('lalowi10', 'lalowi10_prop', \"Low Access + Low Income: 10 Miles\")]\n",
      "    \n",
      "    for (numer_col, prop_col, title), ax in zip(subplot_data, [ax1, ax2, ax3, ax4]):\n",
      "        county_data[prop_col] = county_data[numer_col] / county_data['POP2010']\n",
      "\n",
      "        state_background.plot(ax=ax, color=\"#EEE\")\n",
      "        county_data.plot(ax=ax, column=prop_col, vmin=0, vmax=1)\n",
      "\n",
      "        ax.set_title(title)\n",
      "        ax.set_axis_off()\n",
      "\n",
      "    return(ax1, ax2, ax3, ax4)\n",
      "\n",
      "axs = plot_food_access_by_county_map(state_data, entire_state)\n",
      "assert axs is not None, \"this function should return a 4-tuple of Axes objects\"\n",
      "assert len(axs) == 4, \"the returned tuple's length should be 4\"\n",
      "expected_titles = [\"Low Access: Half Mile\", \"Low Access + Low Income: Half Mile\",\n",
      "                   \"Low Access: 10 Miles\", \"Low Access + Low Income: 10 Miles\"]\n",
      "for ax, expected_num_colors, expected_title in zip(axs, [31, 23, 19, 16], expected_titles):\n",
      "    assert type(ax) == Axes, \"each value in the returned tuple should be an Axes object\"\n",
      "    layers = ax.findobj(PatchCollection)\n",
      "    assert len(layers) == 2, \"expected to have 2 layers (one background and one foreground)\"\\\n",
      "                             +f\" but got {len(layers)} layer(s)\"\n",
      "    assert_patches_allclose(layers[0], geoms=entire_state.geometry, color=\"#EEE\", layer=0)\n",
      "    assert_patches_allclose(layers[1], geoms=all_counties.geometry, num_colors=expected_num_colors, layer=1)\n",
      "    assert ax.get_title() == expected_title, f\"title {ax.get_title()} does not match expected\"\n",
      "    assert ax.get_legend() is None, \"plot should not have a legend\"\n",
      "    assert not ax.axison, \"borders and labels must be hidden\"\n",
      "281/48:\n",
      "def interactive_map(state_data: gpd.GeoDataFrame) -> Map:\n",
      "    \"\"\"\n",
      "    Creates an interactive map of low-income, low-access census tracts in \n",
      "    Washington, where census tracts are included if they are both designated\n",
      "    as low-income and designated as low-access using the appropriate distance\n",
      "    threshold (half mile for urban tracts or 10 miles for rural tracts). The \n",
      "    inputted state_data parameter should be a GeoDataFrame containing Washington \n",
      "    census tract geometries and food access data including columns for low-income\n",
      "    status, urban/rural designation, and low-access flags for both distance \n",
      "    thresholds. Returns a \"folium Map object\" that can be interacted with.\n",
      "    \"\"\"\n",
      "    low_inc = state_data[state_data['LowIncomeTracts'] == 1]\n",
      "    \n",
      "    low_inc_access = low_inc[\n",
      "        ((low_inc['Urban'] == 1) & (low_inc['LATracts_half'] == 1)) |\n",
      "        ((low_inc['Rural'] == 1) & (low_inc['LATracts10'] == 1))]\n",
      "    \n",
      "    access_map = low_inc_access.explore()\n",
      "    return access_map\n",
      "\n",
      "\n",
      "map = interactive_map(state_data)\n",
      "assert type(map) == Map, \"this function should return an interactive Map object\"\n",
      "display(map)\n",
      "last_child = next(reversed(map._children.values()))\n",
      "assert type(last_child) == GeoJson, \"last child should be GeoJson; do not specify column\"\n",
      "geojson = last_child.data\n",
      "assert set(int(d[\"id\"]) for d in geojson[\"features\"]) == set(lalowi_idx), \"wrong selection\"\n",
      "   1:\n",
      "import pandas as pd\n",
      "import csv\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import numpy as np\n",
      "import re\n",
      "import pickle\n",
      "import doctest\n",
      "from pathlib import Path\n",
      "from IPython.display import display\n",
      "from lmd_loader import load_masterdictionary # from custom python file\n",
      "from pandas import DataFrame # for type annotations\n",
      "   2:\n",
      "# Earning Reports - cannot limit pickle read\n",
      "def read_er_pkl(path: str, head_rows=None) -> DataFrame:\n",
      "    '''Load the earnings report pickle file into a df.'''\n",
      "    with open(path, 'rb') as file:\n",
      "        df = pickle.load(file)\n",
      "    return df.head(head_rows) if head_rows else df\n",
      "   3:\n",
      "# NASDAQ OHLCV + SPY Date Conversions\n",
      "def date_transform(df: DataFrame, src_col: str, out_col: str) -> DataFrame:\n",
      "    '''\n",
      "    Converts a column from string to datetime and stores it under a new name.\n",
      "    '''\n",
      "    df[out_col] = pd.to_datetime(out[src_col])\n",
      "\n",
      "    return out\n",
      "   4:\n",
      "# Earning Reports Date Conversion\n",
      "def er_date_transform(df: DataFrame, src_col: str ='date', out_col: str ='date_std') -> DataFrame:\n",
      "    '''\n",
      "    Given a dataset, returns a dataset with a cleaned and standardized date column. Receives \n",
      "    an input for the source date column, and the name for the outputted standardized date column.\n",
      "    '''\n",
      "    date_clean = df[src_col].str.strip()\n",
      "    date_clean = date_clean.str.replace(\".\",\"\")\n",
      "    date_clean = date_clean.str.replace(\"ET\",\"\")\n",
      "    \n",
      "    df[out_col] = pd.to_datetime(date_clean, format='mixed')\n",
      "\n",
      "    return df\n",
      "   5:\n",
      "spy_df = pd.read_csv('data/SPY.csv')\n",
      "spy_df = date_transform(spy_df, 'Date', 'date_std')\n",
      "   6:\n",
      "# NASDAQ OHLCV + SPY Date Conversions\n",
      "def date_transform(df: DataFrame, src_col: str, out_col: str) -> DataFrame:\n",
      "    '''\n",
      "    Converts a column from string to datetime and stores it under a new name.\n",
      "    '''\n",
      "    df[out_col] = pd.to_datetime(out[src_col])\n",
      "\n",
      "    return df\n",
      "   7:\n",
      "spy_df = pd.read_csv('data/SPY.csv')\n",
      "spy_df = date_transform(spy_df, 'Date', 'date_std')\n",
      "   8:\n",
      "# NASDAQ OHLCV + SPY Date Conversions\n",
      "def date_transform(df: DataFrame, src_col: str, out_col: str) -> DataFrame:\n",
      "    '''\n",
      "    Converts a column from string to datetime and stores it under a new name.\n",
      "    '''\n",
      "    df[out_col] = pd.to_datetime(df[src_col])\n",
      "\n",
      "    return df\n",
      "   9:\n",
      "spy_df = pd.read_csv('data/SPY.csv')\n",
      "spy_df = date_transform(spy_df, 'Date', 'date_std')\n",
      "  10:\n",
      "# Sentiment Dictionary\n",
      "lmd_path = 'data/Loughran-McDonald_MasterDictionary_1993-2024.csv'\n",
      "\n",
      "# load everything\n",
      "# vars identified in return stmnt at bottom of LMD loader, incl params for logging\n",
      "master_dict, md_header, sentiment_categories, sentiment_dicts, stopwords, total_docs = \\\n",
      "    load_masterdictionary(lmd_path, print_flag=True, f_log=None, get_other=True)\n",
      "  11:\n",
      "def lmd_features(text: str, sentiment_dicts: dict[str, dict[str, int]], \n",
      "                 cat_order: list[str] = sentiment_categories) -> dict[str, float]:\n",
      "    '''\n",
      "    Computes proportion of words in input text that belong to each LMD sentiment\n",
      "    category. For every category (pre-loaded by sentiment dict), it counts the \n",
      "    number of matching words and produces a proportion of the total for that word.\n",
      "    Designed to avoid creating large intermediate lists. \n",
      "    '''\n",
      "    if not isinstance(text, str): # checks for string to prevent errors\n",
      "        if text is None:\n",
      "            text = \"\"\n",
      "        else:\n",
      "            text = str(text)\n",
      "    \n",
      "    upper_text = text.upper()\n",
      "\n",
      "    # declares the counter and assigns base 0s\n",
      "    counts = {}\n",
      "    for category in cat_order:\n",
      "        counts[category] = 0\n",
      "\n",
      "    text_convert = re.compile(r\"[A-Za-z]+\")\n",
      "    total = 0\n",
      "\n",
      "    # using advanced RE module for memory efficiency **\n",
      "    iterator = text_convert.finditer(upper_text) # identifies matches in the string for iteration\n",
      "    \n",
      "    # match being a container that has text, start position, end position\n",
      "    for match in iterator:                        # \"for words in text\" = n words = O(n)\n",
      "        total += 1\n",
      "        tok = match.group(0) \n",
      "        \n",
      "        for category in cat_order:                # for category in list of categories = 9 cat\n",
      "            cat_dict = sentiment_dicts[category]\n",
      "            if tok in cat_dict:                   # 'if' search through a dict = O(1)\n",
      "                counts[category] += 1\n",
      "    \n",
      "    if total < 1:\n",
      "        denom = 1           # avoids division by 0 errors\n",
      "    else:\n",
      "        denom = total\n",
      "    \n",
      "    # computes the percentages for each category\n",
      "        # does not work for syllables (would need avg syllables)\n",
      "        # -> must ensure the syllable column is dropped\n",
      "    result = {}\n",
      "    for category in cat_order: \n",
      "        result[category] = counts[category] / denom\n",
      "    \n",
      "    return result\n",
      "  12:\n",
      "# feature tests\n",
      "# Test 1: Positive text\n",
      "result = lmd_features(\"able abundance\", sentiment_dicts, sentiment_categories)\n",
      "assert result['positive'] > 0, \"X Should detect positive words\"\n",
      "assert result['negative'] == 0, \"X Should have no negative words\"\n",
      "print(\"Y Test 1: Positive text\")\n",
      "\n",
      "# Test 2: Negative text\n",
      "result = lmd_features(\"loss decline\", sentiment_dicts, sentiment_categories)\n",
      "assert result['negative'] > 0, \"X Should detect negative words\"\n",
      "assert result['positive'] == 0, \"X Should have no positive words\"\n",
      "print(\"Y Test 2: Negative text\")\n",
      "\n",
      "# Test 3: Empty string\n",
      "result = lmd_features(\"\", sentiment_dicts, sentiment_categories)\n",
      "assert result['positive'] == 0, \"X Empty should be 0\"\n",
      "assert result['negative'] == 0, \"X Empty should be 0\"\n",
      "print(\"Y Test 3: Empty string\")\n",
      "\n",
      "# Test 4: None input\n",
      "result = lmd_features(None, sentiment_dicts, sentiment_categories)\n",
      "assert result['positive'] == 0, \"X None should be handled\"\n",
      "print(\"Y Test 4: None input\")\n",
      "  13:\n",
      "def apply_features(df, text_col=\"transcript\", drop_text=True):\n",
      "    \"\"\"\n",
      "    Add sentiment columns to dataframe. **finsish\n",
      "    \"\"\"\n",
      "    # initial gaurdrail\n",
      "    if text_col not in df.columns:\n",
      "        raise KeyError(f\"Column '{text_col}' not in DataFrame\")\n",
      "    \n",
      "    # applies sentiment function to each transcript (s = the 'current' transcript)\n",
      "    feat_dicts = df[text_col].apply(\n",
      "        lambda s: lmd_features(s, sentiment_dicts, sentiment_categories))\n",
      "    \n",
      "    # .apply converts dicts into columns/series -> then appended\n",
      "    feats = feat_dicts.apply(pd.Series)\n",
      "    \n",
      "    # applies transcript drops (per parameter) -- can make this one line in final\n",
      "    if drop_text:\n",
      "        base = df.drop(columns=[text_col])\n",
      "    else:\n",
      "        base = df\n",
      "        \n",
      "    # concat original and features\n",
      "    out = pd.concat([base, feats], axis=1)\n",
      "    \n",
      "    # testing\n",
      "    for cat in sentiment_categories:\n",
      "        assert cat in out.columns, \"Columns missing\"\n",
      "        print(\"Columns present\")\n",
      "        assert (out[cat].between(0, 1)).all(), \"Improper values\"\n",
      "        print(\"Values eligible\")\n",
      "    \n",
      "    return out\n",
      "  14:\n",
      "# Earnings Reports\n",
      "er_df = read_er_pkl(\"data/motley-fool-data.pkl\")\n",
      "er_df = er_date_transform(er_df, 'date', 'date_std')\n",
      "er_df = er_df.dropna(subset=['date_std'])\n",
      "# below: removes the time stamps -> not included in function for separate use case later\n",
      "# --> use times for adjusted window calcs in final\n",
      "# ** er_df['date_std'] = er_df['date_std'].dt.date\n",
      "# applies tokenizer below\n",
      "er_df = apply_features(er_df, 'transcript', True)\n",
      "  15: er_df.head()\n",
      "  16:\n",
      "def load_single_ohlcv(file: str) -> DataFrame:\n",
      "    \"\"\"\n",
      "    Load a single OHLCV file with ticker column, barring\n",
      "    ticker column errors, and returns the dataframe.\n",
      "    \"\"\"\n",
      "    df = pd.read_csv(file)\n",
      "    if 'ticker' not in df.columns:\n",
      "        df['ticker'] = file.stem.upper()\n",
      "    return df\n",
      "  17:\n",
      "def load_relevant_ohlcv(ohlcv_folder: str, er_df: DataFrame) -> DataFrame:\n",
      "    '''\n",
      "    Returns a df with cols [ticker, date_std, open, high, low, close],\n",
      "    after loading OHLCV data only for tickers that appear in the inputted\n",
      "    earnings report. Reduces storage significantly.\n",
      "    '''\n",
      "\n",
      "    # get unique tickers using set()\n",
      "    rel_tickers = set(er_df['ticker'].str.upper()) # standardized to string and upper for comparison\n",
      "    print(f\"Found {len(rel_tickers)} unique tickers in earnings reports\")\n",
      "\n",
      "    # uses path module/object with .glob to get all file paths, and place in list\n",
      "    files = list(Path(ohlcv_folder).glob('*.csv'))\n",
      "\n",
      "    # filters file paths to only retain 'relevant' tickers\n",
      "    rel_files = []\n",
      "    for file in files:\n",
      "        # pulls ticker names using the .stem()\n",
      "        ticker = file.stem.upper()\n",
      "        if ticker in rel_tickers:\n",
      "            rel_files.append(file)\n",
      "    \n",
      "    print(f\"{len(rel_files)} overlapping tickers in OHLCV\")\n",
      "\n",
      "    # full file loading using prev-built loading function\n",
      "    df_list = []\n",
      "    for file in rel_files:\n",
      "        df_list.append(load_single_ohlcv(file))\n",
      "\n",
      "    # takes the list of df, concats into one, and nromalizes dates\n",
      "    combined_df = pd.concat(df_list, ignore_index=True)\n",
      "    combined_df = date_transform(combined_df, 'date', 'date_std')\n",
      "\n",
      "    # filter columns for export\n",
      "    cols_to_keep = ['ticker', 'date_std', 'open', 'high', 'low', 'close']\n",
      "    combined_df = combined_df[cols_to_keep]\n",
      "\n",
      "    return combined_df\n",
      "  18:\n",
      "def load_relevant_ohlcv(ohlcv_folder: str, er_df: DataFrame) -> DataFrame:\n",
      "    '''\n",
      "    Returns a df with cols [ticker, date_std, open, high, low, close],\n",
      "    after loading OHLCV data only for tickers that appear in the inputted\n",
      "    earnings report (reducing storage significantly).\n",
      "    '''\n",
      "\n",
      "    # get unique tickers using set()\n",
      "    rel_tickers = set(er_df['ticker'].str.upper()) # standardized to string and upper for comparison\n",
      "    print(f\"Found {len(rel_tickers)} unique tickers in earnings reports\")\n",
      "\n",
      "    # uses path module/object with .glob to get all file paths, and place in list\n",
      "    files = list(Path(ohlcv_folder).glob('*.csv'))\n",
      "\n",
      "    # filters file paths to only retain 'relevant' tickers\n",
      "    rel_files = []\n",
      "    for file in files:\n",
      "        # pulls ticker names using the .stem()\n",
      "        ticker = file.stem.upper()\n",
      "        if ticker in rel_tickers:\n",
      "            rel_files.append(file)\n",
      "    \n",
      "    print(f\"{len(rel_files)} overlapping tickers in OHLCV\")\n",
      "\n",
      "    # full file loading using prev-built loading function\n",
      "    df_list = []\n",
      "    for file in rel_files:\n",
      "        df_list.append(load_single_ohlcv(file))\n",
      "\n",
      "    # takes the list of df, concats into one, and nromalizes dates\n",
      "    combined_df = pd.concat(df_list, ignore_index=True)\n",
      "    combined_df = date_transform(combined_df, 'date', 'date_std')\n",
      "\n",
      "    # filter columns for export\n",
      "    cols_to_keep = ['ticker', 'date_std', 'open', 'high', 'low', 'close']\n",
      "    combined_df = combined_df[cols_to_keep]\n",
      "\n",
      "    return combined_df\n",
      "  19:\n",
      "ohlcv_df = load_relevant_ohlcv('data/nasdaq_prices', er_df)\n",
      "ohlcv_df.head()\n",
      "  20: sort_by?\n",
      "  21: sort_values?\n",
      "  22: .sort_values?\n",
      "  23: n.sort_values?\n",
      "  24: er_df.sort_values?\n",
      "  25:\n",
      "def daily_returns_calc(df: DataFrame, price_col: str, date_col: str, group_col: str, return_col: str) -> DataFrame:\n",
      "    \"\"\"\n",
      "    Computes daily returns given an input df, price col, date col, and grouping column,\n",
      "    using a simple percent change formula. Returns a df that retains the calculated\n",
      "    returns, and that is sorted ensuring congruent calculations. \n",
      "    \"\"\"\n",
      "    # asserts for input verification (are they necessary?)\n",
      "    assert price_col in df.columns, f\"Column '{price_col}' not found\"\n",
      "    assert date_col in df.columns, f\"Column '{date_col}' not found\"\n",
      "    assert len(df) > 0, \"DataFrame cannot be empty\"\n",
      "    \n",
      "    new_df = df.copy()      # could avoid the copy for memory improvement, but use if possible\n",
      "\n",
      "    # data should be sorted, but pct change calc will be WRONG if it isnt, so we ensure\n",
      "    # group_col must be specified as None in the call if performing for a single stock -> \n",
      "        # risk too high if accidentally forgetten with multi ticker data\n",
      "    if group_col is None:\n",
      "        new_df = new_df.sort_values(date_col)\n",
      "        new_df[return_col] = new_df[price_col].pct_change()\n",
      "    else:\n",
      "        sort_cols = [group_col, date_col]\n",
      "        new_df = out.sort_values(sort_cols)\n",
      "        new_df[return_col] = new_df.groupby(group_col)[price_col].pct_change()\n",
      "\n",
      "    assert new_df[return_col].notna().sum() > 0, \"All returns are NaN | 0\"\n",
      "\n",
      "    return new_df\n",
      "  26:\n",
      "def event_window_car(merged_df: DataFrame, window_days: int = 2) -> DataFrame:\n",
      "    \"\"\"\n",
      "    Computes cumulative abnormal return (CAR) over [0, +1] event window.\n",
      "    Abnormal return = stock return - SPY return\n",
      "    \n",
      "    **NOTE** This is simplified - assumes next calendar day = next trading day;\n",
      "    full version will handle weekends/holidays properly.\n",
      "    \"\"\"\n",
      "    out = merged_df.copy()\n",
      "    \n",
      "    # For each earnings call, gets returns on day 0 and day +1\n",
      "    # Simplified v1: just gets  abnormal return on the call date\n",
      "    out['abnormal_return'] = out['return'] - out['spy_return']\n",
      "    \n",
      "    # **EDA using single-day abnormal return as proxy for CAR**\n",
      "    out['car_0p1'] = out['abnormal_return']  # Placeholder\n",
      "    \n",
      "    return out\n",
      "  27:\n",
      "ohlcv_df = daily_returns_calc(ohlcv_df, 'close', 'date_std', 'ticker', 'return')\n",
      "spy_returns = daily_returns_calc(spy_df, 'Close', 'date_std', group_col = None, return_col='spy_return')\n",
      "  28:\n",
      "def daily_returns_calc(df: DataFrame, price_col: str, date_col: str, group_col: str, return_col: str) -> DataFrame:\n",
      "    \"\"\"\n",
      "    Computes daily returns given an input df, price col, date col, and grouping column,\n",
      "    using a simple percent change formula. Returns a df that retains the calculated\n",
      "    returns, and that is sorted ensuring congruent calculations. \n",
      "    \"\"\"\n",
      "    # asserts for input verification (are they necessary?)\n",
      "    assert price_col in df.columns, f\"Column '{price_col}' not found\"\n",
      "    assert date_col in df.columns, f\"Column '{date_col}' not found\"\n",
      "    assert len(df) > 0, \"DataFrame cannot be empty\"\n",
      "    \n",
      "    new_df = df.copy()      # could avoid the copy for memory improvement, but use if possible\n",
      "\n",
      "    # data should be sorted, but pct change calc will be WRONG if it isnt, so we ensure\n",
      "    # group_col must be specified as None in the call if performing for a single stock -> \n",
      "        # risk too high if accidentally forgetten with multi ticker data\n",
      "    if group_col is None:\n",
      "        new_df = new_df.sort_values(date_col)\n",
      "        new_df[return_col] = new_df[price_col].pct_change()\n",
      "    else:\n",
      "        sort_cols = [group_col, date_col]\n",
      "        new_df = new_df.sort_values(sort_cols)\n",
      "        new_df[return_col] = new_df.groupby(group_col)[price_col].pct_change()\n",
      "\n",
      "    assert new_df[return_col].notna().sum() > 0, \"All returns are NaN | 0\"\n",
      "\n",
      "    return new_df\n",
      "  29:\n",
      "ohlcv_df = daily_returns_calc(ohlcv_df, 'close', 'date_std', 'ticker', 'return')\n",
      "spy_returns = daily_returns_calc(spy_df, 'Close', 'date_std', group_col = None, return_col='spy_return')\n",
      "  30: ohlcv_df.head()\n",
      "  31:\n",
      "ohlcv_df = daily_returns_calc(ohlcv_df, 'close', 'date_std', 'ticker', 'return_fract')\n",
      "spy_returns = daily_returns_calc(spy_df, 'Close', 'date_std', group_col = None, return_col='spy_return_fract')\n",
      "  32: spy_returns.head()\n",
      "  33:\n",
      "ohlcv_returns = daily_returns_calc(ohlcv_df, 'close', 'date_std', 'ticker', 'return_fract')\n",
      "spy_returns = daily_returns_calc(spy_df, 'Close', 'date_std', group_col = None, return_col='spy_return_fract')\n",
      "  34:\n",
      "ohlcv_df = load_relevant_ohlcv('data/nasdaq_prices', er_df)\n",
      "ohlcv_df.head()\n",
      "  35: ohlcv_df['date_std'].dtype\n",
      "  36:\n",
      "# NASDAQ OHLCV + SPY Date Conversions\n",
      "def date_transform(df: DataFrame, src_col: str, out_col: str) -> DataFrame:\n",
      "    '''\n",
      "    Converts a column from string to datetime and stores it under a new name.\n",
      "    '''\n",
      "    df[out_col] = pd.to_datetime(df[src_col])\n",
      "    df[out_col] = df[out_col].dt.normalize()\n",
      "\n",
      "    return df\n",
      "  37:\n",
      "def load_single_ohlcv(file: str) -> DataFrame:\n",
      "    \"\"\"\n",
      "    Load a single OHLCV file with ticker column, barring\n",
      "    ticker column errors, and returns the dataframe.\n",
      "    \"\"\"\n",
      "    df = pd.read_csv(file)\n",
      "    if 'ticker' not in df.columns:\n",
      "        df['ticker'] = file.stem.upper()\n",
      "    \n",
      "    return df\n",
      "  38:\n",
      "def load_relevant_ohlcv(ohlcv_folder: str, er_df: DataFrame) -> DataFrame:\n",
      "    '''\n",
      "    Returns a df with cols [ticker, date_std, open, high, low, close],\n",
      "    after loading OHLCV data only for tickers that appear in the inputted\n",
      "    earnings report (reducing storage significantly).\n",
      "    '''\n",
      "\n",
      "    # get unique tickers using set()\n",
      "    rel_tickers = set(er_df['ticker'].str.upper()) # standardized to string and upper for comparison\n",
      "    print(f\"Found {len(rel_tickers)} unique tickers in earnings reports\")\n",
      "\n",
      "    # uses path module/object with .glob to get all file paths, and place in list\n",
      "    files = list(Path(ohlcv_folder).glob('*.csv'))\n",
      "\n",
      "    # filters file paths to only retain 'relevant' tickers\n",
      "    rel_files = []\n",
      "    for file in files:\n",
      "        # pulls ticker names using the .stem()\n",
      "        ticker = file.stem.upper()\n",
      "        if ticker in rel_tickers:\n",
      "            rel_files.append(file)\n",
      "    \n",
      "    print(f\"{len(rel_files)} overlapping tickers in OHLCV\")\n",
      "\n",
      "    # full file loading using prev-built loading function\n",
      "    df_list = []\n",
      "    for file in rel_files:\n",
      "        df_list.append(load_single_ohlcv(file))\n",
      "\n",
      "    # takes the list of df, concats into one, and normalizes dates\n",
      "    combined_df = pd.concat(df_list, ignore_index=True)\n",
      "    combined_df = date_transform(combined_df, 'date', 'date_std')\n",
      "    \n",
      "    # filter columns for export\n",
      "    cols_to_keep = ['ticker', 'date_std', 'open', 'high', 'low', 'close']\n",
      "    combined_df = combined_df[cols_to_keep]\n",
      "\n",
      "    return combined_df\n",
      "  39:\n",
      "ohlcv_df = load_relevant_ohlcv('data/nasdaq_prices', er_df)\n",
      "ohlcv_df.head()\n",
      "  40: ohlcv_df['date_std'].dtype\n",
      "  41: ohlcv_df['date_std'].dtype?\n",
      "  42: ohlcv_df['date_std'].dtype()\n",
      "  43: ohlcv_df['date_std'].dtype\n",
      "  44:\n",
      "def load_relevant_ohlcv(ohlcv_folder: str, er_df: DataFrame) -> DataFrame:\n",
      "    '''\n",
      "    Returns a df with cols [ticker, date_std, open, high, low, close],\n",
      "    after loading OHLCV data only for tickers that appear in the inputted\n",
      "    earnings report (reducing storage significantly).\n",
      "    '''\n",
      "\n",
      "    # get unique tickers using set()\n",
      "    rel_tickers = set(er_df['ticker'].str.upper()) # standardized to string and upper for comparison\n",
      "    print(f\"Found {len(rel_tickers)} unique tickers in earnings reports\")\n",
      "\n",
      "    # uses path module/object with .glob to get all file paths, and place in list\n",
      "    files = list(Path(ohlcv_folder).glob('*.csv'))\n",
      "\n",
      "    # filters file paths to only retain 'relevant' tickers\n",
      "    rel_files = []\n",
      "    for file in files:\n",
      "        # pulls ticker names using the .stem()\n",
      "        ticker = file.stem.upper()\n",
      "        if ticker in rel_tickers:\n",
      "            rel_files.append(file)\n",
      "    \n",
      "    print(f\"{len(rel_files)} overlapping tickers in OHLCV\")\n",
      "\n",
      "    # full file loading using prev-built loading function\n",
      "    df_list = []\n",
      "    for file in rel_files:\n",
      "        df_list.append(load_single_ohlcv(file))\n",
      "\n",
      "    # takes the list of df, concats into one, and normalizes dates\n",
      "    combined_df = pd.concat(df_list, ignore_index=True)\n",
      "    combined_df = date_transform(combined_df, 'date', 'date_std')\n",
      "\n",
      "    combined_df['date_std'] = combined_df['date_std'].dt.normalize()\n",
      "    \n",
      "    # filter columns for export\n",
      "    cols_to_keep = ['ticker', 'date_std', 'open', 'high', 'low', 'close']\n",
      "    combined_df = combined_df[cols_to_keep]\n",
      "\n",
      "    return combined_df\n",
      "  45:\n",
      "ohlcv_df = load_relevant_ohlcv('data/nasdaq_prices', er_df)\n",
      "ohlcv_df.head()\n",
      "  46: ohlcv_df['date_std'].dtype\n",
      "  47:\n",
      "def load_relevant_ohlcv(ohlcv_folder: str, er_df: DataFrame) -> DataFrame:\n",
      "    '''\n",
      "    Returns a df with cols [ticker, date_std, open, high, low, close],\n",
      "    after loading OHLCV data only for tickers that appear in the inputted\n",
      "    earnings report (reducing storage significantly).\n",
      "    '''\n",
      "\n",
      "    # get unique tickers using set()\n",
      "    rel_tickers = set(er_df['ticker'].str.upper()) # standardized to string and upper for comparison\n",
      "    print(f\"Found {len(rel_tickers)} unique tickers in earnings reports\")\n",
      "\n",
      "    # uses path module/object with .glob to get all file paths, and place in list\n",
      "    files = list(Path(ohlcv_folder).glob('*.csv'))\n",
      "\n",
      "    # filters file paths to only retain 'relevant' tickers\n",
      "    rel_files = []\n",
      "    for file in files:\n",
      "        # pulls ticker names using the .stem()\n",
      "        ticker = file.stem.upper()\n",
      "        if ticker in rel_tickers:\n",
      "            rel_files.append(file)\n",
      "    \n",
      "    print(f\"{len(rel_files)} overlapping tickers in OHLCV\")\n",
      "\n",
      "    # full file loading using prev-built loading function\n",
      "    df_list = []\n",
      "    for file in rel_files:\n",
      "        df_list.append(load_single_ohlcv(file))\n",
      "\n",
      "    # takes the list of df, concats into one, and normalizes dates\n",
      "    combined_df = pd.concat(df_list, ignore_index=True)\n",
      "    combined_df = date_transform(combined_df, 'date', 'date_std')\n",
      "    \n",
      "    # filter columns for export\n",
      "    cols_to_keep = ['ticker', 'date_std', 'open', 'high', 'low', 'close']\n",
      "    combined_df = combined_df[cols_to_keep]\n",
      "\n",
      "    return combined_df\n",
      "  48:\n",
      "# NASDAQ OHLCV + SPY Date Conversions\n",
      "def date_transform(df: DataFrame, src_col: str, out_col: str) -> DataFrame:\n",
      "    '''\n",
      "    Converts a column from string to datetime and stores it under a new name.\n",
      "    '''\n",
      "    df[out_col] = pd.to_datetime(df[src_col])\n",
      "    df[out_col] = df[out_col].dt.normalize()   # precautionary check\n",
      "\n",
      "    return df\n",
      "  49: ohlcv_df['date_std'].dtype\n",
      "  50:\n",
      "# uses inner join to merge pricing and report data (post ticker filter)\n",
      "merged = er_df.merge(ohlcv_df, on=['ticker', 'date_std'], how='inner')\n",
      "merged\n",
      "  51: er_df['date_std'].dtype\n",
      "  52: ohlcv_df['date_std'].dtype\n",
      "  53: ohlcv_df['ticker'].head()\n",
      "  54: er_df['ticker'].head()\n",
      "  55: print(sorted(er_df['ticker'].unique())[:10])\n",
      "  56:\n",
      "# uses inner join to merge pricing and report data (post ticker filter)\n",
      "merged = er_df.merge(ohlcv_returns, on=['ticker', 'date_std'], how='inner')\n",
      "merged\n",
      "  57: ohlcv_returns['date_std'].dtype\n",
      "  58: ohlcv_returns['ticker'].head()\n",
      "  59: print(sorted(ohlcv_returns['ticker'].unique())[:10])\n",
      "  60:\n",
      "print(\"Sample from er_df:\")\n",
      "print(er_df[['ticker', 'date_std']].head())\n",
      "print(f\"\\ner_df dtypes:\\n{er_df[['ticker', 'date_std']].dtypes}\")\n",
      "\n",
      "print(\"\\n\" + \"=\"*50)\n",
      "print(\"Sample from ohlcv_df:\")\n",
      "print(ohlcv_df[['ticker', 'date_std']].head())\n",
      "print(f\"\\nohlcv_df dtypes:\\n{ohlcv_df[['ticker', 'date_std']].dtypes}\")\n",
      "\n",
      "print(\"\\n\" + \"=\"*50)\n",
      "test_ticker = er_df['ticker'].iloc[0]\n",
      "test_date = er_df['date_std'].iloc[0]\n",
      "print(f\"Looking for: ticker='{test_ticker}', date='{test_date}'\")\n",
      "\n",
      "match = ohlcv_df[(ohlcv_df['ticker'] == test_ticker) & \n",
      "                  (ohlcv_df['date_std'] == test_date)]\n",
      "print(f\"Found {len(match)} matches in ohlcv_df\")\n",
      "\n",
      "if len(match) == 0:\n",
      "    ticker_exists = ohlcv_df[ohlcv_df['ticker'] == test_ticker]\n",
      "    print(f\"\\nTicker '{test_ticker}' has {len(ticker_exists)} rows in ohlcv_df\")\n",
      "    if len(ticker_exists) > 0:\n",
      "        print(f\"Available dates for {test_ticker}: {ticker_exists['date_std'].head().tolist()}\")\n",
      "  61:\n",
      "# Earning Reports Date Conversion\n",
      "def er_date_transform(df: DataFrame, src_col: str ='date', out_col: str ='date_std') -> DataFrame:\n",
      "    '''\n",
      "    Given a dataset, returns a dataset with a cleaned and standardized date column. Receives \n",
      "    an input for the source date column, and the name for the outputted standardized date column.\n",
      "    '''\n",
      "    date_clean = df[src_col].str.strip()\n",
      "    date_clean = date_clean.str.replace(\".\",\"\")\n",
      "    date_clean = date_clean.str.replace(\"ET\",\"\")\n",
      "    \n",
      "    df[out_col] = pd.to_datetime(date_clean, format='mixed')\n",
      "\n",
      "    # boolean mask for past 5:00 pm identification -> works because indexes line up etc\n",
      "    after_close = df[out_col].dt.hour >= 17\n",
      "    row_updates = df.loc[after_close, out_col]            # stores all \n",
      "    updated_dates = row_updates + pd.Timedelta(days=1)    # uses pandas timedelta function to add 1 day to the date\n",
      "    df.loc[after_close, 'date_std'] = updated_date        # uses .loc for conditional selection base on True | False values in after_close\n",
      "    df[out_col] = df[out_col].dt.normalize()\n",
      "\n",
      "    return df\n",
      "  62:\n",
      "# Earning Reports Date Conversion\n",
      "def er_date_transform(df: DataFrame, src_col: str ='date', out_col: str ='date_std') -> DataFrame:\n",
      "    '''\n",
      "    Given a dataset, returns a dataset with a cleaned and standardized date column. Receives \n",
      "    an input for the source date column, and the name for the outputted standardized date column.\n",
      "    '''\n",
      "    date_clean = df[src_col].str.strip()\n",
      "    date_clean = date_clean.str.replace(\".\",\"\")\n",
      "    date_clean = date_clean.str.replace(\"ET\",\"\")\n",
      "    \n",
      "    df[out_col] = pd.to_datetime(date_clean, format='mixed')\n",
      "\n",
      "    # boolean mask for past 5:00 pm identification -> works because indexes line up etc\n",
      "    after_close = df[out_col].dt.hour >= 17\n",
      "    row_updates = df.loc[after_close, out_col]            # stores all \n",
      "    updated_dates = row_updates + pd.Timedelta(days=1)    # uses pandas timedelta function to add 1 day to the date\n",
      "    df.loc[after_close, 'date_std'] = updated_date        # uses .loc for conditional selection base on True | False values in after_close\n",
      "    df[out_col] = df[out_col].dt.normalize()              # strip times for use with \n",
      "\n",
      "    return df\n",
      "  63: er_df = er_date_transform(er_df, 'date', 'date_std')\n",
      "  64:\n",
      "# Earning Reports Date Conversion\n",
      "def er_date_transform(df: DataFrame, src_col: str ='date', out_col: str ='date_std') -> DataFrame:\n",
      "    '''\n",
      "    Given a dataset, returns a dataset with a cleaned and standardized date column. Receives \n",
      "    an input for the source date column, and the name for the outputted standardized date column.\n",
      "    '''\n",
      "    date_clean = df[src_col].str.strip()\n",
      "    date_clean = date_clean.str.replace(\".\",\"\")\n",
      "    date_clean = date_clean.str.replace(\"ET\",\"\")\n",
      "    \n",
      "    df[out_col] = pd.to_datetime(date_clean, format='mixed')\n",
      "\n",
      "    # boolean mask for past 5:00 pm identification -> works because indexes line up etc\n",
      "    after_close = df[out_col].dt.hour >= 17\n",
      "    row_updates = df.loc[after_close, out_col]            # stores all \n",
      "    updated_dates = row_updates + pd.Timedelta(days=1)    # uses pandas timedelta function to add 1 day to the date\n",
      "    df.loc[after_close, 'date_std'] = updated_dates       # uses .loc for conditional selection base on True | False values in after_close\n",
      "    df[out_col] = df[out_col].dt.normalize()              # strip times for use with \n",
      "\n",
      "    return df\n",
      "  65: er_df = er_date_transform(er_df, 'date', 'date_std')\n",
      "  66: er_df.head()\n",
      "  67:\n",
      "# uses inner join to merge pricing and report data (post ticker filter)\n",
      "merged = er_df.merge(ohlcv_returns, on=['ticker', 'date_std'], how='inner')\n",
      "merged\n",
      "  68:\n",
      "# data size summary\n",
      "print(f\"ER (earnings reports) shape: {merged.shape[0]} rows × {merged.shape[1]} cols\")\n",
      "print(f\"SPY shape: {spy_df.shape[0]} rows × {spy_df.shape[1]} cols\")\n",
      "\n",
      "print(\"ER preview:\")\n",
      "display(er_df.head(3))\n",
      "print(\"OHLCV preview:\")\n",
      "display(ohlcv_df.head(3))\n",
      "print(\"SPY preview:\")\n",
      "display(spy_df.head(3))\n",
      "  69:\n",
      "# data size summary\n",
      "print(f\"Merged stock data shape: {merged.shape[0]} rows × {merged.shape[1]} cols\")\n",
      "print(f\"SPY shape: {spy_df.shape[0]} rows × {spy_df.shape[1]} cols\")\n",
      "\n",
      "print(\"ER preview:\")\n",
      "display(er_df.head(3))\n",
      "print(\"OHLCV preview:\")\n",
      "display(ohlcv_df.head(3))\n",
      "print(\"SPY preview:\")\n",
      "display(spy_df.head(3))\n",
      "  70:\n",
      "# data size summary\n",
      "print(f\"Merged stock data shape: {merged.shape[0]} rows × {merged.shape[1]} cols\")\n",
      "print(f\"SPY shape: {spy_df.shape[0]} rows × {spy_df.shape[1]} cols\")\n",
      "\n",
      "print(\"Merged data preview:\")\n",
      "display(merged.head(3))\n",
      "print(\"SPY preview:\")\n",
      "display(spy_df.head(3))\n",
      "  71:\n",
      "# uses inner join to merge pricing and report data (post ticker filter)\n",
      "merged = er_df.merge(ohlcv_df, on=['ticker', 'date_std'], how='inner')\n",
      "merged\n",
      "  72:\n",
      "# data size summary\n",
      "print(f\"Merged stock data shape: {merged.shape[0]} rows × {merged.shape[1]} cols\")\n",
      "print(f\"SPY shape: {spy_df.shape[0]} rows × {spy_df.shape[1]} cols\")\n",
      "\n",
      "print(\"Merged data preview:\")\n",
      "display(merged.head(3))\n",
      "print(\"SPY preview:\")\n",
      "display(spy_df.head(3))\n",
      "  73:\n",
      "ohlcv_returns = daily_returns_calc(ohlcv_df, 'close', 'date_std', 'ticker', 'return')\n",
      "spy_returns = daily_returns_calc(spy_df, 'Close', 'date_std', return_col='spy_return')\n",
      "  74:    %history -n -l 50\n"
     ]
    }
   ],
   "source": [
    "   %history -n -l 100\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
