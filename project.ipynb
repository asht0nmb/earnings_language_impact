{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab138c5-86cd-40e9-a056-c2e3a1b2cfb7",
   "metadata": {},
   "source": [
    "# Earnings Call Tone Impact on Short-Term Stock Returns\n",
    "#### By: Ashton Meyer-Bibbins\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24018313-ad8a-4c47-a484-1e0e0e6e2f52",
   "metadata": {},
   "source": [
    "## Research Questions\n",
    "\n",
    "1) Does the tone (positive, negative, uncertain) expressed during an earnings call predict short-window abnormal stock returns, defined as the firm’s actual return over the [0, +1]-day event window surrounding the call in excess of the market return (proxied by the S&P 500 ETF, SPY)?\n",
    "2) Does the tone to return relationship differ across industries, firm sizes, or leadership?\n",
    "3) Do tone effects weaken or strengthen during high-volatility market days, as measured against the SPY (if there are large increases/decreases in SPY price, are the impacts of tone amplified or dampened)?\n",
    "4) *Potential question*: After controlling for EPS surprise (the difference between actual returns and the forecasted returns by external analysts, which, when positive or negative, can have a significant impact on a company's stock performance), does tone still explain residual abnormal returns (measured with a [-1,+1] event window)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98247ef-f125-4224-ac36-0f6e6a1125eb",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "Corporate earnings calls serve as the main bridge between enterprises and investors. They shape how markets interpret financial performance beyond the raw numbers, providing context to numeric output. While the quantitative outcomes of an earnings report are easy to measure, the language executives use – be it their tone, confidence, or underlying uncertainty – can carry additional weight, which has the potential to influence investor sentiment when localized to each occurrence.\n",
    "\n",
    "It is important to study this relationship because, while markets are a quantitative beast, they also rely on narrative, context, and behavioral signals. Prior work has shown that tone effects on “abnormal performance” can be predicted in gradual post-announcement stock price drift. As a contrast, this project isolates the short-window reaction ([0,+1]) to measure the immediate market response to tone, providing a complementary perspective that is clear of other market influences, which conflate analyses. Understanding this aspect of the psychology behind financial decision-making provides an interesting lens into the impacts of behavior on markets, informing future analysis and serving as an input for future models.\n",
    "\n",
    "Beyond its economic ties, this project provides an interesting computational exploration, combining natural language analysis and statistical models, which is becoming an ever-larger part of financial and economic research. By linking these natural language signals to numerical outcomes, it deepens (my) understanding of how unstructured information can be linked to statistical analysis, and how this information translates to measurable impacts.\n",
    "\n",
    "Experience with finance and markets, which I have gained over the last 3 years, along with my interest in data science, serve as the foundation for my desire to pursue this project. It is particularly interesting in its combination of NLP and statistical analysis, and I look forward to seeing the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd470f7-1694-4d59-b288-eb0a7882fd5a",
   "metadata": {},
   "source": [
    "## Data Setting\n",
    "This project draws on three publicly available datasets that together support analysis of how executive tone in earnings calls relates to short-window abnormal stock returns.\n",
    "1. **[Earnings Call Transcripts (Motley Fool / Kaggle)](https://www.kaggle.com/datasets/tpotterer/motley-fool-scraped-earnings-call-transcripts)** - This dataset includes roughly 18,000 quarterly earnings-call transcripts for U.S.-listed companies. Each record provides the company ticker, call date, exchange, quarter, and full transcript text. The data were scraped from The Motley Fool’s public archives and compiled by Kaggle contributors. The transcripts are the unstructured textual foundation for tone analysis, allowing extraction of sentiment features using finance-specific linguistic dictionaries (see #4).\n",
    "2. **[NASDAQ Daily Prices (Kaggle / Paul Mooney)](https://www.kaggle.com/datasets/svaningelgem/nasdaq-daily-stock-prices)** - This dataset contains daily open, high, low, close, adjusted-close, and volume (OHLCV) data for U.S. equities from roughly 2015-2024. It enables the computation of firm-level daily returns and the construction of event-window returns surrounding each earnings call date.\n",
    "3. **[S&P 500 ETF (SPY) Prices (Kaggle)](https://www.kaggle.com/datasets/benjaminbtang/spy-historical-prices)*** - This dataset provides historical daily prices for the SPY ETF, which is used as a market benchmark. Subtracting SPY’s daily return from a firm’s daily return produces a simple measure of abnormal return, controlling for broad market movements.\n",
    "4. *Supplemental dataset/tool* **|** ***[Loughran-McDonald Financial Sentiment Dictionary](https://sraf.nd.edu/loughranmcdonald-master-dictionary/)*** - Used map word occurrences in transcripts to finance-specific tone categories (positive, negative, uncertainty, etc.). This resource, widely adopted in accounting and finance research, ensures that the tone scores reflect financial meaning rather than generic sentiment.\n",
    "\n",
    "Each dataset is stored in CSV format and will be merged on ticker and date keys to align firm-level and market-level data for each event window.\n",
    "\n",
    "#### Potential Challenges\n",
    "None of the datasets include formal datasheets; however, several contextual details may complicate or encourage deeper analysis:\n",
    "1. **Coverage and survivorship bias** - The transcript dataset includes only companies covered by The Motley Fool, potentially omitting small-cap or delisted firms. This may over-represent large, stable firms and bias results toward those with stronger disclosure practices.\n",
    "2. **Timing misalignment** - Earnings calls often take place after market hours, while price data are recorded at the market close. As a result, a “day 0” return may reflect information or expectations formed before the call rather than the call itself, making it important to define the event window ([0,+1]) carefully and account for weekends and holidays.\n",
    "3. **Linguistic and formatting variation** - Transcripts differ in speaker labeling, punctuation, and inclusion of boilerplate disclaimers or operator remarks. These inconsistencies may distort tone-scoring unless the text is systematically cleaned.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd8332-27e9-4c25-9b0f-1c7e4b4f816a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Method\n",
    "Step 1: Load and prepare data\n",
    "- Load the three datasets (earnings call transcripts, stock prices, and SPY benchmark) using pandas\n",
    "- Standardize date formats and align all data by ticker and date.\n",
    "- Functions (data-manipulation): load_data(), standardize_dates()\n",
    "- Tests: Use small 3–5 row samples to confirm correct data types and successful merges.\n",
    "- Output: Three clean DataFrames with properly formatted and aligned dates.\n",
    "- Connection: Establishes base for the Multiple Datasets challenge goal by merging separate data sources.\n",
    "\n",
    "\n",
    "Step 2: Clean transcripts and compute tone features\n",
    "- Use Python’s built-in re library to remove punctuation, lowercase text, and normalize spacing.\n",
    "- Count occurrences of positive, negative, and uncertainty words using the Loughran–McDonald financial dictionary.\n",
    "- Calculate each tone category as a percentage of total words in the transcript.\n",
    "- Functions (data-manipulation): clean_text(), compute_tone_scores()\n",
    "- Tests: Verify results on short sample texts (“profits increased,” “uncertain outlook”) with known word counts.\n",
    "- Output: Dataset with tone metrics (pos_pct, neg_pct, uncert_pct) for each earnings call.\n",
    "- Connection: Creates the independent variables used in hypothesis testing (RQ1 and RQ2).\n",
    "\n",
    "\n",
    "Step 3: Compute event-window and abnormal returns\n",
    "- Compute daily returns for each stock and for SPY using adjusted close prices.\n",
    "-Define Day 0 as the first trading day on or after the call date and Day +1 as the following trading day.\n",
    "- Calculate abnormal returns as firm return minus SPY return, then sum over [0,+1] to get cumulative abnormal return (CAR).\n",
    "- Functions (data-manipulation): compute_returns(), compute_abnormal_returns()\n",
    "- Tests: Hand-check results on a small, synthetic dataset to confirm correct math and event-window handling.\n",
    "- Output: Event-level dataset linking each call to its short-window abnormal return.\n",
    "- Connection: Provides the dependent variable for statistical testing and supports Multiple Datasets.\n",
    "\n",
    "Step 4: Merge tone and return data\n",
    "- Merge tone metrics with event returns and add basic controls such as sector and firm-size proxies.\n",
    "- Functions (data-manipulation): merge_features(), add_controls()\n",
    "- Tests: Ensure one row per event after merging and confirm correct ticker/date alignment.\n",
    "- Output: Combined dataset ready for modeling.\n",
    "- Connection: Prepares data for hypothesis testing (RQ1 and RQ2).\n",
    "\n",
    "Step 5: Hypothesis testing and modeling\n",
    "- Run regression models using statsmodels to test whether tone predicts short-term abnormal returns:\n",
    "     car_0p1 ~ pos_pct + neg_pct + uncert_pct + sector + size_proxy\n",
    "- Evaluate coefficients, p-values, and confidence intervals to test significance.\n",
    "- Adjust for multiple comparisons (e.g., Benjamini–Hochberg correction) if running across multiple tone types or sectors.\n",
    "- Functions (data-manipulation): fit_model(), summarize_results()\n",
    "- Tests: Use synthetic data with known relationships to confirm correct coefficient direction and model behavior.\n",
    "- Interpretation:\n",
    "- - RQ1: Positive coefficients on pos_pct or negative on neg_pct indicate tone predicts abnormal returns.\n",
    "- - RQ2: Interaction terms or coefficient differences by sector suggest heterogeneity.\n",
    "- Connection: Directly achieves the Statistical Hypothesis Testing challenge goal.\n",
    "\n",
    "Step 6: Visualization\n",
    "- Create plots to display tone distributions, tone vs. return relationships, and regression coefficients.\n",
    "- Functions (plotting): plot_tone_vs_returns(), plot_coefficients()\n",
    "- Tests: No formal testing; figures checked visually for accuracy and clarity.\n",
    "- Output: Visual confirmation of tone–return relationships.\n",
    "- Connection: Helps interpret quantitative results for RQ1 and RQ2.\n",
    "\n",
    "\n",
    "*Step 7: Robustness and reporting (optional)*\n",
    "- *Re-run models using alternative event windows ([−1,+1] or [0,+5]) to confirm consistency.*\n",
    "- *Winsorize extreme returns to check for sensitivity to outliers.*\n",
    "- *Save outputs, figures, and summary tables for reporting.*\n",
    "- *Connection: Provides robustness checks for RQ1 and RQ3, ensuring conclusions are not window-dependent.*\n",
    "\n",
    "#### Plan\n",
    "\n",
    "The project will be completed in JupyterHub and divided into five main tasks, each designed to be clear, independent, and reproducible.\n",
    "\n",
    "\n",
    "1) Setup and data preparation (2 hours): I will create an organized folder structure in JupyterHub with subfolders for raw data, processed data, figures, and reports. After confirming the environment setup, I will load the earnings call transcripts, stock prices, and SPY benchmark data using pandas. During this step, I will standardize date formats, check for missing or duplicated keys, and ensure that tickers and dates align across datasets to prepare for merging.\n",
    "\n",
    "\n",
    "2) Text cleaning and tone computation (3 hours): Using Python’s re library, I will remove punctuation, normalize spacing, and lowercase the transcript text. I will then apply the Loughran-McDonald financial dictionary or the spaCy API to calculate the percentage of positive, negative, and uncertainty words for each transcript. The resulting tone features will be saved as a separate dataset and tested on a small subset of text examples to confirm accuracy.\n",
    "\n",
    "\n",
    "3) Return calculations and event-window construction (3 hours): I will compute daily returns for both individual tickers and the SPY benchmark. For each earnings call, I will define the event window as [0,+1], where Day 0 represents the first trading day on or after the call. Abnormal returns will be calculated as the firm’s return minus SPY’s return, and cumulative abnormal returns (CAR) will be saved for each event. Manual checks on a small synthetic dataset will verify the accuracy of these calculations.\n",
    "\n",
    "\n",
    "4) Merging, modeling, and hypothesis testing (5 hours): I will merge the tone dataset with abnormal returns and add control variables such as industry sector and firm size proxies (e.g., log of average volume). Using statsmodels, I will run regression models to test whether tone predicts short-window abnormal returns while controlling for other factors. I will interpret coefficients, p-values, and confidence intervals directly in the context of the research questions.\n",
    "\n",
    "\n",
    "5) Visualization and reporting (~3 hours): The final step will involve creating plots to display the distribution of tone features, the relationship between tone and abnormal returns, and regression coefficients with confidence intervals. If time allows, I will perform quick robustness checks such as alternate event windows or light outlier filtering. All intermediate results, figures, and tables will be saved for reproducibility.\n",
    "\n",
    "\n",
    "*This plan builts in buffer time and may be an overestimation*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dab228-c348-4832-897e-0cb08da79783",
   "metadata": {},
   "source": [
    "## EDA Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81162ef9-cf6b-4cd1-8a69-e5e76df90bc4",
   "metadata": {},
   "source": [
    "lalalla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf174a32-5149-4480-905a-6f6e0f5a1164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "import doctest\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf13c8e-3b9b-4cdf-b24b-bb5b0373cb1e",
   "metadata": {},
   "source": [
    "### Initial Loading\n",
    "#### Earnings Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "92111f5d-1457-47f9-aa5f-7e2f769f5bf7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>exchange</th>\n",
       "      <th>q</th>\n",
       "      <th>ticker</th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aug 27, 2020, 9:00 p.m. ET</td>\n",
       "      <td>NASDAQ: BILI</td>\n",
       "      <td>2020-Q2</td>\n",
       "      <td>BILI</td>\n",
       "      <td>Prepared Remarks:\\nOperator\\nGood day, and wel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jul 30, 2020, 4:30 p.m. ET</td>\n",
       "      <td>NYSE: GFF</td>\n",
       "      <td>2020-Q3</td>\n",
       "      <td>GFF</td>\n",
       "      <td>Prepared Remarks:\\nOperator\\nThank you for sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Oct 23, 2019, 5:00 p.m. ET</td>\n",
       "      <td>NASDAQ: LRCX</td>\n",
       "      <td>2020-Q1</td>\n",
       "      <td>LRCX</td>\n",
       "      <td>Prepared Remarks:\\nOperator\\nGood day and welc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nov 6, 2019, 12:00 p.m. ET</td>\n",
       "      <td>NASDAQ: BBSI</td>\n",
       "      <td>2019-Q3</td>\n",
       "      <td>BBSI</td>\n",
       "      <td>Prepared Remarks:\\nOperator\\nGood day, everyon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aug 7, 2019, 8:30 a.m. ET</td>\n",
       "      <td>NASDAQ: CSTE</td>\n",
       "      <td>2019-Q2</td>\n",
       "      <td>CSTE</td>\n",
       "      <td>Prepared Remarks:\\nOperator\\nGreetings and wel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         date      exchange        q ticker  \\\n",
       "0  Aug 27, 2020, 9:00 p.m. ET  NASDAQ: BILI  2020-Q2   BILI   \n",
       "1  Jul 30, 2020, 4:30 p.m. ET     NYSE: GFF  2020-Q3    GFF   \n",
       "2  Oct 23, 2019, 5:00 p.m. ET  NASDAQ: LRCX  2020-Q1   LRCX   \n",
       "3  Nov 6, 2019, 12:00 p.m. ET  NASDAQ: BBSI  2019-Q3   BBSI   \n",
       "4   Aug 7, 2019, 8:30 a.m. ET  NASDAQ: CSTE  2019-Q2   CSTE   \n",
       "\n",
       "                                          transcript  \n",
       "0  Prepared Remarks:\\nOperator\\nGood day, and wel...  \n",
       "1  Prepared Remarks:\\nOperator\\nThank you for sta...  \n",
       "2  Prepared Remarks:\\nOperator\\nGood day and welc...  \n",
       "3  Prepared Remarks:\\nOperator\\nGood day, everyon...  \n",
       "4  Prepared Remarks:\\nOperator\\nGreetings and wel...  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reports_path = \"data/motley-fool-data.pkl\"\n",
    "with open(reports_path, 'rb') as file:\n",
    "    er = pickle.load(file)\n",
    "\n",
    "er.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c839d8a0-17ea-45df-b727-d4dae108ac38",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2020-08-27 21:00:00\n",
       "1   2020-07-30 16:30:00\n",
       "2   2019-10-23 17:00:00\n",
       "3   2019-11-06 12:00:00\n",
       "4   2019-08-07 08:30:00\n",
       "Name: datetime_std, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "er['date_cleaned'] = er['date'].str.strip()\n",
    "er['date_cleaned'] = er['date_cleaned'].str.replace(\".\", \"\")\n",
    "er['date_cleaned'] = er['date_cleaned'].str.replace(\"ET\",\"\")\n",
    "er['datetime_std'] = pd.to_datetime(er['date_cleaned'], format='mixed')\n",
    "\n",
    "er['datetime_std'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ca7238ae-680c-466c-a664-6dfd8bedb31e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime_std</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-08-27 21:00:00</td>\n",
       "      <td>BILI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-07-30 16:30:00</td>\n",
       "      <td>GFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-10-23 17:00:00</td>\n",
       "      <td>LRCX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-06 12:00:00</td>\n",
       "      <td>BBSI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-08-07 08:30:00</td>\n",
       "      <td>CSTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18750</th>\n",
       "      <td>2021-11-09 13:00:00</td>\n",
       "      <td>SWX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18751</th>\n",
       "      <td>2021-11-18 12:00:00</td>\n",
       "      <td>PNNT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18752</th>\n",
       "      <td>2022-02-08 11:00:00</td>\n",
       "      <td>TDG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18753</th>\n",
       "      <td>2022-02-28 16:30:00</td>\n",
       "      <td>DVAX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18754</th>\n",
       "      <td>2021-08-12 09:00:00</td>\n",
       "      <td>CIB)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18755 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime_std ticker\n",
       "0     2020-08-27 21:00:00   BILI\n",
       "1     2020-07-30 16:30:00    GFF\n",
       "2     2019-10-23 17:00:00   LRCX\n",
       "3     2019-11-06 12:00:00   BBSI\n",
       "4     2019-08-07 08:30:00   CSTE\n",
       "...                   ...    ...\n",
       "18750 2021-11-09 13:00:00    SWX\n",
       "18751 2021-11-18 12:00:00   PNNT\n",
       "18752 2022-02-08 11:00:00    TDG\n",
       "18753 2022-02-28 16:30:00   DVAX\n",
       "18754 2021-08-12 09:00:00   CIB)\n",
       "\n",
       "[18755 rows x 2 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "er[[\"datetime_std\",\"ticker\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c617fd-f79b-4267-8916-60d2f8f5c594",
   "metadata": {},
   "source": [
    "#### Nasdaq Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "62f2ee07-aa6a-4722-bb16-3050a73f4faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = Path('data/nasdaq_prices')\n",
    "nasdaq_files = list(folder_path.glob('*.csv'))\n",
    "\n",
    "df_list = [pd.read_csv(file) for file in nasdaq_files]        \n",
    "ohlcv = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5c2c25fc-88b1-4099-90f7-9f17c344da34",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4621943 entries, 0 to 4621942\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Dtype  \n",
      "---  ------  -----  \n",
      " 0   ticker  object \n",
      " 1   date    object \n",
      " 2   open    float64\n",
      " 3   high    float64\n",
      " 4   low     float64\n",
      " 5   close   float64\n",
      "dtypes: float64(4), object(2)\n",
      "memory usage: 211.6+ MB\n"
     ]
    }
   ],
   "source": [
    "ohlcv.head()\n",
    "ohlcv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "51a43e73-2996-4957-a722-64fc068cad3d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4621943 entries, 0 to 4621942\n",
      "Data columns (total 7 columns):\n",
      " #   Column    Dtype         \n",
      "---  ------    -----         \n",
      " 0   ticker    object        \n",
      " 1   date      object        \n",
      " 2   open      float64       \n",
      " 3   high      float64       \n",
      " 4   low       float64       \n",
      " 5   close     float64       \n",
      " 6   date_std  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(4), object(2)\n",
      "memory usage: 246.8+ MB\n"
     ]
    }
   ],
   "source": [
    "ohlcv['date_std'] = pd.to_datetime(ohlcv['date'])\n",
    "# ohlcv.head()\n",
    "ohlcv.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8a44a4-d047-46dd-b973-a142fb5f3317",
   "metadata": {},
   "source": [
    "#### SPY Index Data (S&P 500 Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "302a2e53-1b92-4a28-8ad1-42b5ff37273e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7703 entries, 0 to 7702\n",
      "Data columns (total 7 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Date       7703 non-null   object \n",
      " 1   Open       7703 non-null   float64\n",
      " 2   High       7703 non-null   float64\n",
      " 3   Low        7703 non-null   float64\n",
      " 4   Close      7703 non-null   float64\n",
      " 5   Adj Close  7703 non-null   float64\n",
      " 6   Volume     7703 non-null   int64  \n",
      "dtypes: float64(5), int64(1), object(1)\n",
      "memory usage: 421.4+ KB\n"
     ]
    }
   ],
   "source": [
    "spy = pd.read_csv('data/SPY.csv')\n",
    "spy.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "eb70d8bf-674b-411e-a19b-0ed9381026c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spy['Date'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bc44a613-434f-4f87-be66-1e6b313fb701",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7703 entries, 0 to 7702\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   Date       7703 non-null   object        \n",
      " 1   Open       7703 non-null   float64       \n",
      " 2   High       7703 non-null   float64       \n",
      " 3   Low        7703 non-null   float64       \n",
      " 4   Close      7703 non-null   float64       \n",
      " 5   Adj Close  7703 non-null   float64       \n",
      " 6   Volume     7703 non-null   int64         \n",
      " 7   date_std   7703 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(5), int64(1), object(1)\n",
      "memory usage: 481.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# should i be making a new df for this update\n",
    "spy['date_std'] = pd.to_datetime(spy['Date'])\n",
    "# spy.head()\n",
    "spy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e63095c-5f7f-403a-a4af-1c13c992c6d2",
   "metadata": {},
   "source": [
    "#### Loughran-McDonald Master Dictionary w/ Sentiment Word Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a2bf4a2d-bd51-40d8-ac31-382a69c999af",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 86553 entries, 0 to 86552\n",
      "Data columns (total 17 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   Word                86552 non-null  object \n",
      " 1   Seq_num             86553 non-null  int64  \n",
      " 2   Word Count          86553 non-null  int64  \n",
      " 3   Word Proportion     86553 non-null  float64\n",
      " 4   Average Proportion  86553 non-null  float64\n",
      " 5   Std Dev             86553 non-null  float64\n",
      " 6   Doc Count           86553 non-null  int64  \n",
      " 7   Negative            86553 non-null  int64  \n",
      " 8   Positive            86553 non-null  int64  \n",
      " 9   Uncertainty         86553 non-null  int64  \n",
      " 10  Litigious           86553 non-null  int64  \n",
      " 11  Strong_Modal        86553 non-null  int64  \n",
      " 12  Weak_Modal          86553 non-null  int64  \n",
      " 13  Constraining        86553 non-null  int64  \n",
      " 14  Complexity          86553 non-null  int64  \n",
      " 15  Syllables           86553 non-null  int64  \n",
      " 16  Source              86553 non-null  object \n",
      "dtypes: float64(3), int64(12), object(2)\n",
      "memory usage: 11.2+ MB\n"
     ]
    }
   ],
   "source": [
    "lmd = pd.read_csv('data/Loughran-McDonald_MasterDictionary_1993-2024.csv')\n",
    "# lmd.head()\n",
    "lmd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd9abf9-b291-4c82-b384-d50d4fd8410d",
   "metadata": {},
   "source": [
    "### Table Join\n",
    "Joining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9a52e551-956a-4f6d-ab3e-bd5c920aaa14",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticker\n",
       "False    15832\n",
       "True      2923\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "er['ticker'].isin(ohlcv['ticker']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e521eda3-2259-4359-bf18-e4a88495f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ticker_compare (df1, col1, df2, col2):\n",
    "    '''\n",
    "    Given two dataframes and two column names, returns the values shared\n",
    "    between the two dataframe columns, for each dataframe (returned as a\n",
    "    tuple of two separate dataframes).\n",
    "\n",
    "    >>> df1 = pd.DataFrame({'ticker': ['AAPL', 'MSFT', 'GOOG'], 'price': [100, 200, 300]})\n",
    "    >>> df2 = pd.DataFrame({'ticker': ['AAPL', 'TSLA'], 'text': ['apple er', 'tesla er']})\n",
    "    >>> df1_common, df2_common = ticker_compare(df1, 'ticker', df2, 'ticker')\n",
    "    >>> sorted(df1_common['ticker'].unique())\n",
    "    ['AAPL']\n",
    "    >>> sorted(df1_common['price'].unique())\n",
    "    [100]\n",
    "    >>> sorted(df2_common['ticker'].unique())\n",
    "    ['AAPL']\n",
    "\n",
    "    >>> df3 = pd.DataFrame({'ticker': ['AMZN'], 'close': [150]})\n",
    "    >>> df4 = pd.DataFrame({'ticker': ['NFLX'], 'text': ['netflix']})\n",
    "    >>> df3_common, df4_common = ticker_compare(df3, 'ticker', df4, 'ticker')\n",
    "    >>> len(df3_common)\n",
    "    0\n",
    "    >>> len(df4_common)\n",
    "    0\n",
    "    >>> sorted(df2_common['text'].unique())\n",
    "    ['apple er']\n",
    "    '''\n",
    "    ticker_1 = set(df1[col1])\n",
    "    ticker_2 = set(df2[col2])\n",
    "    shared = ticker_1 & ticker_2\n",
    "\n",
    "    df1_common = df1[df1[col1].isin(shared)]\n",
    "    df2_common = df2[df2[col2].isin(shared)]\n",
    "\n",
    "    return df1_common, df2_common\n",
    "\n",
    "doctest.run_docstring_examples(ticker_compare, globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35435aef-3c3d-41e7-9507-d056195b02c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ohlcv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ohlcv_common, er_common \u001b[38;5;241m=\u001b[39m ticker_compare(\u001b[43mohlcv\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mticker\u001b[39m\u001b[38;5;124m'\u001b[39m, er, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mticker\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ohlcv' is not defined"
     ]
    }
   ],
   "source": [
    "ohlcv_common, er_common = ticker_compare(ohlcv, 'ticker', er, 'ticker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d3f8b510-8b77-4ba2-9a21-3fdf2ffb1f9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ticker</th>\n",
       "      <th>date</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>date_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28689</th>\n",
       "      <td>AXGN</td>\n",
       "      <td>1986-12-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1986-12-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28690</th>\n",
       "      <td>AXGN</td>\n",
       "      <td>1986-12-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1986-12-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28691</th>\n",
       "      <td>AXGN</td>\n",
       "      <td>1986-12-19</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1986-12-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28692</th>\n",
       "      <td>AXGN</td>\n",
       "      <td>1986-12-22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.10</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1986-12-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28693</th>\n",
       "      <td>AXGN</td>\n",
       "      <td>1986-12-23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.67</td>\n",
       "      <td>3.67</td>\n",
       "      <td>1986-12-23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ticker        date  open  high   low  close   date_std\n",
       "28689   AXGN  1986-12-17   0.0  4.10  4.00   4.00 1986-12-17\n",
       "28690   AXGN  1986-12-18   0.0  4.10  4.00   4.00 1986-12-18\n",
       "28691   AXGN  1986-12-19   0.0  4.10  4.00   4.00 1986-12-19\n",
       "28692   AXGN  1986-12-22   0.0  4.10  4.00   4.00 1986-12-22\n",
       "28693   AXGN  1986-12-23   0.0  3.89  3.67   3.67 1986-12-23"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohlcv_common.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154a6d03-ec4c-4e31-99a6-a2d787f2bf84",
   "metadata": {},
   "source": [
    "### Test Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07de0794-4cb6-4ac6-9c62-17d1a9acc809",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = [100, 112, 125, 180, 111, 96]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "228070b8-c2e0-4de3-8a15-c620c6be4972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120.66666666666667\n"
     ]
    }
   ],
   "source": [
    "# average price all-time\n",
    "total = 0\n",
    "for price in prices:\n",
    "    total += price\n",
    "avg_price = total / len(prices)\n",
    "print(avg_price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a910a3ca-2b8c-4ab3-b1b2-bf6286667690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: None, 1: 12.0, 2: 11.607142857142858, 3: 44.0, 4: -38.333333333333336, 5: -13.513513513513514}\n"
     ]
    }
   ],
   "source": [
    "# daily return (as %)\n",
    "returns = {}\n",
    "for i in range(len(prices)):\n",
    "    if i != 0:\n",
    "        returns[i] = (prices[i] - prices[i-1])/prices[i-1]*100\n",
    "    else:\n",
    "        returns[i] = None\n",
    "\n",
    "print(returns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
